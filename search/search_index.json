{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"Probabilistic Confusion Matrices <p><code>prob_conf_mat</code> is a Python package for performing statistical inference with confusion matrices. It quantifies the amount of uncertainty present, aggregates semantically related experiments into experiment groups, and compares experiments against each other for significance.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>Installation can be done using from pypi can be done using <code>pip</code>:</p> <pre><code>pip install prob_conf_mat\n</code></pre> <p>Or, if you're using <code>uv</code>, simply run:</p> <pre><code>uv add prob_conf_mat\n</code></pre> <p>The project currently depends on the following packages:</p> Dependency tree <pre><code>bayes-conf-mat\n\u251c\u2500\u2500 jaxtyping v0.3.2\n\u251c\u2500\u2500 matplotlib v3.10.3\n\u251c\u2500\u2500 numpy v2.3.0\n\u251c\u2500\u2500 scipy v1.15.3\n\u251c\u2500\u2500 seaborn v0.13.2\n\u2502   \u2514\u2500\u2500 pandas v2.3.0\n\u2514\u2500\u2500 tabulate v0.9.0\n</code></pre>"},{"location":"index.html#development-environment","title":"Development Environment","text":"<p>This project was developed using <code>uv</code>. To install the development environment, simply clone this github repo:</p> <pre><code>git clone https://github.com/ioverho/prob_conf_mat.git\n</code></pre> <p>And then run the <code>uv sync --dev</code> command:</p> <pre><code>uv sync --dev\n</code></pre> <p>The development dependencies should automatically install into the <code>.venv</code> folder.</p>"},{"location":"index.html#documentation","title":"Documentation","text":"<p>For more information about the package, motivation, how-to guides and implementation, please see the documentation website. We try to use Daniele Procida's structure for Python documentation.</p> <p>The documentation is broadly divided into 4 sections:</p> <ol> <li>Getting Started: a collection of small tutorials to help new users get started</li> <li>How To: more expansive guides on how to achieve specific things</li> <li>Reference: in-depth information about how to interface with the library</li> <li>Explanation: explanations about why things are the way they are</li> </ol> Learning Coding Practical Getting Started How-To Guides Theoretical Explanation Reference"},{"location":"index.html#quick-start","title":"Quick Start","text":"<p>In depth tutorials taking you through all basic steps are available on the documentation site. For the impatient, here's a standard use case.</p> <p>First define a study, and set some sensible hyperparameters for the simulated confusion matrices.</p> <pre><code>from prob_conf_mat import Study\n\nstudy = Study(\n    seed=0,\n    num_samples=10000,\n    ci_probability=0.95,\n)\n</code></pre> <p>Then add a experiment and confusion matrix to the study:</p> <pre><code>study.add_experiment(\n  experiment_name=\"model_1/fold_0\",\n  confusion_matrix=[\n    [13, 0, 0],\n    [0, 10, 6],\n    [0,  0, 9],\n  ],\n  confusion_prior=0,\n  prevalence_prior=1,\n)\n</code></pre> <p>Finally, add some metrics to the study:</p> <pre><code>study.add_metric(\"acc\")\n</code></pre> <p>We are now ready to start generating summary statistics about this experiment. For example:</p> <pre><code>study.report_metric_summaries(\n  metric=\"acc\",\n  table_fmt=\"github\"\n)\n</code></pre> Group Experiment Observed Median Mode 95.0% HDI MU Skew Kurt model_1 fold_0 0.8421 0.8499 0.8673 [0.7307, 0.9464] 0.2157 -0.5627 0.2720 <p>So while this experiment achieves an accuracy of 84.21%, a more reasonable estimate (given the size of the test set, and) would be 84.99%. There is a 95% probability that the true accuracy lies between 73.07%-94.64%.</p> <p>Visually that looks something like:</p> <pre><code>fig = study.plot_metric_summaries(metric=\"acc\")\n</code></pre> <p> </p> <p>Now let's add a confusion matrix for the same model, but estimated using a different fold. We want to know what the average performance is for that model across the different folds:</p> <pre><code>study.add_experiment(\n  experiment_name=\"model_1/fold_1\",\n  confusion_matrix=[\n      [12, 1, 0],\n      [1, 8, 7],\n      [0, 2, 7],\n  ],\n  confusion_prior=0,\n  prevalence_prior=1,\n)\n</code></pre> <p>We can equip each metric with an inter-experiment aggregation method, and we can then request summary statistics about the aggregate performance of the experiments using <code>'model_1'</code>:</p> <pre><code>study.add_metric(\n    metric=\"acc\",\n    aggregation=\"beta\",\n)\n\nfig = study.plot_forest_plot(metric=\"acc\")\n</code></pre> <p> </p> <p>Note that estimated aggregate accuracy has much less uncertainty (a smaller HDI/MU).</p> <p>These experiments seem pretty different. But is this difference significant? Let's assume that for this example a difference needs to be at least <code>'0.05'</code> to be considered significant. In that case, we can quickly request the probability of their difference:</p> <pre><code>fig = study.plot_pairwise_comparison(\n    metric=\"acc\",\n    experiment_a=\"model_1/fold_0\",\n    experiment_b=\"model_1/fold_1\",\n    min_sig_diff=0.05,\n)\n</code></pre> <p> </p> <p>There's about an 82% probability that the difference is in fact significant. While likely, there isn't quite enough data to be sure.</p>"},{"location":"index.html#credits","title":"Credits","text":"<p>The following are some packages and libraries which served as inspiration for aspects of this project: arviz, bayestestR, BERTopic, jaxtyping, mici, , python-ci, statsmodels.</p> <p>A lot of the approaches and methods used in this project come from published works. Some especially important works include:</p> <ol> <li>Goutte, C., &amp; Gaussier, E. (2005). A probabilistic interpretation of precision, recall and F-score, with implication for evaluation. In European conference on information retrieval (pp. 345-359). Berlin, Heidelberg: Springer Berlin Heidelberg.</li> <li>T\u00f6tsch, N., &amp; Hoffmann, D. (2021). Classifier uncertainty: evidence, potential impact, and probabilistic treatment. PeerJ Computer Science, 7, e398.</li> <li>Kruschke, J. K. (2013). Bayesian estimation supersedes the t test. Journal of Experimental Psychology: General, 142(2), 573.</li> <li>Makowski, D., Ben-Shachar, M. S., Chen, S. A., &amp; L\u00fcdecke, D. (2019). Indices of effect existence and significance in the Bayesian framework. Frontiers in psychology, 10, 2767.</li> <li>Hill, T. (2011). Conflations of probability distributions. Transactions of the American Mathematical Society, 363(6), 3351-3372.</li> <li>Chandler, J., Cumpston, M., Li, T., Page, M. J., &amp; Welch, V. J. H. W. (2019). Cochrane handbook for systematic reviews of interventions. Hoboken: Wiley, 4.</li> </ol>"},{"location":"index.html#citation","title":"Citation","text":"<pre><code>@software{ioverho_prob_conf_mat,\n    author = {Verhoeven, Ivo},\n    license = {MIT},\n    title = {{prob\\_conf\\_mat}},\n    url = {https://github.com/ioverho/prob_conf_mat}\n}\n</code></pre>"},{"location":"Explanation/experiment_aggregation.html","title":"Experiment Aggregation","text":""},{"location":"Explanation/generating_confusion_matrices.html","title":"Generating Confusion Matrices","text":"<p>Classifiers, or classification models, are everywhere in machine-learning. Specifically, we consider models, denoted \\(f\\), which map some set of input features \\(X\\) to a condition \\(Y\\): \\(\\hat{y}=f(x),~\\mathcal{X}\\rightarrow \\mathcal{Y}\\).</p> <p>Note</p> <p>I use the word 'condition' here, because it seems to be the convention in the (bio)statistics literature. The word 'class', 'label', 'target' are all equivalent.</p> <p>The goal of a machine-learning algorithm is typically to optimize the classifier over some training data where labels are available, \\((x_{n},y_{n})\\in\\mathcal{D}^{\\text{(train)}}\\), in order to perform well on data where labels are not available. Since we want to deploy the classifier on unseen data, but since we do not have access to deployment data distribution, we instead estimate the model's 'performance' on a held-out subset of our data \\(\\mathcal{D}^{\\text{(test)}}\\), such that \\(\\mathcal{D}^{\\text{(train)}}\\cup \\mathcal{D}^{\\text{(test)}}=\\varnothing\\).</p> <p>The confusion matrix, \\(\\mathbf{C}\\), is a \\(\\left|\\mathcal{Y}\\right|\\times\\left|\\mathcal{Y}\\right|\\) matrix that summarizes the model's performance on the test set. Each entry contains the count of a \\((y_{n}, \\hat{y}_{n})\\) pair. In other words, element \\(\\mathbf{C}_{i,j}\\) denotes the number of times the model predict class \\(j\\), when the sample's condition is actually \\(i\\).</p> <p>In a binary classification scenario (\\(|\\mathcal{Y}|=2\\)), each element of the confusion matrix has a simple interpretation. Assuming the positive class is index \\(1\\):</p> <ul> <li>\\(\\mathbf{C}_{1,1}\\) contains the number of True Positives (TP), where the model correctly predicts the condition is present</li> <li>\\(\\mathbf{C}_{1,2}\\) contains the count of False Negatives (FN), where the model predicted the condition is not present, when in truth it was</li> <li>\\(\\mathbf{C}_{2,1}\\), the False Positives (FP), where the model predicts the condition is present, when in truth it is not</li> <li>\\(\\mathbf{C}_{2,2}\\), the True Negatives (TN), contains the count of all predictions for which the model correctly estimated that the condition was not present</li> </ul> <p>Visually, this looks like:</p> <p> </p>"},{"location":"Explanation/generating_confusion_matrices.html#sampling-confusion-matrices","title":"Sampling Confusion Matrices","text":"<p>With <code>prob_conf_mat</code>, we want to be able to quantify the uncertainty present in our metric estimates stemming from the size and composition of the test data. To achieve this, we generate synthetic confusion matrices by sampling from the product of two Dirichlet-Categorical posteriors.</p> <p>This approach is directly inspired by two papers Caelen (2017)<sup>1</sup> and T\u00f6tsch et al. (2020)<sup>2</sup>. The former treats the entire confusion matrix as a single Dirichlet-Categorical model, whereas the latter computes a Beta-Binomial distribution for three distinct quantities, and reconstructs the confusion matrix through their product. In either approach, however, the posterior is 'learned' by taking into account the various different counts of prediction-condition pairs.</p> <p>Most important to <code>prob_conf_mat</code> is the T\u00f6tsch model. In their paper<sup>2</sup>, they show that the confidence intervals they produce are consistently closer to the truth than those produced by Caelen (2017)<sup>1</sup>, despite the simpler model used in the latter.</p> <p>Specifically, T\u00f6tsch et al. (2020)<sup>2</sup> model the binary confusion matrix as products of the following probabilities:</p> <ul> <li>The prevalence \\(\\phi\\), or how often each condition occurs</li> </ul> \\[ \\phi=\\dfrac{\\text{TP}+\\text{FN}}{\\text{TP}+\\text{FN}+\\text{FP}+\\text{TN}} \\] <ul> <li>The True Positive Rate (TPR), or the proportion of times the model predicts the condition is present, conditioned on the fact that the condition is actually present</li> </ul> \\[\\text{TPR}=\\dfrac{\\text{TP}}{\\text{TP}+\\text{FN}}\\] <ul> <li>The True Negative Rate (TNR), or the proportion of times the model predicts the condition is not present, conditioned on the fact that the condition is actually not present</li> </ul> \\[\\text{TNR}=\\dfrac{\\text{TN}}{\\text{TN}+\\text{FP}}\\] <p>With these 3 proportions, it is possible to reconstruct the entire confusion matrix as:</p> \\[ \\mathbf{C}=\\begin{bmatrix}     \\phi\\cdot\\text{TPR} &amp; \\phi\\cdot(1-\\text{TPR}) \\\\     (1-\\phi)\\cdot(1-\\text{TNR}) &amp; (1-\\phi)\\cdot\\text{TNR} \\\\ \\end{bmatrix} \\] <p>To generate counterfactual confusion matrices, they model each probability as a Beta-Binomial distribution, with the following posteriors:</p> \\[ \\begin{align*}     \\phi&amp;\\sim \\text{Beta}\\left(\\alpha+\\text{TP}+\\text{FP},\\beta+\\text{FN}+\\text{TN}\\right) \\\\     \\text{TPR}&amp;\\sim \\text{Beta}\\left(\\alpha+\\text{TP},\\beta+\\text{FP}\\right) \\\\     \\text{TNR}&amp;\\sim \\text{Beta}\\left(\\alpha+\\text{TN},\\beta+\\text{FN}\\right) \\end{align*} \\] <p>Here, \\(\\alpha\\) and \\(\\beta\\) are different prior hyperparameters.</p> <p>With the T\u00f6tsch model, it's possible to produce plausible synthetic confusion matrices by sampling a prevalence, true positive rate and true negative rate independently, and computing four products. At least for their purposes, this model proved highly effective at generating counterfactual confusion matrices, and allowed the authors to provide confidence intervals for any metric.</p> <p>One difference between the T\u00f6tsch model and convention, however, is that their approach only generates normalized confusion matrices, i.e., \\(\\sum_{i,j}\\mathbf{C}_{i,j}=1\\). As we'll see in another explainer, this has no effect on the value of the computed metrics.</p>"},{"location":"Explanation/generating_confusion_matrices.html#multi-class-confusion-matrices","title":"Multi-Class Confusion Matrices","text":"<p>While the T\u00f6tsch model is applicable to binary confusion matrices, they did not make the extension to multi-class confusion matrices. In a multi-class confusion matrix, each condition has its own set of TP, FN, FP, and TN. Specifically, all the elements on the same row or column form the FN and FP, respectively, and the TN are all elements which share neither the same row or column. This is pictured in the following figure:</p> <p> </p> <p>Luckily, an extension is easily made. Given that the method presently only produces normalized confusion matrices, we can essentially view this as a joint probability distribution: \\(p(Y, \\hat{Y})\\). Then, by the definition of the conditional probability distribution, we can rewrite this as,</p> \\[ p(Y, \\hat{Y})=p(\\hat{Y}|Y)p(Y) \\] <p>Here \\(p(Y)\\) is the probability of a particular condition occurring, and \\(p(\\hat{Y}|Y)\\) is the probability of a model making a certain prediction, given that the ground-truth condition is \\(Y=y\\). The former is simply the multi-class prevalence, and the latter we call the confusion conditional. The \\(\\text{TPR}\\) and \\(\\text{TNR}\\) rates are essentially just different elements of that confusion conditional:</p> \\[ \\begin{align*} \\text{TPR}=p(Y=2|\\hat{Y}=1),&amp;\\quad (1-\\text{TPR})=p(Y=1|\\hat{Y}=1)\\\\ \\text{TNR}=p(Y=1|\\hat{Y}=2),&amp;\\quad (1-\\text{TNR})=p(Y=2|\\hat{Y}=2) \\\\ \\end{align*} \\] <p>So in practice, the T\u00f6tsch model is just computing the product between the prevalence of a class \\(p(Y)\\), and the confusion conditional, \\(p(\\hat{Y}|Y)\\).</p> <p>This is easy to replicate in a multi-class setting. We sample each row of the confusion conditional and the prevalence marginal from a Dirichlet-Multinomial model. Then, to construct a confusion matrix, we compute the product between the two. Specifically, we generate each synthetic confusion matrix, \\(\\tilde{\\mathbf{C}}\\) as the Hadamard product between the sampled conditional confusion matrix, and the sampled prevalence vector:</p> \\[ \\begin{align*} \\tilde{\\mathbf{C}}=~~&amp;\\text{Diag}(\\phi) \\begin{bmatrix}     \\tilde{C}_{1}   \\\\     \\vdots          \\\\     \\tilde{C}_{|\\mathcal{Y}|} \\end{bmatrix} \\\\ &amp;\\phi\\sim\\text{Dirichlet}(\\alpha_{1}+\\sum_{j}^{|\\mathcal{Y}|}\\mathbf{C}_{1,j},\\ldots \\alpha_{|\\mathcal{Y}|}+\\sum_{j}^{|\\mathcal{Y}|}\\mathbf{C}_{|\\mathcal{Y}|,j}) \\\\ &amp;\\tilde{C}_{1}\\sim \\text{Dirichlet}(\\alpha_{1,1}+\\mathbf{C}_{1,1},\\ldots, \\alpha_{1,|\\mathcal{Y}|}+\\mathbf{C}_{1,|\\mathcal{Y}|}) \\\\ &amp;\\qquad\\qquad\\vdots \\\\ &amp;\\tilde{C}_{|\\mathcal{Y}|}\\sim \\text{Dirichlet}(\\alpha_{|\\mathcal{Y}|,1}+\\mathbf{C}_{|\\mathcal{Y}|,1},\\ldots, \\alpha_{|\\mathcal{Y}|,|\\mathcal{Y}|}+\\mathbf{C}_{|\\mathcal{Y}|,|\\mathcal{Y}|}) \\end{align*} \\] <p>where \\(\\alpha\\) are again various prior hyperparameters. In total, we learn \\(\\mathcal{Y}+1\\) Dirichlet-Multinomial posteriors, and sample from each independently to produce a synthetic confusion matrix. The output of the product is a \\(\\mathcal{Y}\\times \\mathcal{Y}\\) matrix, normalized such that \\(\\sum_{i,j}\\tilde{\\mathbf{C}}_{i,j}=1\\).</p> <p>Sounds complicated, but it's Numpy implementation is dead simple:</p> <pre><code>condition_counts, condition_prior: Float[ndarray, \"num_classes\"]\nconfusion_matrix, confusion_prior: Float[ndarray, \"num_classes num_classes\"]\n\np_condition = dirichlet_sample(\n    rng=self.rng,\n    alphas=condition_counts + condition_prior,\n    num_samples=num_samples,\n)\n\np_pred_given_condition = dirichlet_sample(\n    rng=self.rng,\n    alphas=confusion_matrix + confusion_prior,\n    num_samples=num_samples,\n)\n\nnorm_confusion_matrix = p_pred_given_condition * p_condition[:, :, np.newaxis]\n</code></pre> <p>Note</p> <p>This uses the array annotation syntax introduced by <code>jaxtyping</code>. A <code>Float[ndarray, \"num_classes\"]</code> is a one-dimensioanl numpy array float, with length 'num_classes'</p> <p>Note</p> <p>The output of the <code>dirichlet_sample</code> method is batched, hence the extra dimension in the multiplication</p> <p>To see the <code>prob_conf_mat</code> implementation in action, check out the <code>Experiment._sample</code> source code.</p> <ol> <li> <p>Caelen, O. (2017). A Bayesian interpretation of the confusion matrix. Annals of Mathematics and Artificial Intelligence, 81(3), 429-450. \u21a9\u21a9</p> </li> <li> <p>T\u00f6tsch, N., &amp; Hoffmann, D. (2021). Classifier uncertainty: evidence, potential impact, and probabilistic treatment. PeerJ Computer Science, 7, e398. \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"Explanation/hypothesis_testing.html","title":"Comparing Different Experiments' Metric Distributions","text":""},{"location":"Explanation/metric_computation.html","title":"Computing Metrics on Counterfactual Confusion Matrices","text":""},{"location":"Explanation/totsch_replication.html","title":"A Replication of T\u00f6tsch, N. &amp; Hoffmann, D. (2020). 'Classifier uncertainty: evidence, potential impact, and probabilistic treatment'","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd  In\u00a0[2]: Copied! <pre>totsch_table_2 = [\n    [\"1\", \"10.1080/10629360903278800\", \"Table 2\", 5, 0, 3, 0, 8, 10],\n    [\"2\", \"10.1021/ci200579f\", \"Table 3\", 10, 0, 3, 1, 14, 48],\n    [\"3\", \"10.1021/ci020045\", \"Table 5\", 6, 0, 7, 1, 14, 51],\n    [\"4a\", \"10.1155/2015/485864\", \"Table 4\", 5, 1, 10, 1, 17, 10],\n    [\"4b\", \"10.1155/2015/485864\", \"Table 5\", 4, 2, 10, 1, 17, 10],\n    [\"5a\", \"10.1016/j.ejmech.2010.11.029\", \"Table 6\", 16, 1, 3, 2, 22, 86],\n    [\"5b\", \"10.1016/j.ejmech.2010.11.029\", \"Table 10\", 8, 9, 4, 1, 22, 86],\n    [\"6a\", \"10.1016/j.vascn.2014.07.002\", \"Table 2\", 2, 12, 19, 1, 34, 77],\n    [\"6b\", \"10.1016/j.vascn.2014.07.002\", \"Table 3\", 10, 4, 20, 0, 34, 77],\n    [\"7a\", \"10.5935/0103-5053.20130066\", \"Table 2\", 26, 0, 6, 2, 34, 61],\n    [\"7b\", \"10.5935/0103-5053.20130066\", \"Table 3\", 24, 2, 6, 2, 34, 61],\n    [\"8\", \"10.1016/j.scitotenv.2018.05.081\", \"Table 2\", 28, 9, 3, 4, 44, 18],\n    [\"9a\", \"10.4314/wsa.v36i4.58411\", \"Table 2\", 19, 3, 18, 10, 50, 14],\n    [\"9b\", \"10.4314/wsa.v36i4.58411\", \"Table 2\", 21, 1, 20, 8, 50, 14],\n    [\"10\", \"10.1016/j.bspc.2017.01.012\", \"Figure 2\", 31, 5, 24, 4, 64, 80],\n    [\"11\", \"10.1039/C7MD00633K\", \"Figure 3\", 40, 7, 15, 8, 70, 9],\n    [\"12\", \"10.3389/fnins.2018.01008\", \"Figure 3\", 31, 9, 20, 13, 73, 1],\n    [\"13a\", \"10.4315/0362-028X-61.2.221\", \"Table 3\", 79, 14, 19, 0, 112, 52],\n    [\"13b\", \"10.4315/0362-028X-61.2.221\", \"Table 3\", 89, 4, 16, 3, 112, 52],\n    [\"14a\", \"10.1016/j.ancr.2014.06.005\", \"Figure 6.3\", 136, 2, 2, 12, 152, 7],\n    [\"15a\", \"10.1016/j.saa.2016.09.028\", \"Table 2\", 3, 12, 150, 0, 165, 65],\n    [\"15b\", \"10.1016/j.saa.2016.09.028\", \"Table 2\", 6, 9, 150, 0, 165, 65],\n    [\"16\", \"10.1021/acs.analchem.7b00426\", \"Table 3\", 188, 0, 13, 2, 203, 28],\n    [\"14b\", \"10.1016/j.ancr.2014.06.005\", \"Table 3\", 253, 27, 11, 59, 350, 7],\n]\n\ntotsch_table_2_df = pd.DataFrame.from_records(\n    totsch_table_2,\n    columns=[\"ID\", \"DOI\", \"Location\", \"TP\", \"FN\", \"TN\", \"FP\", \"N\", \"Citations\"],\n    index=\"ID\",\n)\n\ntotsch_table_2_df\n</pre> totsch_table_2 = [     [\"1\", \"10.1080/10629360903278800\", \"Table 2\", 5, 0, 3, 0, 8, 10],     [\"2\", \"10.1021/ci200579f\", \"Table 3\", 10, 0, 3, 1, 14, 48],     [\"3\", \"10.1021/ci020045\", \"Table 5\", 6, 0, 7, 1, 14, 51],     [\"4a\", \"10.1155/2015/485864\", \"Table 4\", 5, 1, 10, 1, 17, 10],     [\"4b\", \"10.1155/2015/485864\", \"Table 5\", 4, 2, 10, 1, 17, 10],     [\"5a\", \"10.1016/j.ejmech.2010.11.029\", \"Table 6\", 16, 1, 3, 2, 22, 86],     [\"5b\", \"10.1016/j.ejmech.2010.11.029\", \"Table 10\", 8, 9, 4, 1, 22, 86],     [\"6a\", \"10.1016/j.vascn.2014.07.002\", \"Table 2\", 2, 12, 19, 1, 34, 77],     [\"6b\", \"10.1016/j.vascn.2014.07.002\", \"Table 3\", 10, 4, 20, 0, 34, 77],     [\"7a\", \"10.5935/0103-5053.20130066\", \"Table 2\", 26, 0, 6, 2, 34, 61],     [\"7b\", \"10.5935/0103-5053.20130066\", \"Table 3\", 24, 2, 6, 2, 34, 61],     [\"8\", \"10.1016/j.scitotenv.2018.05.081\", \"Table 2\", 28, 9, 3, 4, 44, 18],     [\"9a\", \"10.4314/wsa.v36i4.58411\", \"Table 2\", 19, 3, 18, 10, 50, 14],     [\"9b\", \"10.4314/wsa.v36i4.58411\", \"Table 2\", 21, 1, 20, 8, 50, 14],     [\"10\", \"10.1016/j.bspc.2017.01.012\", \"Figure 2\", 31, 5, 24, 4, 64, 80],     [\"11\", \"10.1039/C7MD00633K\", \"Figure 3\", 40, 7, 15, 8, 70, 9],     [\"12\", \"10.3389/fnins.2018.01008\", \"Figure 3\", 31, 9, 20, 13, 73, 1],     [\"13a\", \"10.4315/0362-028X-61.2.221\", \"Table 3\", 79, 14, 19, 0, 112, 52],     [\"13b\", \"10.4315/0362-028X-61.2.221\", \"Table 3\", 89, 4, 16, 3, 112, 52],     [\"14a\", \"10.1016/j.ancr.2014.06.005\", \"Figure 6.3\", 136, 2, 2, 12, 152, 7],     [\"15a\", \"10.1016/j.saa.2016.09.028\", \"Table 2\", 3, 12, 150, 0, 165, 65],     [\"15b\", \"10.1016/j.saa.2016.09.028\", \"Table 2\", 6, 9, 150, 0, 165, 65],     [\"16\", \"10.1021/acs.analchem.7b00426\", \"Table 3\", 188, 0, 13, 2, 203, 28],     [\"14b\", \"10.1016/j.ancr.2014.06.005\", \"Table 3\", 253, 27, 11, 59, 350, 7], ]  totsch_table_2_df = pd.DataFrame.from_records(     totsch_table_2,     columns=[\"ID\", \"DOI\", \"Location\", \"TP\", \"FN\", \"TN\", \"FP\", \"N\", \"Citations\"],     index=\"ID\", )  totsch_table_2_df  Out[2]: DOI Location TP FN TN FP N Citations ID 1 10.1080/10629360903278800 Table 2 5 0 3 0 8 10 2 10.1021/ci200579f Table 3 10 0 3 1 14 48 3 10.1021/ci020045 Table 5 6 0 7 1 14 51 4a 10.1155/2015/485864 Table 4 5 1 10 1 17 10 4b 10.1155/2015/485864 Table 5 4 2 10 1 17 10 5a 10.1016/j.ejmech.2010.11.029 Table 6 16 1 3 2 22 86 5b 10.1016/j.ejmech.2010.11.029 Table 10 8 9 4 1 22 86 6a 10.1016/j.vascn.2014.07.002 Table 2 2 12 19 1 34 77 6b 10.1016/j.vascn.2014.07.002 Table 3 10 4 20 0 34 77 7a 10.5935/0103-5053.20130066 Table 2 26 0 6 2 34 61 7b 10.5935/0103-5053.20130066 Table 3 24 2 6 2 34 61 8 10.1016/j.scitotenv.2018.05.081 Table 2 28 9 3 4 44 18 9a 10.4314/wsa.v36i4.58411 Table 2 19 3 18 10 50 14 9b 10.4314/wsa.v36i4.58411 Table 2 21 1 20 8 50 14 10 10.1016/j.bspc.2017.01.012 Figure 2 31 5 24 4 64 80 11 10.1039/C7MD00633K Figure 3 40 7 15 8 70 9 12 10.3389/fnins.2018.01008 Figure 3 31 9 20 13 73 1 13a 10.4315/0362-028X-61.2.221 Table 3 79 14 19 0 112 52 13b 10.4315/0362-028X-61.2.221 Table 3 89 4 16 3 112 52 14a 10.1016/j.ancr.2014.06.005 Figure 6.3 136 2 2 12 152 7 15a 10.1016/j.saa.2016.09.028 Table 2 3 12 150 0 165 65 15b 10.1016/j.saa.2016.09.028 Table 2 6 9 150 0 165 65 16 10.1021/acs.analchem.7b00426 Table 3 188 0 13 2 203 28 14b 10.1016/j.ancr.2014.06.005 Table 3 253 27 11 59 350 7 In\u00a0[3]: Copied! <pre>totsch_table_3 = [\n    [1, \"3467175\", 0.99763, 15087, 36, 7544, 18, 18, 7544],\n    [2, \"3394520\", 0.99672, 15073, 50, 7537, 25, 25, 7537],\n    [3, \"3338942\", 0.99596, 15062, 61, 7531, 31, 31, 7531],\n    [4, \"3339018\", 0.99512, 15049, 74, 7525, 37, 37, 7525],\n    [5, \"3338836\", 0.99498, 15047, 76, 7524, 38, 38, 7524],\n    [6, \"3429037\", 0.9938, 15029, 94, 7515, 47, 47, 7515],\n    [7, \"3346448\", 0.99296, 15017, 106, 7509, 53, 53, 7509],\n    [8, \"3338664\", 0.99296, 15017, 106, 7509, 53, 53, 7509],\n    [9, \"3338358\", 0.99282, 15014, 109, 7507, 55, 55, 7507],\n    [10, \"3339624\", 0.9924, 15008, 115, 7504, 58, 58, 7504],\n]\n\ntotsch_table_3_df = pd.DataFrame.from_records(\n    totsch_table_3,\n    columns=[\"Rank\", \"TeamId\", \"Score\", \"TP+TN\", \"FP+FN\", \"TP\", \"FN\", \"FP\", \"TN\"],\n    index=\"Rank\",\n)\n\ntotsch_table_3_df\n</pre> totsch_table_3 = [     [1, \"3467175\", 0.99763, 15087, 36, 7544, 18, 18, 7544],     [2, \"3394520\", 0.99672, 15073, 50, 7537, 25, 25, 7537],     [3, \"3338942\", 0.99596, 15062, 61, 7531, 31, 31, 7531],     [4, \"3339018\", 0.99512, 15049, 74, 7525, 37, 37, 7525],     [5, \"3338836\", 0.99498, 15047, 76, 7524, 38, 38, 7524],     [6, \"3429037\", 0.9938, 15029, 94, 7515, 47, 47, 7515],     [7, \"3346448\", 0.99296, 15017, 106, 7509, 53, 53, 7509],     [8, \"3338664\", 0.99296, 15017, 106, 7509, 53, 53, 7509],     [9, \"3338358\", 0.99282, 15014, 109, 7507, 55, 55, 7507],     [10, \"3339624\", 0.9924, 15008, 115, 7504, 58, 58, 7504], ]  totsch_table_3_df = pd.DataFrame.from_records(     totsch_table_3,     columns=[\"Rank\", \"TeamId\", \"Score\", \"TP+TN\", \"FP+FN\", \"TP\", \"FN\", \"FP\", \"TN\"],     index=\"Rank\", )  totsch_table_3_df  Out[3]: TeamId Score TP+TN FP+FN TP FN FP TN Rank 1 3467175 0.99763 15087 36 7544 18 18 7544 2 3394520 0.99672 15073 50 7537 25 25 7537 3 3338942 0.99596 15062 61 7531 31 31 7531 4 3339018 0.99512 15049 74 7525 37 37 7525 5 3338836 0.99498 15047 76 7524 38 38 7524 6 3429037 0.99380 15029 94 7515 47 47 7515 7 3346448 0.99296 15017 106 7509 53 53 7509 8 3338664 0.99296 15017 106 7509 53 53 7509 9 3338358 0.99282 15014 109 7507 55 55 7507 10 3339624 0.99240 15008 115 7504 58 58 7504 In\u00a0[4]: Copied! <pre>import prob_conf_mat\n\nstudy = prob_conf_mat.Study(\n    seed=0,\n    num_samples=20000,\n    ci_probability=0.95,\n)\n\nstudy.add_metric(\"tpr\")\nstudy.add_metric(\"tnr\")\n\nstudy.add_experiment(\n    experiment_name=\"7a\",\n    confusion_matrix=[\n        [totsch_table_2_df.loc[\"7a\"].TP, totsch_table_2_df.loc[\"7a\"].FN],\n        [totsch_table_2_df.loc[\"7a\"].FP, totsch_table_2_df.loc[\"7a\"].TN],\n    ],\n    prevalence_prior=1,\n    confusion_prior=1,\n)\n</pre> import prob_conf_mat  study = prob_conf_mat.Study(     seed=0,     num_samples=20000,     ci_probability=0.95, )  study.add_metric(\"tpr\") study.add_metric(\"tnr\")  study.add_experiment(     experiment_name=\"7a\",     confusion_matrix=[         [totsch_table_2_df.loc[\"7a\"].TP, totsch_table_2_df.loc[\"7a\"].FN],         [totsch_table_2_df.loc[\"7a\"].FP, totsch_table_2_df.loc[\"7a\"].TN],     ],     prevalence_prior=1,     confusion_prior=1, )  In\u00a0[5]: Copied! <pre>study[\"7a/7a\"].confusion_matrix  # type: ignore\n</pre> study[\"7a/7a\"].confusion_matrix  # type: ignore  Out[5]: <pre>array([[26,  0],\n       [ 2,  6]])</pre> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(\n    2,\n    1,\n    figsize=(7, 4),\n    sharex=True,\n)\n\n# TPR ==========================================================================\ntpr_samples = study.get_metric_samples(\n    metric=\"tpr\",\n    experiment_name=\"7a/7a\",\n    sampling_method=\"posterior\",\n).values[:, 0]\n\ntpr_point_estimate = study.get_metric_samples(\n    metric=\"tpr\",\n    experiment_name=\"7a/7a\",\n    sampling_method=\"input\",\n).values[:, 0]\n\nax[0].hist(tpr_samples, bins=50, alpha=0.5, density=True, label=\"Posterior\")\n\ncur_ylim = ax[0].get_ylim()\nax[0].vlines(\n    tpr_point_estimate,\n    cur_ylim[0],\n    cur_ylim[1],\n    color=\"black\",\n    linewidths=2,\n    label=\"Point Estimate\",\n)\nax[0].set_ylim(cur_ylim)\n\nax[0].set_yticks([])\n\nax[0].set_xlabel(\"TPR\")\n\n# TNR ==========================================================================\ntnr_samples = study.get_metric_samples(\n    metric=\"tnr\",\n    experiment_name=\"7a/7a\",\n    sampling_method=\"posterior\",\n).values[:, 0]\n\ntnr_point_estimate = study.get_metric_samples(\n    metric=\"tnr\",\n    experiment_name=\"7a/7a\",\n    sampling_method=\"input\",\n).values[:, 0]\n\nax[1].hist(tnr_samples, bins=50, alpha=0.5, density=True)\n\ncur_ylim = ax[1].get_ylim()\nax[1].vlines(tnr_point_estimate, cur_ylim[0], cur_ylim[1], color=\"black\", linewidths=2)\nax[1].set_ylim(cur_ylim)\n\nax[1].set_xlabel(\"TNR\")\n\n# Figure =======================================================================\nax[1].set_yticks([])\n\nax[1].set_xlim(-0.05, 1.05)\nax[1].xaxis.set_major_formatter(\"{x:.0%}\")\n\nax[0].legend()\n\nfig.supylabel(\"Probability Density\")\n\nfig.tight_layout()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(     2,     1,     figsize=(7, 4),     sharex=True, )  # TPR ========================================================================== tpr_samples = study.get_metric_samples(     metric=\"tpr\",     experiment_name=\"7a/7a\",     sampling_method=\"posterior\", ).values[:, 0]  tpr_point_estimate = study.get_metric_samples(     metric=\"tpr\",     experiment_name=\"7a/7a\",     sampling_method=\"input\", ).values[:, 0]  ax[0].hist(tpr_samples, bins=50, alpha=0.5, density=True, label=\"Posterior\")  cur_ylim = ax[0].get_ylim() ax[0].vlines(     tpr_point_estimate,     cur_ylim[0],     cur_ylim[1],     color=\"black\",     linewidths=2,     label=\"Point Estimate\", ) ax[0].set_ylim(cur_ylim)  ax[0].set_yticks([])  ax[0].set_xlabel(\"TPR\")  # TNR ========================================================================== tnr_samples = study.get_metric_samples(     metric=\"tnr\",     experiment_name=\"7a/7a\",     sampling_method=\"posterior\", ).values[:, 0]  tnr_point_estimate = study.get_metric_samples(     metric=\"tnr\",     experiment_name=\"7a/7a\",     sampling_method=\"input\", ).values[:, 0]  ax[1].hist(tnr_samples, bins=50, alpha=0.5, density=True)  cur_ylim = ax[1].get_ylim() ax[1].vlines(tnr_point_estimate, cur_ylim[0], cur_ylim[1], color=\"black\", linewidths=2) ax[1].set_ylim(cur_ylim)  ax[1].set_xlabel(\"TNR\")  # Figure ======================================================================= ax[1].set_yticks([])  ax[1].set_xlim(-0.05, 1.05) ax[1].xaxis.set_major_formatter(\"{x:.0%}\")  ax[0].legend()  fig.supylabel(\"Probability Density\")  fig.tight_layout()  In\u00a0[7]: Copied! <pre>import prob_conf_mat\n\nstudy = prob_conf_mat.Study(\n    seed=0,\n    num_samples=20000,\n    ci_probability=0.95,\n)\n\nstudy.add_metric(\"prevalence\")\nstudy.add_metric(\"tpr\")\nstudy.add_metric(\"tnr\")\n\nfor id, row in totsch_table_2_df.iterrows():\n    study.add_experiment(\n        experiment_name=str(id),\n        confusion_matrix=[[row.TP, row.FN], [row.FP, row.TN]],\n        prevalence_prior=1,\n        confusion_prior=1,\n    )\n</pre> import prob_conf_mat  study = prob_conf_mat.Study(     seed=0,     num_samples=20000,     ci_probability=0.95, )  study.add_metric(\"prevalence\") study.add_metric(\"tpr\") study.add_metric(\"tnr\")  for id, row in totsch_table_2_df.iterrows():     study.add_experiment(         experiment_name=str(id),         confusion_matrix=[[row.TP, row.FN], [row.FP, row.TN]],         prevalence_prior=1,         confusion_prior=1,     )  In\u00a0[8]: Copied! <pre>study\n</pre> study  Out[8]: <pre>Study(experiments=['1/1', '10/10', '11/11', '12/12', '13a/13a', '13b/13b', '14a/14a', '14b/14b', '15a/15a', '15b/15b', '16/16', '2/2', '3/3', '4a/4a', '4b/4b', '5a/5a', '5b/5b', '6a/6a', '6b/6b', '7a/7a', '7b/7b', '8/8', '9a/9a', '9b/9b']), metrics=MetricCollection([Metric(prevalence), Metric(tpr), Metric(tnr)]))</pre> In\u00a0[9]: Copied! <pre>mus = dict()\nfor metric in [\"prevalence\", \"tpr\", \"tnr\"]:\n    metric_summaries = study.report_metric_summaries(\n        metric=metric,\n        class_label=0,\n        table_fmt=\"pd\",\n    )\n\n    mus[metric] = metric_summaries.MU  # type: ignore\n</pre> mus = dict() for metric in [\"prevalence\", \"tpr\", \"tnr\"]:     metric_summaries = study.report_metric_summaries(         metric=metric,         class_label=0,         table_fmt=\"pd\",     )      mus[metric] = metric_summaries.MU  # type: ignore  In\u00a0[10]: Copied! <pre>import matplotlib.pyplot as plt\n\ncmap = plt.get_cmap(\"tab10\")\n\nfig, ax = plt.subplots(\n    2,\n    1,\n    figsize=(7, 4),\n    sharex=True,\n    height_ratios=[1, 3],\n)\n\n# N figure =====================================================================\nax[0].plot(totsch_table_2_df.N)\n\nax[0].set_yscale(\"log\")\n\nax[0].set_ylabel(\"Sample Size\")\n\n# MU figure ====================================================================\nax[1].plot(mus[\"prevalence\"], c=cmap(0), label=\"prevalence\")\nax[1].plot(mus[\"tpr\"], c=cmap(1), label=\"tpr\")\nax[1].plot(mus[\"tnr\"], c=cmap(2), label=\"tnr\")\n\nax[1].set_ylim(0, 1)\n\nax[1].legend()\n\nax[1].set_ylabel(\"Metric Uncertainty\")\n\nax[1].set_xticklabels(list(totsch_table_2_df.index), rotation=90)\n\nfig.show()\n</pre> import matplotlib.pyplot as plt  cmap = plt.get_cmap(\"tab10\")  fig, ax = plt.subplots(     2,     1,     figsize=(7, 4),     sharex=True,     height_ratios=[1, 3], )  # N figure ===================================================================== ax[0].plot(totsch_table_2_df.N)  ax[0].set_yscale(\"log\")  ax[0].set_ylabel(\"Sample Size\")  # MU figure ==================================================================== ax[1].plot(mus[\"prevalence\"], c=cmap(0), label=\"prevalence\") ax[1].plot(mus[\"tpr\"], c=cmap(1), label=\"tpr\") ax[1].plot(mus[\"tnr\"], c=cmap(2), label=\"tnr\")  ax[1].set_ylim(0, 1)  ax[1].legend()  ax[1].set_ylabel(\"Metric Uncertainty\")  ax[1].set_xticklabels(list(totsch_table_2_df.index), rotation=90)  fig.show()  <pre>/tmp/ipykernel_2301/2769521887.py:31: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax[1].set_xticklabels(list(totsch_table_2_df.index), rotation=90)\n</pre> In\u00a0[11]: Copied! <pre>import prob_conf_mat\n\nstudy = prob_conf_mat.Study(\n    seed=0,\n    num_samples=20000,\n    ci_probability=0.95,\n)\n\nstudy.add_metric(\"bm\")\n\nfor id, row in totsch_table_2_df.iterrows():\n    study.add_experiment(\n        experiment_name=str(id),\n        confusion_matrix=[[row.TP, row.FN], [row.FP, row.TN]],\n        prevalence_prior=1,\n        confusion_prior=1,\n    )\n</pre> import prob_conf_mat  study = prob_conf_mat.Study(     seed=0,     num_samples=20000,     ci_probability=0.95, )  study.add_metric(\"bm\")  for id, row in totsch_table_2_df.iterrows():     study.add_experiment(         experiment_name=str(id),         confusion_matrix=[[row.TP, row.FN], [row.FP, row.TN]],         prevalence_prior=1,         confusion_prior=1,     )  In\u00a0[12]: Copied! <pre>import matplotlib.pyplot as plt\n\ncmap = plt.get_cmap(\"tab10\")\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 4))\n\nupper_ticks = []\nfor i, experiment_name in enumerate(totsch_table_2_df.index):\n    metric_samples = study.get_metric_samples(\n        metric=\"bm\", experiment_name=experiment_name, sampling_method=\"posterior\"\n    )\n\n    parts = ax.violinplot(\n        metric_samples.values[:, 0],\n        positions=[i],\n        orientation=\"vertical\",\n        widths=0.8,\n        showmeans=False,\n        showmedians=False,\n        showextrema=False,\n        side=\"high\",\n    )\n\n    for pc in parts[\"bodies\"]:  # type: ignore\n        pc.set_facecolor(cmap(0))\n        pc.set_edgecolor(\"black\")\n        pc.set_alpha(0.90)\n\n    box_dict = ax.boxplot(\n        metric_samples.values[:, 0],\n        positions=[i],\n        orientation=\"vertical\",\n        showfliers=False,\n        notch=True,\n        patch_artist=True,\n    )\n\n    box_dict[\"medians\"][0].set_alpha(0)\n    box_dict[\"boxes\"][0].set_facecolor(\"black\")\n\n    r_deceptive = (metric_samples.values[:, 0] &lt; 0).mean()\n\n    upper_ticks.append(f\"{r_deceptive:.0%}\")\n\n# Add the expected rewards\nsecx = ax.secondary_xaxis(\"top\")\n\nsecx.set_xticks(\n    range(len(upper_ticks)),\n    rotation=90,\n    ha=\"center\",\n    labels=upper_ticks,\n)\n\nsecx.set_xlabel(f\"$r_{{\\\\text{{deceptive}}}}$\")\n\ncur_xlim = ax.get_xlim()\nax.hlines(0, cur_xlim[0], cur_xlim[1], colors=\"black\")\n\nax.set_xticklabels(totsch_table_2_df.index, rotation=90)\n\nax.set_ylim(-1, 1)\n\nax.set_ylabel(\"p(BM|D)\")\n\nax.yaxis.set_major_formatter(\"{x:.0%}\")\n\nax.set_xlabel(\"classifier\")\n\nfig.tight_layout()\n</pre> import matplotlib.pyplot as plt  cmap = plt.get_cmap(\"tab10\")  fig, ax = plt.subplots(1, 1, figsize=(7, 4))  upper_ticks = [] for i, experiment_name in enumerate(totsch_table_2_df.index):     metric_samples = study.get_metric_samples(         metric=\"bm\", experiment_name=experiment_name, sampling_method=\"posterior\"     )      parts = ax.violinplot(         metric_samples.values[:, 0],         positions=[i],         orientation=\"vertical\",         widths=0.8,         showmeans=False,         showmedians=False,         showextrema=False,         side=\"high\",     )      for pc in parts[\"bodies\"]:  # type: ignore         pc.set_facecolor(cmap(0))         pc.set_edgecolor(\"black\")         pc.set_alpha(0.90)      box_dict = ax.boxplot(         metric_samples.values[:, 0],         positions=[i],         orientation=\"vertical\",         showfliers=False,         notch=True,         patch_artist=True,     )      box_dict[\"medians\"][0].set_alpha(0)     box_dict[\"boxes\"][0].set_facecolor(\"black\")      r_deceptive = (metric_samples.values[:, 0] &lt; 0).mean()      upper_ticks.append(f\"{r_deceptive:.0%}\")  # Add the expected rewards secx = ax.secondary_xaxis(\"top\")  secx.set_xticks(     range(len(upper_ticks)),     rotation=90,     ha=\"center\",     labels=upper_ticks, )  secx.set_xlabel(f\"$r_{{\\\\text{{deceptive}}}}$\")  cur_xlim = ax.get_xlim() ax.hlines(0, cur_xlim[0], cur_xlim[1], colors=\"black\")  ax.set_xticklabels(totsch_table_2_df.index, rotation=90)  ax.set_ylim(-1, 1)  ax.set_ylabel(\"p(BM|D)\")  ax.yaxis.set_major_formatter(\"{x:.0%}\")  ax.set_xlabel(\"classifier\")  fig.tight_layout()  In\u00a0[13]: Copied! <pre>from prob_conf_mat import Study\n\nstudy = Study(\n    seed=0,\n    num_samples=100000,\n    ci_probability=0.95,\n)\n\nstudy.add_metric(metric=\"acc\")\n\nfor rank, row in totsch_table_3_df.iterrows():\n    study.add_experiment(\n        experiment_name=f\"{rank}/{row.TeamId}\",\n        confusion_matrix=[[row.TP, row.FN], [row.FP, row.TN]],\n        confusion_prior=1,\n        prevalence_prior=1,\n    )\n</pre> from prob_conf_mat import Study  study = Study(     seed=0,     num_samples=100000,     ci_probability=0.95, )  study.add_metric(metric=\"acc\")  for rank, row in totsch_table_3_df.iterrows():     study.add_experiment(         experiment_name=f\"{rank}/{row.TeamId}\",         confusion_matrix=[[row.TP, row.FN], [row.FP, row.TN]],         confusion_prior=1,         prevalence_prior=1,     )  In\u00a0[14]: Copied! <pre>study.report_listwise_comparison(metric=\"acc\")\n</pre> study.report_listwise_comparison(metric=\"acc\")  Out[14]: Group  Experiment    Rank 1  Rank 2  Rank 3  Rank 4  Rank 5  Rank 6  Rank 7  Rank 8  Rank 9  Rank 10 1      3467175       0.9297  0.0677  0.0025 2      3394520       0.0678  0.7916  0.1264  0.0090  0.0053 3      3338942       0.0024  0.1254  0.6522  0.1282  0.0902  0.0016 4      3339018        0.0137  0.1659  0.4415  0.3568  0.0191  0.0012  0.0013  0.0004 5      3338836        0.0016  0.0502  0.3609  0.4572  0.0997  0.0120  0.0124  0.0049   0.0010 6      3429037        0.0026  0.0518  0.0755  0.5209  0.1282  0.1281  0.0688   0.0241 8      3338664        0.0001  0.0070  0.0121  0.2024  0.2622  0.2572  0.1764   0.0825 7      3346448        0.0014  0.0024  0.0964  0.2524  0.2540  0.2381   0.1552 9      3338358        0.0002  0.0006  0.0445  0.2099  0.2116  0.2734   0.2598 10     3339624        0.0154  0.1341  0.1353  0.2379   0.4773 In\u00a0[15]: Copied! <pre>expected_reward = study.report_expected_reward(\n    metric=\"acc\",\n    rewards=[10000, 2000, 1000],\n    table_fmt=\"pd\",\n)\n\nexpected_reward = expected_reward.set_index([\"Group\", \"Experiment\"])\n\nexpected_reward\n</pre> expected_reward = study.report_expected_reward(     metric=\"acc\",     rewards=[10000, 2000, 1000],     table_fmt=\"pd\", )  expected_reward = expected_reward.set_index([\"Group\", \"Experiment\"])  expected_reward  Out[15]: E[Reward] Group Experiment 1 3467175 9435.19 2 3394520 2385.68 3 3338942 930.13 4 3339018 146.50 5 3338836 100.89 6 3429037 1.57 8 3338664 0.03 7 3346448 0.01 9 3338358 0.00 10 3339624 0.00 In\u00a0[16]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nlistwise_comparsion_result = study.get_listwise_comparsion_result(metric=\"acc\")\n\ncmap = plt.get_cmap(\"tab10\")\n\nfig, ax = plt.subplots(\n    2,\n    1,\n    figsize=(7, 4),\n    sharex=True,\n    height_ratios=[1, 1],\n)\n\n# Axis 0: p(acc) ===============================================================\nfor i, experiment_name in enumerate(listwise_comparsion_result.experiment_names):\n    metric_samples = study.get_metric_samples(\n        metric=\"acc\",\n        experiment_name=experiment_name,\n        sampling_method=\"posterior\",\n    )\n\n    parts = ax[0].violinplot(\n        metric_samples.values[:, 0],\n        positions=[i],\n        orientation=\"vertical\",\n        widths=0.8,\n        showmeans=False,\n        showmedians=False,\n        showextrema=False,\n    )\n\n    for pc in parts[\"bodies\"]:\n        pc.set_facecolor(cmap(i))\n        pc.set_edgecolor(\"black\")\n        pc.set_alpha(0.90)\n\n    box_dict = ax[0].boxplot(\n        metric_samples.values[:, 0],\n        positions=[i],\n        orientation=\"vertical\",\n        showfliers=False,\n        notch=True,\n        patch_artist=True,\n    )\n\n    box_dict[\"medians\"][0].set_alpha(0)\n    box_dict[\"boxes\"][0].set_facecolor(\"black\")\n\nax[0].set_ylim(0.99, 1)\nax[0].yaxis.set_major_formatter(\"{x:.1%}\")\n\nax[0].set_ylabel(\"p(Acc|D)\")\n\n# Add the expected rewards\nsecx = ax[0].secondary_xaxis(\"top\")\n\nsecx.set_xticks(\n    np.arange(10),\n    rotation=45,\n    ha=\"center\",\n    labels=[\n        f\"${expected_reward.loc[study._split_experiment_name(experiment_name)]['E[Reward]']:.2f}\"\n        for experiment_name in listwise_comparsion_result.experiment_names\n    ],\n)\n\n# Axis 1: p(rank|experiment) ===================================================\nfor i in range(listwise_comparsion_result.p_rank_given_experiment.shape[0]):\n    ax[1].plot(listwise_comparsion_result.p_rank_given_experiment[i, :], c=cmap(i))\n\nax[1].set_ylim(0, 1)\nax[1].yaxis.set_major_formatter(\"{x:.0%}\")\n\nax[1].set_xticks(np.arange(10))\nax[1].set_xticklabels(np.arange(10) + 1)\nax[1].set_xlim(-0.5, 9.5)\n\nax[1].set_ylabel(\"p(Rank|Experiment)\")\n\nax[1].set_xlabel(\"Leaderboard Position\")\n\nfig.align_ylabels(ax)\n\nfig.tight_layout()\n</pre> import numpy as np import matplotlib.pyplot as plt  listwise_comparsion_result = study.get_listwise_comparsion_result(metric=\"acc\")  cmap = plt.get_cmap(\"tab10\")  fig, ax = plt.subplots(     2,     1,     figsize=(7, 4),     sharex=True,     height_ratios=[1, 1], )  # Axis 0: p(acc) =============================================================== for i, experiment_name in enumerate(listwise_comparsion_result.experiment_names):     metric_samples = study.get_metric_samples(         metric=\"acc\",         experiment_name=experiment_name,         sampling_method=\"posterior\",     )      parts = ax[0].violinplot(         metric_samples.values[:, 0],         positions=[i],         orientation=\"vertical\",         widths=0.8,         showmeans=False,         showmedians=False,         showextrema=False,     )      for pc in parts[\"bodies\"]:         pc.set_facecolor(cmap(i))         pc.set_edgecolor(\"black\")         pc.set_alpha(0.90)      box_dict = ax[0].boxplot(         metric_samples.values[:, 0],         positions=[i],         orientation=\"vertical\",         showfliers=False,         notch=True,         patch_artist=True,     )      box_dict[\"medians\"][0].set_alpha(0)     box_dict[\"boxes\"][0].set_facecolor(\"black\")  ax[0].set_ylim(0.99, 1) ax[0].yaxis.set_major_formatter(\"{x:.1%}\")  ax[0].set_ylabel(\"p(Acc|D)\")  # Add the expected rewards secx = ax[0].secondary_xaxis(\"top\")  secx.set_xticks(     np.arange(10),     rotation=45,     ha=\"center\",     labels=[         f\"${expected_reward.loc[study._split_experiment_name(experiment_name)]['E[Reward]']:.2f}\"         for experiment_name in listwise_comparsion_result.experiment_names     ], )  # Axis 1: p(rank|experiment) =================================================== for i in range(listwise_comparsion_result.p_rank_given_experiment.shape[0]):     ax[1].plot(listwise_comparsion_result.p_rank_given_experiment[i, :], c=cmap(i))  ax[1].set_ylim(0, 1) ax[1].yaxis.set_major_formatter(\"{x:.0%}\")  ax[1].set_xticks(np.arange(10)) ax[1].set_xticklabels(np.arange(10) + 1) ax[1].set_xlim(-0.5, 9.5)  ax[1].set_ylabel(\"p(Rank|Experiment)\")  ax[1].set_xlabel(\"Leaderboard Position\")  fig.align_ylabels(ax)  fig.tight_layout()  In\u00a0[17]: Copied! <pre>fig.savefig(\n    \"../assets/figures/replication/totsch_fig_6.svg\",\n    bbox_inches=\"tight\",\n    pad_inches=0,\n    transparent=True,\n)\n</pre> fig.savefig(     \"../assets/figures/replication/totsch_fig_6.svg\",     bbox_inches=\"tight\",     pad_inches=0,     transparent=True, )  In\u00a0[18]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nimport prob_conf_mat\n\nstudy = prob_conf_mat.Study(seed=0, num_samples=20000, ci_probability=0.95)\n\nns = np.concatenate([np.arange(start=1, stop=10 + 1) * (10**i) for i in range(6)])\nfor i, val in enumerate(ns):\n    conf_mat = np.full(shape=(2, 2), fill_value=val, dtype=np.int32)\n\n    study.add_experiment(\n        experiment_name=f\"{i}\",\n        confusion_matrix=conf_mat,\n        prevalence_prior=0,\n        confusion_prior=0,\n    )\n\nstudy.add_metric(\"acc\")\n\n\nmetric_summaries = study.report_metric_summaries(\n    metric=\"acc\",\n    class_label=0,\n    table_fmt=\"pd\",\n)\n\n\ncmap = plt.get_cmap(\"tab10\")\n\nfig, ax = plt.subplots(\n    nrows=2,\n    ncols=1,\n    figsize=(7, 4),\n    # height_ratios=[1, 3],\n)\n\n# N figure\nax[0].scatter(x=ns[ns &lt;= 100], y=metric_summaries.MU[ns &lt;= 100], c=cmap(0))  # type: ignore\n\nxs = np.linspace(0, 100, num=1000)\npred_ys = np.pow(xs, -1 / study.num_classes)\n\nax[0].plot(xs, pred_ys, c=cmap(1))\n\nax[0].set_ylim(0, 1)\n\n# ax[0].set_yscale(\"log\")\n# plt.scatter(x=ns[ns &lt;= 100], y=)\n\nax[1].scatter(x=ns, y=metric_summaries.MU, c=cmap(0))  # type: ignore\n\nxs = np.linspace(1, max(ns), num=1000)\npred_ys = np.pow(xs, -1 / study.num_classes)\n\nax[1].plot(xs, pred_ys, c=cmap(1))\n\nax[1].set_xscale(\"log\")\nax[1].set_yscale(\"log\")\n\nfig.suptitle(\"Accuracy MU by N\")\n</pre> import numpy as np import matplotlib.pyplot as plt  import prob_conf_mat  study = prob_conf_mat.Study(seed=0, num_samples=20000, ci_probability=0.95)  ns = np.concatenate([np.arange(start=1, stop=10 + 1) * (10**i) for i in range(6)]) for i, val in enumerate(ns):     conf_mat = np.full(shape=(2, 2), fill_value=val, dtype=np.int32)      study.add_experiment(         experiment_name=f\"{i}\",         confusion_matrix=conf_mat,         prevalence_prior=0,         confusion_prior=0,     )  study.add_metric(\"acc\")   metric_summaries = study.report_metric_summaries(     metric=\"acc\",     class_label=0,     table_fmt=\"pd\", )   cmap = plt.get_cmap(\"tab10\")  fig, ax = plt.subplots(     nrows=2,     ncols=1,     figsize=(7, 4),     # height_ratios=[1, 3], )  # N figure ax[0].scatter(x=ns[ns &lt;= 100], y=metric_summaries.MU[ns &lt;= 100], c=cmap(0))  # type: ignore  xs = np.linspace(0, 100, num=1000) pred_ys = np.pow(xs, -1 / study.num_classes)  ax[0].plot(xs, pred_ys, c=cmap(1))  ax[0].set_ylim(0, 1)  # ax[0].set_yscale(\"log\") # plt.scatter(x=ns[ns &lt;= 100], y=)  ax[1].scatter(x=ns, y=metric_summaries.MU, c=cmap(0))  # type: ignore  xs = np.linspace(1, max(ns), num=1000) pred_ys = np.pow(xs, -1 / study.num_classes)  ax[1].plot(xs, pred_ys, c=cmap(1))  ax[1].set_xscale(\"log\") ax[1].set_yscale(\"log\")  fig.suptitle(\"Accuracy MU by N\")  <pre>/tmp/ipykernel_2301/2018670692.py:39: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  ax[0].scatter(x=ns[ns &lt;= 100], y=metric_summaries.MU[ns &lt;= 100], c=cmap(0))  # type: ignore\n/tmp/ipykernel_2301/2018670692.py:42: RuntimeWarning: divide by zero encountered in power\n  pred_ys = np.pow(xs, -1 / study.num_classes)\n/tmp/ipykernel_2301/2018670692.py:51: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  ax[1].scatter(x=ns, y=metric_summaries.MU, c=cmap(0))  # type: ignore\n</pre> Out[18]: <pre>Text(0.5, 0.98, 'Accuracy MU by N')</pre>"},{"location":"Explanation/totsch_replication.html#a-replication-of-totsch-n-hoffmann-d-2020-classifier-uncertainty-evidence-potential-impact-and-probabilistic-treatment","title":"A Replication of T\u00f6tsch, N. &amp; Hoffmann, D. (2020). 'Classifier uncertainty: evidence, potential impact, and probabilistic treatment'\u00b6","text":"<p>Note: the appearance of this notebook will depend on the environment and screen size you're using. If the tables are being clipped or the figures look off, consider trying Google Colab or Github via the buttons below. This notebook was created in VSCode, and will likely look best locally.</p> <p>In this notebook, we'll recreate all the figures presented in T\u00f6tsch &amp; Hoffmann (2020) using <code>prob_conf_mat</code>. While perfect replication is not possible, we'll see that all of the results are very similar to each other.</p> <p>This is not entirely surprising, since the synthetic confusion matrix sampling in <code>prob_conf_mat</code> is a multiclass generalisation of the model presented in T\u00f6tsch &amp; Hoffmann (2020).</p> <p>Besides serving as a sanity check, it further showcases how the library might be used to generate custom figures and analyses.</p>"},{"location":"Explanation/totsch_replication.html#preamble","title":"Preamble\u00b6","text":"<p>First, some setup code. T\u00f6tsch &amp; Hoffmann (2020) primarily use two different meta-datasets for their uncertainty quantification experiments. The first is a collection of unrelated binary classifiers, evaluated on relatively small datasets. It can be found under 'Table S1'.</p> <p>The second meta-dataset used comes from a Kaggle competition. While the evaluation data is kept hidden, T\u00f6tsch &amp; Hoffmann managed to reconstruct the sample size $N$, and the achieved accuracy scores from the public leaderboard. Since the evaluation metric is accuracy, we can just pretend the evaluation data and model responses are perfectly balanced, without affecting the analysis.</p>"},{"location":"Explanation/totsch_replication.html#figure-4","title":"Figure 4\u00b6","text":"<p>Figure 4 in T\u00f6tsch &amp; Hoffmann (2020) displays the posterior distribution and metric uncertainty for a cocaine purity classifier, corresponding to study 7a in Table S2.</p>"},{"location":"Explanation/totsch_replication.html#figure-3","title":"Figure 3\u00b6","text":"<p>Figure 3 in T\u00f6tsch &amp; Hoffmann (2020) displays the metric uncertainty (MU) for prevalence, the true positive rate (TPR) and the true negative rate (TNR). The figure clearly shows a negative correlation between the sample size and the metric uncertainty size for all 3 metrics (although some variation is present).</p>"},{"location":"Explanation/totsch_replication.html#figure-5","title":"Figure 5\u00b6","text":"<p>Figure 5 in T\u00f6tsch &amp; Hoffmann (2020) shows the posterior distributions for the informedness metric, for all experiments in Table S2. They also compute the $r_{\\text{deceptive}}$ score: the probability that the classifier achieves a score lower than 0.</p>"},{"location":"Explanation/totsch_replication.html#figure-6","title":"Figure 6\u00b6","text":"<p>Figure 6 of T\u00f6tsch &amp; Hoffmann (2020) depicts the posterior distributions of the accuracy scores various classifiers on the same test data, the probability that each model achieved a certain rank when compared to all other classifiers, and the expected prize money that each classifier should have received.</p> <p>Note that the expected reward differs somewhat. We attribute this to the fact that T\u00f6tsch &amp; Hoffmann used a different probabilistic model for accuracy scores specifically, whereas we compute it indirectly from the synthetic confusion matrices. Regardless, the differences are minor at best.</p>"},{"location":"Explanation/totsch_replication.html#figure-7","title":"Figure 7\u00b6","text":"<p>Figure 7 in T\u00f6tsch &amp; Hoffmann (2020) displays the inverse relationship of metric uncertainty and sample size. These come from simulated confusion matrices. They show that the metric uncertainty decreases according to the inverse square of the sample size.</p> <p>This is a little trickier to pull of in <code>prob_conf_mat</code>, since it requires circumventing the confusion matrix validation checks. So we first define a valid study with a single experiment, then we replace the validated confusion matrix with one of our own.</p>"},{"location":"Getting%20Started/01_estimating_uncertainty.html","title":"01. Estimating Uncertainty","text":"In\u00a0[1]: Copied! <pre>confusion_matrix = [\n    [13, 0, 0],\n    [0, 10, 6],\n    [0, 0, 9],\n]\n</pre> confusion_matrix = [     [13, 0, 0],     [0, 10, 6],     [0, 0, 9], ]  <p>Here, the true labels (the condition) appear on the rows, and the predicted labels across the columns. So for example, for class 1, 10 examples were predicted correctly, but in 6 cases the model confused it for class 2.</p> <p>Let's start analysing this confusion matrix.</p> <p>First we instantiate a <code>Study</code> object, which will handle interfacing with the library. We pass some initial parameters:</p> <ol> <li><code>seed</code>: the seed for the RNG, makes it reproducible</li> <li><code>num_samples</code>: this is the number of synthetic confusion matrices to sample. The higher, the less variable the study results will, but at the cost of increased memory footprint and computation time</li> <li><code>ci_probability</code>: the desired size of the credibility interval, a bit like the confidence intervals in frequentist statistics</li> </ol> In\u00a0[2]: Copied! <pre>from prob_conf_mat import Study\n\nstudy = Study(\n    seed=0,\n    num_samples=10000,\n    ci_probability=0.95,\n)\n\nstudy\n</pre> from prob_conf_mat import Study  study = Study(     seed=0,     num_samples=10000,     ci_probability=0.95, )  study  Out[2]: <pre>Study(experiments=[]), metrics=MetricCollection([]))</pre> <p>The <code>Study</code> object is currently empty. Let's add the confusion matrix to it now. Feel free to ignore the warning, this will only become important later.</p> In\u00a0[3]: Copied! <pre>study.add_experiment(experiment_name=\"test\", confusion_matrix=confusion_matrix)\n\nstudy\n</pre> study.add_experiment(experiment_name=\"test\", confusion_matrix=confusion_matrix)  study  <pre>/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/config.py:263: ConfigWarning: Experiment 'test/test's prevalence prior is `None`. Defaulting to the 0 (Haldane) prior.\n  warnings.warn(\n/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/config.py:350: ConfigWarning: Experiment 'test/test's confusion prior is `None`. Defaulting to the 0 (Haldane) prior.\n  warnings.warn(\n</pre> Out[3]: <pre>Study(experiments=['test/test']), metrics=MetricCollection([]))</pre> <p>Beyond an experiment, we also need to add some evaluation metric to the study. This will summarize the performance of the confusion matrix. For now, let's add the accuracy metric. It's not the best option available, but it't easy to understand.</p> In\u00a0[4]: Copied! <pre>study.add_metric(\"accuracy\")\n\nstudy\n</pre> study.add_metric(\"accuracy\")  study  Out[4]: <pre>Study(experiments=['test/test']), metrics=MetricCollection([Metric(accuracy)]))</pre> In\u00a0[5]: Copied! <pre>study.report_metric_summaries(metric=\"accuracy\", table_fmt=\"html\")\n</pre> study.report_metric_summaries(metric=\"accuracy\", table_fmt=\"html\")  Out[5]: Group  Experiment    Observed  Median  Mode       95.0% HDI    MU   Skew  Kurt test   test            0.8421  0.84840.8616[0.7291, 0.9484]0.2193-0.57500.3291 <p>This produced a table with lots of information already. Going across the columns we have:</p> <ol> <li>Group: the group to which the experiment belongs, feel free to ignore for now</li> <li>Experiment: the name of our experiment</li> <li>Point: the accuracy computed on the actual confusion matrix we passed</li> <li>Median: the median accuracy across all synthetic confusion matrices</li> <li>Mode: likewise, the mode (most common) accuracy score</li> <li>95.0% HDI: the edges of the credibility interval. This means that 95% of accuracy scores fell within the range [0.7231, 0.9452]</li> <li>MU: the Metric Uncertainty (MU), or the width of the credibility interval. The smaller, the better!</li> <li>Skew: the skewness of the distribution of accuracy scores. Here the value is negative, meaning values tend to bunch up towards the right side</li> <li>Kurt: the kurtosis of the distribution. Here the value is slightly positive, meaning the distribution has a slightly fatter tail than the standard normal distribution</li> </ol> <p>All together, this already gives us a more informed picture of the model's performance. An accuracy score of 0.8421 is pretty good, but given the small test dataset, it could be substantially smaller or larger, so we should take it with a grain of salt.</p> <p>As mentioned, accuracy is not the most informative metric. Luckily, many more evaluation metrics have been implemented. One common metric is the F1 score. Unlike accuracy, it provides a score for each class individually. This is a 'binary' metric. One common method for averaging the per-class performances is via the macro-average (just the arithmetic mean). Let's add both metrics to the study now.</p> In\u00a0[6]: Copied! <pre>study.add_metric(metric=\"f1@macro\")\n\nstudy.metrics\n</pre> study.add_metric(metric=\"f1@macro\")  study.metrics  Out[6]: <pre>OrderedDict([('accuracy', {}), ('f1@macro', {})])</pre> In\u00a0[7]: Copied! <pre>study.report_metric_summaries(metric=\"f1@macro\", table_fmt=\"html\")\n</pre> study.report_metric_summaries(metric=\"f1@macro\", table_fmt=\"html\")  Out[7]: Group  Experiment    Observed  Median  Mode       95.0% HDI    MU   Skew  Kurt test   test            0.8397  0.83910.8377[0.7309, 0.9456]0.2148-0.41020.0136 <p>Since F1 is a binary metric, we need to specify which class we want to look at. Let's start with class 0.</p> In\u00a0[8]: Copied! <pre>study.add_metric(\"f1\")\n\nstudy.metrics\n</pre> study.add_metric(\"f1\")  study.metrics  Out[8]: <pre>OrderedDict([('accuracy', {}), ('f1@macro', {}), ('f1', {})])</pre> In\u00a0[9]: Copied! <pre>study.report_metric_summaries(metric=\"f1\", class_label=0, table_fmt=\"html\")\n</pre> study.report_metric_summaries(metric=\"f1\", class_label=0, table_fmt=\"html\")  <pre>/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/stats/summary.py:89: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n  skew=stats.skew(posterior_samples),\n/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/stats/summary.py:90: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n  kurtosis=stats.kurtosis(posterior_samples),\n</pre> Out[9]: Group  Experiment    Observed  Median  Mode               95.0% HDI    MU  Skew  Kurt test   test            1.0000  1.00001.0000[1.0000e+00, 1.0000e+00]0.0000   nan   nan <p>Looks like something went wrong... Apparently there's a 100% chance that the F1 score is perfect. Given the size of the test set, we would expect to see some ambiguity.</p> In\u00a0[10]: Copied! <pre>study.add_experiment(\n    \"test\", confusion_matrix=confusion_matrix, confusion_prior=1.0, prevalence_prior=1.0\n)\n\nstudy.report_metric_summaries(metric=\"f1\", class_label=0, table_fmt=\"html\")\n</pre> study.add_experiment(     \"test\", confusion_matrix=confusion_matrix, confusion_prior=1.0, prevalence_prior=1.0 )  study.report_metric_summaries(metric=\"f1\", class_label=0, table_fmt=\"html\")  <pre>/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/experiment_group.py:132: UserWarning: Experiment 'test/test' already exists. Overwriting.\n  warn(\n</pre> Out[10]: Group  Experiment    Observed  Median  Mode       95.0% HDI    MU   Skew  Kurt test   test            1.0000  0.88200.9106[0.7428, 0.9710]0.2283-0.94741.2644 <p>Now when we ask for metric summaries, we get far more sensible output. Indeed, the F1 score for class 0 is still (probably) decent, but a perfect 1.0 seems somewhat high. We should expect to see scores between in the range [0.7465, 0.9755], with a good point estimate being 0.8816.</p> In\u00a0[11]: Copied! <pre>fig = study.plot_metric_summaries(metric=\"f1\", class_label=0);\n</pre> fig = study.plot_metric_summaries(metric=\"f1\", class_label=0);  <p>Admittedly, the default figure is a bit... bland. Luckily, the plots are highly configurable, making it easy to customize the look for your own down-stream reports/papers.</p> In\u00a0[12]: Copied! <pre>fig = study.plot_metric_summaries(\n    metric=\"f1\",\n    class_label=0,\n    method=\"histogram\",\n    bins=50,\n    edge_colour=\"tab:blue\",\n    area_colour=\"tab:blue\",\n    area_alpha=1.0,\n    hdi_lines_colour=\"white\",\n    median_line_format=\"-.\",\n    figsize=(6.9, 3.4),\n);\n</pre> fig = study.plot_metric_summaries(     metric=\"f1\",     class_label=0,     method=\"histogram\",     bins=50,     edge_colour=\"tab:blue\",     area_colour=\"tab:blue\",     area_alpha=1.0,     hdi_lines_colour=\"white\",     median_line_format=\"-.\",     figsize=(6.9, 3.4), );"},{"location":"Getting%20Started/01_estimating_uncertainty.html#01-estimating-uncertainty","title":"01. Estimating Uncertainty\u00b6","text":"<p>Note: the appearance of this notebook will depend on the environment and screen size you're using. If the tables are being clipped or the figures look off, consider trying Google Colab or Github via the buttons below. This notebook was created in VSCode, and will likely look best locally.</p>"},{"location":"Getting%20Started/01_estimating_uncertainty.html#setup","title":"Setup\u00b6","text":"<p>Let's pretend we just finished training a model, and we evaluated it on a tiny test data. The model's performance can be determined through the produced confusion matrix.</p>"},{"location":"Getting%20Started/01_estimating_uncertainty.html#reporting","title":"Reporting\u00b6","text":"<p>That's it for the setup! We've initiated a the <code>Study</code> object, added an experiment to it, and a metric by which we want to evaluate that experiment.</p> <p>Now we're ready to start the analysis. For now, let's just ask for some summary statistics. The <code>Study</code> object contains a variety of methods starting with <code>report_*</code>. These usually return a string or figure for use in a notebook.</p> <p>Let's try <code>report_metric_summaries</code>.</p>"},{"location":"Getting%20Started/01_estimating_uncertainty.html#prior-configuration","title":"Prior Configuration\u00b6","text":"<p>By default, the prior used in sampling the synthetic confusion matrices is 0, which is truly uninformative. However, this also means the synthetic confusion matrices will never include unseen (condition, prediction) combinations. We can fix this easily though by setting a (still uninformative) prior. Ignore the following error, it just means we're overwriting our existing experiment configuration.</p>"},{"location":"Getting%20Started/01_estimating_uncertainty.html#plotting","title":"Plotting\u00b6","text":"<p>Those numbers are nice and all, but it doesn't really provide use with an 'intuition' of what the distribution is like. Luckily, we've got that covered as well. Simply use the study's <code>plot_metric_summaries</code> instead we get a kernel-density estimate of the sampled metric distribution.</p> <p>The median is given by a dashed line, with smaller solid lines to its left and right giving the HDI range. On the extremes of the x-axis we see marks denoting the maximum and minimum sampled value. Finally, a diamond along the x-axis gives the true, observed metric value.</p>"},{"location":"Getting%20Started/01_estimating_uncertainty.html#next-steps","title":"Next Steps\u00b6","text":"<p>In the next tutorial, we'll cover how to perform significance tests between experiments, and against baselines.</p> <p>For more on configuring <code>Study</code> objects, metrics and confusion matrices:</p> <ul> <li>Check out the <code>Study</code> documentation to get a sense of what a <code>Study</code> can do</li> <li>Check out the how-to guide on <code>Study</code> configuration to discover more advanced configuration settings</li> <li>Check out the how-to guide on priors to find out which prior hyperparameters can be used, and which are recommended</li> <li>Check out the documentation on metrics &amp; averaging to figure out which metrics you can use out of the box</li> <li>See the explainer on generating confusion matrices</li> </ul>"},{"location":"Getting%20Started/02_comparing_experiments.html","title":"02. Comparing Experiments","text":"In\u00a0[1]: Copied! <pre>confusion_matrix_a = [\n    [2508, 70, 102, 42],\n    [22, 3820, 421, 36],\n    [14, 202, 3071, 11],\n    [37, 9, 6, 199],\n]\n\nconfusion_matrix_b = [\n    [2482, 75, 106, 59],\n    [30, 3813, 423, 33],\n    [22, 239, 3030, 7],\n    [34, 13, 13, 191],\n]\n</pre> confusion_matrix_a = [     [2508, 70, 102, 42],     [22, 3820, 421, 36],     [14, 202, 3071, 11],     [37, 9, 6, 199], ]  confusion_matrix_b = [     [2482, 75, 106, 59],     [30, 3813, 423, 33],     [22, 239, 3030, 7],     [34, 13, 13, 191], ]  <p>To assess how much better model A is relative to model B, we use a classification metric on the two confusion matrices, for example Matthew's Correlation Coefficient (MCC). If MCC is higher for model A, we can publish our paper, and get it accepted without reviewers complaining, right?</p> In\u00a0[2]: Copied! <pre>import prob_conf_mat as pcm\n\nstudy = pcm.Study(\n    seed=0,\n    num_samples=10000,\n    ci_probability=0.95,\n)\n\nstudy.add_experiment(\"a\", confusion_matrix_a, prevalence_prior=1.0, confusion_prior=1.0)\n\nstudy.add_experiment(\"b\", confusion_matrix_b, prevalence_prior=1.0, confusion_prior=1.0)\n\nstudy.add_metric(metric=\"mcc\")\n</pre> import prob_conf_mat as pcm  study = pcm.Study(     seed=0,     num_samples=10000,     ci_probability=0.95, )  study.add_experiment(\"a\", confusion_matrix_a, prevalence_prior=1.0, confusion_prior=1.0)  study.add_experiment(\"b\", confusion_matrix_b, prevalence_prior=1.0, confusion_prior=1.0)  study.add_metric(metric=\"mcc\")  In\u00a0[3]: Copied! <pre>study.report_metric_summaries(metric=\"mcc\")\n</pre> study.report_metric_summaries(metric=\"mcc\")  Out[3]: Group  Experiment    Observed  Median  Mode       95.0% HDI    MU   Skew  Kurt a      a               0.8641  0.86260.8635[0.8544, 0.8706]0.0162-0.09040.0979 b      b               0.8523  0.85100.8514[0.8424, 0.8594]0.0170-0.05880.0190 <p>Great! When estimating MCC scores, it seems model A does outperform model B, and by a margin of $0.0116$ median MCC. Before we get on with writing our manuscript, however, note that the credible intervals overlap at the 95% credibility level. So while model A is better, this is not always the case. Then how much better is model A actually?</p> In\u00a0[4]: Copied! <pre>study.plot_pairwise_comparison(\n    metric=\"mcc\",\n    experiment_a=\"a\",\n    experiment_b=\"b\",\n    min_sig_diff=0.0,\n);\n</pre> study.plot_pairwise_comparison(     metric=\"mcc\",     experiment_a=\"a\",     experiment_b=\"b\",     min_sig_diff=0.0, );  <p>The area in red corresponds to area where model B is better than model A, and the region in green where model A is better than model B. It's quite clear that model A is better most of the time, but is it enough to draw conclusions? Or do we need to test our models on a larger dataset? In other words, is this difference statistically significant, or just a 'lucky' outcome?</p> In\u00a0[5]: Copied! <pre>comparison_result = study.report_pairwise_comparison(\n    metric=\"mcc\",\n    experiment_a=\"a\",\n    experiment_b=\"b\",\n    min_sig_diff=0.00,\n)\n\nprint(comparison_result)\n</pre> comparison_result = study.report_pairwise_comparison(     metric=\"mcc\",     experiment_a=\"a\",     experiment_b=\"b\",     min_sig_diff=0.00, )  print(comparison_result)  <pre>Experiment a's mcc being greater than b could be considered 'likely'* (Median \u0394=0.0116, 95.00% HDI=[0.0005, 0.0237], p_direction=97.41%).\n\nThere is a 100.00% probability that this difference is bidirectionally significant (ROPE=[-0.0000, 0.0000], p_ROPE=0.00%).\n\nBidirectional significance could be considered 'certain'*.\n\nThere is a 97.41% probability that this difference is significantly positive (p_pos=97.41%, p_neg=2.59%).\n\n* These interpretations are based off of loose guidelines, and should change according to the application.\n</pre> <p>To test this, the first sentence of the above output is relevant. The median difference is $0.0116$, with the true difference falling somewhere in the range [0.0005, 0.0237] at the 95% credibility level. The $p_{\\text{direction}}$ statistic is most relevant, being somewhat comparable to the frequentist $p$-value. At a value of $97.41$%, we can conclude that model A being better than model B is probable. More data is always better, but currently there's only a $\\approx 2.50$% chance that model B is actually better.</p> In\u00a0[6]: Copied! <pre>study.plot_pairwise_comparison(\n    metric=\"mcc\",\n    experiment_a=\"a\",\n    experiment_b=\"b\",\n    min_sig_diff=0.005,\n);\n</pre> study.plot_pairwise_comparison(     metric=\"mcc\",     experiment_a=\"a\",     experiment_b=\"b\",     min_sig_diff=0.005, );  <p>All of a sudden there is a gray region around $0$. This is the ROPE. The green region is now the area corresponding to model A being practically significantly better than model B, and vice versa for the red region. While model A still appears to be better than model B, there is $13.32$% probability that that difference is practically 0, or in favour of model B.</p> In\u00a0[7]: Copied! <pre>comparison_result = study.report_pairwise_comparison(\n    metric=\"mcc\",\n    experiment_a=\"a\",\n    experiment_b=\"b\",\n    min_sig_diff=0.005,\n)\n\nprint(comparison_result)\n</pre> comparison_result = study.report_pairwise_comparison(     metric=\"mcc\",     experiment_a=\"a\",     experiment_b=\"b\",     min_sig_diff=0.005, )  print(comparison_result)  <pre>Experiment a's mcc being greater than b could be considered 'likely'* (Median \u0394=0.0116, 95.00% HDI=[0.0005, 0.0237], p_direction=97.41%).\n\nThere is a 86.68% probability that this difference is bidirectionally significant (ROPE=[-0.0050, 0.0050], p_ROPE=13.32%).\n\nBidirectional significance could be considered 'undecided'*.\n\nThere is a 86.38% probability that this difference is significantly positive (p_pos=86.38%, p_neg=0.30%).\n\nRelative to two random models (p_ROPE,random=47.27%) significance is 3.5488 times more likely.\n\n* These interpretations are based off of loose guidelines, and should change according to the application.\n</pre> <p>If we don't care about the direction of significance (either model A or model B is better), practical significance is mostly undecided at an $86.68$% probability. Most of this difference is in favour of model A ($86.38$%), though. When comparing to the difference between random models on the same dataset, we see that practical significance is $3.55$ times more likely than not. Interpreting this as a Bayes Factor, this is usually seen as weak evidence.</p> <p>All in all, while the difference likely does exist, it's not large enough to warrant celebration just yet. Of course, this is entirely dependent on the chosen minimally significant difference. If you decide that a much difference is already worth your time, the probabilities of practical significance will increase.</p> In\u00a0[8]: Copied! <pre>study.report_random_metric_summaries(metric=\"mcc\")\n</pre> study.report_random_metric_summaries(metric=\"mcc\")  Out[8]: Group  Experiment    Median   Mode        95.0% HDI    MU   Skew  Kurt a      a            -0.0001 0.0015[-0.0120, 0.0099]0.0219-0.01500.0012 b      b             0.0000-0.0004[-0.0109, 0.0111]0.0220 0.01390.0430 <p>And in fact, that is the value we (almost) retrieve.</p> <p>Given the previous results, it seems trivial to compare the trained classifiers to the random ones, but this is possible.</p> In\u00a0[9]: Copied! <pre>study.report_pairwise_comparison_to_random(metric=\"mcc\", min_sig_diff=0.05)\n</pre> study.report_pairwise_comparison_to_random(metric=\"mcc\", min_sig_diff=0.05)  Out[9]: Group  Experiment    Median \u0394  p_direction             ROPE  p_ROPE  p_sig a      a               0.8627       1.0000[-0.0500, 0.0500]  0.0000 1.0000 b      b               0.8510       1.0000[-0.0500, 0.0500]  0.0000 1.0000 <p>Here we can see that for both model A and model B, the achieved classification performance is much better than random.</p>"},{"location":"Getting%20Started/02_comparing_experiments.html#02-comparing-experiments","title":"02. Comparing Experiments\u00b6","text":"<p>Note: the appearance of this notebook will depend on the environment and screen size you're using. If the tables are being clipped or the figures look off, consider trying Google Colab or Github via the buttons below. This notebook was created in VSCode, and will likely look best locally.</p>"},{"location":"Getting%20Started/02_comparing_experiments.html#setup","title":"Setup\u00b6","text":"<p>Now consider the following common scenario. We train two different models, A and B, on the same dataset. Here model A is our brilliantly designed model, and B is the boring baseline. Now we want to test if our model (A) is a significant improvement over the baseline. To do so, we apply the two models to the held-out test set independently and get the following confusion matrices:</p>"},{"location":"Getting%20Started/02_comparing_experiments.html#bayesian-hypothesis-testing","title":"Bayesian Hypothesis Testing\u00b6","text":"<p>In the frequentist framework, situations like these call for hypothesis tests. For example, we want to compare the means of the different models' MCC scores, so we can run a t-test. But since we're not operating in the frequentist framework, and the distributions of the various metrics we compute likely invalidate the stringent assumptions that go along with these hypotheses tests, we instead have to a use a different framework.</p> <p>Kruschke et al. approach the problem from a Bayesian modelling perspective in 'Kruschke in Bayesian Estimation Supersedes the t Test'. If we assume our samples from the two estimated posterior predictive distributions to be correct, we can estimate the difference between the two experiments by simply taking the differences of their samples. The produced difference distribution looks something like this:</p>"},{"location":"Getting%20Started/02_comparing_experiments.html#the-region-of-practical-equivalence-rope","title":"The Region of Practical Equivalence (ROPE)\u00b6","text":"<p>That covers statistical significance, but what about practical significance? The apparent gain that model A books over model B is\u2026 modest to say the least.</p> <p>On a large enough test set, even very small differences become statistically significant, even when the models are practically equivalent. To test for practical significance, we can construct a 'Region of Practical Equivalence' (ROPE). This is some region bounded by two minimally significant values, outside which we may conclude that the difference between the models is worth noting. Within the ROPE, however, we assume that the difference is not relevant.</p> <p>For this example, a difference of at least $0.005$ MCC is needed for us to be convinced using model A is not a waste of time. We can now repeat the analysis, with a <code>min_sig_diff</code> parameter:</p>"},{"location":"Getting%20Started/02_comparing_experiments.html#comparing-to-a-random-baseline","title":"Comparing to a Random Baseline\u00b6","text":"<p>One important case to check is whether the trained classifier performs better than random. Early on during training, it won't be much better, but later on, the trained model should be much better.</p> <p>The random baseline largely depends on (1) the classification metric used, and (2) the label proportion. Luckily, the Study class can simulate performance of a random classifier on this dataset for any implemented metric. For example, if using the MCC metric, we would expect random performance to be 0 exactly.</p>"},{"location":"Getting%20Started/02_comparing_experiments.html#next-steps","title":"Next Steps\u00b6","text":"<p>In the next tutorial, we'll cover how to aggregate the results from related experiments into an experiment group.</p> <p>For more on statistical and effect size significance testing:</p> <ul> <li>Check out the <code>Study</code> documentation on comparing two studies</li> <li>Check out the how-to guide on comparing many experiments at once, and explore a simple case study</li> <li>Check out the <code>bayestestR</code> articles on Bayesian hypothesis testing, indices of effect and significance, the RoPE, etc.</li> </ul>"},{"location":"Getting%20Started/03_aggregating_experiments.html","title":"03. Aggregating Experiments","text":"In\u00a0[1]: Copied! <pre>fold_0 = [\n    [32, 11],\n    [0, 71],\n]\n\nfold_1 = [\n    [33, 10],\n    [1, 70],\n]\n\nfold_2 = [\n    [25, 17],\n    [0, 72],\n]\n\nfold_3 = [\n    [42, 0],\n    [24, 48],\n]\n\nfold_4 = [\n    [34, 8],\n    [3, 68],\n]\n</pre> fold_0 = [     [32, 11],     [0, 71], ]  fold_1 = [     [33, 10],     [1, 70], ]  fold_2 = [     [25, 17],     [0, 72], ]  fold_3 = [     [42, 0],     [24, 48], ]  fold_4 = [     [34, 8],     [3, 68], ]  <p>For now we'll add the confusion matrices as before, and use the adjusted Balanced Accuracy score as our metric.</p> In\u00a0[2]: Copied! <pre>import prob_conf_mat as pcm\n\nstudy = pcm.Study(\n    seed=0,\n    num_samples=10000,\n    ci_probability=0.95,\n)\n\nfor i, conf_mat in enumerate([fold_0, fold_1, fold_2, fold_3, fold_4]):\n    study.add_experiment(\n        f\"fold_{i}\",\n        conf_mat,\n        confusion_prior=1,\n        prevalence_prior=1,\n    )\n\nstudy.add_metric(metric=\"ba+adjusted=True\")\n\nfig = study.plot_metric_summaries(\n    metric=\"ba+adjusted=True\",\n    normalize=True,\n)\n</pre> import prob_conf_mat as pcm  study = pcm.Study(     seed=0,     num_samples=10000,     ci_probability=0.95, )  for i, conf_mat in enumerate([fold_0, fold_1, fold_2, fold_3, fold_4]):     study.add_experiment(         f\"fold_{i}\",         conf_mat,         confusion_prior=1,         prevalence_prior=1,     )  study.add_metric(metric=\"ba+adjusted=True\")  fig = study.plot_metric_summaries(     metric=\"ba+adjusted=True\",     normalize=True, )  In\u00a0[3]: Copied! <pre>study.report_metric_summaries(metric=\"ba+adjusted=True\")\n</pre> study.report_metric_summaries(metric=\"ba+adjusted=True\")  Out[3]: Group  Experiment    Observed  Median  Mode       95.0% HDI    MU   Skew   Kurt fold_0 fold_0          0.7442  0.72260.7282[0.5915, 0.8499]0.2584-0.3079 0.0694 fold_1 fold_1          0.7534  0.73220.7241[0.5976, 0.8548]0.2572-0.3055-0.0176 fold_2 fold_2          0.5952  0.57810.5694[0.4285, 0.7173]0.2888-0.1290-0.1465 fold_3 fold_3          0.6667  0.64120.6516[0.5259, 0.7573]0.2313-0.2383 0.0886 fold_4 fold_4          0.7673  0.74550.7678[0.6097, 0.8640]0.2543-0.4009 0.1590 <p>In a standard analysis, we would simply report the average of the 5 observed values and call it a day. However, given that the dataset is small, and yields wide distributions of metric values, this would only provide part of the picture. Ideally, we'd be able to report an average distribution of metric values. Intuitively, this distribution would be much more precise (i.e. narrow), and centered close to the various averages.</p> In\u00a0[4]: Copied! <pre>del study\n\nstudy = pcm.Study(\n    seed=0,\n    num_samples=10000,\n    ci_probability=0.95,\n)\n\nfor i, conf_mat in enumerate([fold_0, fold_1, fold_2, fold_3, fold_4]):\n    study.add_experiment(\n        f\"foo/fold_{i}\",\n        conf_mat,\n        confusion_prior=1,\n        prevalence_prior=1,\n    )\n</pre> del study  study = pcm.Study(     seed=0,     num_samples=10000,     ci_probability=0.95, )  for i, conf_mat in enumerate([fold_0, fold_1, fold_2, fold_3, fold_4]):     study.add_experiment(         f\"foo/fold_{i}\",         conf_mat,         confusion_prior=1,         prevalence_prior=1,     )  <p>And we'll also add the same metric again, but now with the 'aggregation' key specified.</p> In\u00a0[5]: Copied! <pre>study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"gaussian\")\n</pre> study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"gaussian\")  <p>We can now request the aggregated distribution as well.</p> In\u00a0[6]: Copied! <pre>fig = study.plot_experiment_aggregation(\n    metric=\"ba+adjusted=True\",\n    experiment_group=\"foo\",\n    normalize=True,\n)\n</pre> fig = study.plot_experiment_aggregation(     metric=\"ba+adjusted=True\",     experiment_group=\"foo\",     normalize=True, )  In\u00a0[7]: Copied! <pre>study.report_aggregated_metric_summaries(metric=\"ba+adjusted=True\")\n</pre> study.report_aggregated_metric_summaries(metric=\"ba+adjusted=True\")  Out[7]: Group    Median  Mode             HDI    MU  Kurtosis   Skew  Var. Within  Var. Between   I2 foo      0.68310.6774[0.6289, 0.7438]0.1149   -0.0121-0.0161       0.0045        0.00000.00% <p>Looking at the figure and the table, the aggregated distribution is indeed much narrower (the MU is roughly half the size of the individual experiment distributions) and both the observed values and the medians of their distributions are symmetrically spread around the aggregated median value of $0.7071$.</p> <p>The summary table now reports three additional metrics:</p> <ol> <li>Var. Within: measure the variance of the distributions within each experiment</li> <li>Var. Between: measure the variance between the different experiment distributions</li> <li>I2: is the ratio of the variance between distrubtions to the total variance These metrics have to do with the observed heterogeneity between the different distributions. Currently there is none ($I^2=4.28\\%$), but we'll experiment with a case where this is the case later.</li> </ol> In\u00a0[8]: Copied! <pre>study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"beta\")\n\nstudy.report_aggregated_metric_summaries(metric=\"ba+adjusted=True\")\n</pre> study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"beta\")  study.report_aggregated_metric_summaries(metric=\"ba+adjusted=True\")  Out[8]: Group    Median  Mode             HDI    MU  Kurtosis  Skew  Var. Within  Var. Between   I2 foo      0.68450.6821[0.6257, 0.7452]0.1196   -0.07940.0822       0.0044        0.00013.04% <p>Not really... although the new aggregate distribution is slightly negatively skewed as well now. Currently we've implemented the following aggregation methods:</p> In\u00a0[9]: Copied! <pre>from collections import defaultdict\n\nfrom prob_conf_mat.experiment_aggregation import AGGREGATION_REGISTRY\n\nmethod_to_alias = defaultdict(list)\nfor alias, method in AGGREGATION_REGISTRY.items():\n    method_to_alias[method].append(alias)\n\nfor method, aliases in method_to_alias.items():\n    print(f\"{method.__name__}: {aliases}\")\n</pre> from collections import defaultdict  from prob_conf_mat.experiment_aggregation import AGGREGATION_REGISTRY  method_to_alias = defaultdict(list) for alias, method in AGGREGATION_REGISTRY.items():     method_to_alias[method].append(alias)  for method, aliases in method_to_alias.items():     print(f\"{method.__name__}: {aliases}\")  <pre>SingletonAggregator: ['singleton', 'identity']\nBetaAggregator: ['beta', 'beta_conflation']\nGammaAggregator: ['gamma', 'gamma_conflation']\nFEGaussianAggregator: ['fe', 'fixed_effect', 'fe_gaussian', 'gaussian', 'normal', 'fe_normal']\nREGaussianAggregator: ['re', 'random_effect', 're_gaussian', 're_normal']\nHistogramAggregator: ['hist', 'histogram']\n</pre> <p>Which is best will depend on the parametric assumption one is willing to make, and the metric. For example, the 'histogram' method does not make a parametric assumption at all, and instead computes the conflated distribution using probability density estimates using the distribution's histogram. It fails whenever there are regions of non-overlap between different experiments. Since that isn't the case here, we safely use it:</p> In\u00a0[10]: Copied! <pre>study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"histogram\")\n\nstudy.report_aggregated_metric_summaries(metric=\"ba+adjusted=True\")\n</pre> study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"histogram\")  study.report_aggregated_metric_summaries(metric=\"ba+adjusted=True\")  Out[10]: Group    Median  Mode             HDI    MU  Kurtosis  Skew  Var. Within  Var. Between   I2 foo      0.68540.6835[0.6251, 0.7519]0.1267   -0.11190.1342       0.0044        0.00012.53% <p>Most metrics are bounded in the regions [0, 1] or [-1, 1]. In those cases, the Beta distribution is especially strong. Some metrics, though, are not bounded or only bounded on one side. For example, the diagnostic odds ratio (a ratio of ratios) is bounded by [0, $\\infty$). In that case, a Gamma distribution might be a better fit:</p> In\u00a0[11]: Copied! <pre>study.add_metric(metric=\"dor\", aggregation=\"gamma\")\n\nstudy.report_aggregated_metric_summaries(metric=\"dor\", class_label=0)\n</pre> study.add_metric(metric=\"dor\", aggregation=\"gamma\")  study.report_aggregated_metric_summaries(metric=\"dor\", class_label=0)  Out[11]: Group    Median  Mode                  HDI     MU  Kurtosis   Skew    Var. Within  Var. Between   I2 foo      5.30860.8144[6.3628e-09, 61.0066]61.0066    2.984612.07232702959667.0963        0.00000.00% <p>When looking at the distribution plot, however, it is clear that this is a particularly poorly behaved metric distribution, likely due to the fact that the confusion matrices have zero counts in the off-diagonal elements. These give non-finite results:</p> In\u00a0[12]: Copied! <pre>study.report_metric_summaries(metric=\"dor\", class_label=0)\n</pre> study.report_metric_summaries(metric=\"dor\", class_label=0)  <pre>/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/metrics/_metrics.py:976: RuntimeWarning: divide by zero encountered in divide\n  return tpr / fpr\n/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/metrics/_metrics.py:1166: RuntimeWarning: divide by zero encountered in divide\n  return plr / nlr\n</pre> Out[12]: Group  Experiment    Observed  Median      Mode           95.0% HDI       MU   Skew     Kurt foo    fold_0        inf     300.7750 3286.6797[19.3161, 4371.1336]4351.817550.76963056.8731 foo    fold_1        231.0000135.7813  167.3978 [15.8327, 696.0532] 680.220556.59594457.6553 foo    fold_2        inf     151.399025788.6734 [7.8530, 2122.3753]2114.522278.62236795.9909 foo    fold_3        inf     124.2630 1761.1212 [7.5551, 1742.3460]1734.790856.60833710.3558 foo    fold_4         96.3333 75.6193   48.4036 [13.3525, 256.1462] 242.7937 3.9357  30.4562 In\u00a0[13]: Copied! <pre>fig = study.plot_experiment_aggregation(\n    metric=\"dor\",\n    class_label=0,\n    experiment_group=\"foo\",\n    normalize=True,\n)\n</pre> fig = study.plot_experiment_aggregation(     metric=\"dor\",     class_label=0,     experiment_group=\"foo\",     normalize=True, )  <p>In that case, using the same metric, but log-transformed, we get a much easier distribution to aggregate:</p> In\u00a0[14]: Copied! <pre>study.add_metric(metric=\"log_dor\", aggregation=\"normal\")\n\nstudy.report_aggregated_metric_summaries(metric=\"log_dor\", class_label=0)\n</pre> study.add_metric(metric=\"log_dor\", aggregation=\"normal\")  study.report_aggregated_metric_summaries(metric=\"log_dor\", class_label=0)  Out[14]: Group    Median  Mode             HDI    MU  Kurtosis  Skew  Var. Within  Var. Between   I2 foo      4.84184.8560[3.9684, 5.6893]1.7209   -0.02970.0085       1.2940        0.00000.00% In\u00a0[15]: Copied! <pre>study.add_metric(metric=\"log_dor\", aggregation=\"normal\")\n\nfig = study.plot_experiment_aggregation(\n    metric=\"log_dor\",\n    class_label=0,\n    experiment_group=\"foo\",\n    normalize=True,\n)\n</pre> study.add_metric(metric=\"log_dor\", aggregation=\"normal\")  fig = study.plot_experiment_aggregation(     metric=\"log_dor\",     class_label=0,     experiment_group=\"foo\",     normalize=True, )  <pre>/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/metrics/_metrics.py:976: RuntimeWarning: divide by zero encountered in divide\n  return tpr / fpr\n/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/metrics/_metrics.py:1025: RuntimeWarning: divide by zero encountered in log\n  lplr = np.log(tpr) - np.log(fpr)\n/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/metrics/_metrics.py:1124: RuntimeWarning: divide by zero encountered in log\n  lnlr = np.log(fnr) - np.log(tnr)\n/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/metrics/_metrics.py:1166: RuntimeWarning: divide by zero encountered in divide\n  return plr / nlr\n</pre> In\u00a0[16]: Copied! <pre>fig = study.plot_forest_plot(metric=\"ba+adjusted=True\")\n</pre> fig = study.plot_forest_plot(metric=\"ba+adjusted=True\")  <p>The medians of the individual experiments are depicted using squares, and their HDI uing a horizontal line. The aggregated distribution is shown at the bottom using a large diamond for its median. The vertical line allows for easy comparison of the individual experiments to the aggregated median. As usual, there are lots of customization options available.</p> In\u00a0[17]: Copied! <pre>study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"normal\")\n\nfor i, conf_mat in enumerate([fold_0, fold_1, fold_2, fold_3, fold_4]):\n    study.add_experiment(\n        f\"bar/fold_{i}\",\n        conf_mat,\n        confusion_prior=1,\n        prevalence_prior=1,\n    )\n\noutlier = [\n    [164, 4],\n    [8, 288],\n]\n\nstudy.add_experiment(\n    experiment_name=\"bar/outlier\",\n    confusion_matrix=outlier,\n    prevalence_prior=1.0,\n    confusion_prior=1.0,\n)\n</pre> study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"normal\")  for i, conf_mat in enumerate([fold_0, fold_1, fold_2, fold_3, fold_4]):     study.add_experiment(         f\"bar/fold_{i}\",         conf_mat,         confusion_prior=1,         prevalence_prior=1,     )  outlier = [     [164, 4],     [8, 288], ]  study.add_experiment(     experiment_name=\"bar/outlier\",     confusion_matrix=outlier,     prevalence_prior=1.0,     confusion_prior=1.0, )  In\u00a0[18]: Copied! <pre>fig = study.plot_forest_plot(metric=\"ba+adjusted=True\")\n</pre> fig = study.plot_forest_plot(metric=\"ba+adjusted=True\")  <p>All of a sudden, the aggregated distribution lies in no-man's land. It's median does not lie within the HDI of any of the individual experiments. What's worse, the aggregated distribution is extremely narrow: we're extremely confident that the average metric lies in a region of values not seen in any of the other experiments!</p> <p>Note that the I2 score is also provided. In the case without the outlier, this score was still very low $\\approx3\\%$. With the outlier, however, it is now $\\approx 78\\%$. This means that the inter-experiment variance represents roughly 3/4 of the total variance in the different experiments, which corresponds to a high degree of heterogeneity.</p> <p>What can we do about it? The <code>REGaussianAggregator</code> aggregator employs a random-effects meta-analysis approach. It assumes the different experiments will have some degree of inter-experiment variance, and tries to explicitly correct for it.</p> In\u00a0[19]: Copied! <pre>study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"re_normal\")\n\nfig = study.plot_forest_plot(metric=\"ba+adjusted=True\")\n</pre> study.add_metric(metric=\"ba+adjusted=True\", aggregation=\"re_normal\")  fig = study.plot_forest_plot(metric=\"ba+adjusted=True\")  <p>When using it, we can immediately see the reduced effect of the outlier. While the aggregate distribution has still shifted somewhat, it now lies comfortably within the HDI of most experiments. Furthermore, the aggregate distribution is considerably wider, less precise.</p> <p>So, in cases where you might expect heterogeneity to be present, consider using the <code>re_gaussian</code> approach, as it's considerably more robust. Unfortunately, we have (not yet) implemented robust variants for the other aggregation methods.</p>"},{"location":"Getting%20Started/03_aggregating_experiments.html#03-aggregating-experiments","title":"03. Aggregating Experiments\u00b6","text":"<p>Note: the appearance of this notebook will depend on the environment and screen size you're using. If the tables are being clipped or the figures look off, consider trying Google Colab or Github via the buttons below. This notebook was created in VSCode, and will likely look best locally.</p>"},{"location":"Getting%20Started/03_aggregating_experiments.html#setup","title":"Setup\u00b6","text":"<p>In this example, we're considering another common scenario. Unlike the previous notebook, we ran several experiments using the same model, on different subsets of the same dataset (for example, through k-fold cross-validation). This gave us 5 independent folds, all of which together is going to say something about the average behaviour of the model on similar datasets.</p>"},{"location":"Getting%20Started/03_aggregating_experiments.html#experiment-aggregation","title":"Experiment Aggregation\u00b6","text":"<p>This is possible using a variety of methods. We use conflation, an operation that generates an aggregate distribution that minimizes the loss of information. This requires making a parametric assumption though. If we (roughly) know the distribution type of the metric distributions, we can compute the conflated distribution in closed form for many different distribution families. Usually, as long as the distributions aren't clumped near the metric boundaries, the normal/Gaussian distribution is a decent first guess.</p> <p>To get started we'll register the various confusion matrices again, but now to the same group (note the \"group/experiment\" structure).</p>"},{"location":"Getting%20Started/03_aggregating_experiments.html#parametric-assumptions","title":"Parametric Assumptions\u00b6","text":"<p>The parametric assumption we make can impact the shape and summary statistics of the final aggregated distribution. For example, the independent experiment distributions are all negatively skewed. We assumed the aggregated distribution to be a Gaussian, which has no skew or excess kurtosis. If instead, we had chosen a Beta distribution would we have gotten a substantially different result?</p>"},{"location":"Getting%20Started/03_aggregating_experiments.html#forest-plots","title":"Forest Plots\u00b6","text":"<p>While the <code>experiment_aggregation</code> plots are useful for visually establishing the shape of the metric distributions, and assessing to which degree the parametric assumptions made work, it is not the most efficient method for providing information about the experiments and their effect on the aggregated distribution.</p> <p>For comparing multiple experiment groups at a quick glance, a forest plot strikes a good balance between the <code>report_aggregated_metric_summaries</code> and <code>plot_experiment_aggregation</code> methods. It is commonly used in the quantitative meta-analysis literature.</p>"},{"location":"Getting%20Started/03_aggregating_experiments.html#dealing-with-heterogeneity","title":"Dealing with Heterogeneity\u00b6","text":"<p>Inter-exeperiment heterogeneity means that the individual experiments' medians lie far apart. So far we've only come across cases of little to no heterogeneity. There are instances, however, where one might expect substantially different metric distributions belonging to the same group. For example, if we're estimating the same model's performance on different datasets.</p> <p>We can model this by adding an outlier distribution. Experiment group 'bar' is identical to 'foo' for the first 5 experiments, but includes a final outlier as well. This outlier experiment is very precise, relative to the rest.</p>"},{"location":"Getting%20Started/03_aggregating_experiments.html#next-steps","title":"Next Steps\u00b6","text":"<p>In the next tutorial, we'll go into how to read confusion matrices from disk, how to save and load a <code>Study</code> object, and other input/output operations.</p> <p>For more on experiment aggregation:</p> <ul> <li>Check out the <code>Study</code> documentation on aggregating experiments</li> <li>Check out the reference documentation on implemented experiment aggregation methods</li> <li>Check out the explainer on the theory behind experiment aggregation and inter-experiment heterogeneity</li> </ul>"},{"location":"Getting%20Started/04_interfacing_with_the_filesystem.html","title":"04. Interfacing with the Filesystem","text":"In\u00a0[1]: Copied! <pre># Shows the contents of the './mnist_digits' directory\n%ls ./mnist_digits\n</pre> # Shows the contents of the './mnist_digits' directory %ls ./mnist_digits  <pre>mlp_0.csv  mlp_2.csv  mlp_4.csv  svm_1.csv  svm_3.csv\r\nmlp_1.csv  mlp_3.csv  svm_0.csv  svm_2.csv  svm_4.csv\r\n</pre> <p>First, let's configure a Study, where we analyze the models based on their average accuracy, F1 (binary, micro and macro aggregated) and MCC scores.</p> In\u00a0[2]: Copied! <pre>import prob_conf_mat as pcm\n\nstudy = pcm.Study(seed=0, num_samples=10000, ci_probability=0.95)\n\nstudy.add_metric(metric=\"acc\", aggregation=\"fe_gaussian\")\nstudy.add_metric(metric=\"f1\", aggregation=\"fe_gaussian\")\nstudy.add_metric(metric=\"f1@weighted\", aggregation=\"fe_gaussian\")\nstudy.add_metric(metric=\"f1@macro\", aggregation=\"fe_gaussian\")\nstudy.add_metric(metric=\"mcc\", aggregation=\"beta\")\n</pre> import prob_conf_mat as pcm  study = pcm.Study(seed=0, num_samples=10000, ci_probability=0.95)  study.add_metric(metric=\"acc\", aggregation=\"fe_gaussian\") study.add_metric(metric=\"f1\", aggregation=\"fe_gaussian\") study.add_metric(metric=\"f1@weighted\", aggregation=\"fe_gaussian\") study.add_metric(metric=\"f1@macro\", aggregation=\"fe_gaussian\") study.add_metric(metric=\"mcc\", aggregation=\"beta\")  <p>The Study object requires us to pass in a valid confusion matrix for each experiment. Luckily, some useful utilities are provided in the <code>prob_conf_mat.io</code> module. Using Python's standard <code>pathlib</code>, we can now iterate over all produced confusion matrices, and create an experiment for them.</p> <p>Make sure to provide a prior for the prevalence and confusion, otherwise we'll see a lot of warnings.</p> In\u00a0[3]: Copied! <pre>from pathlib import Path\n\nfrom prob_conf_mat.io import load_csv\n\n# Iterate over all found csv files\nfor file_path in sorted(Path(\"./mnist_digits\").glob(\"*.csv\")):\n    # Split the file name to recover the model and fold\n    model, fold = file_path.stem.split(\"_\")\n\n    # Load in the confusion matrix using the utility function\n    confusion_matrix = load_csv(location=file_path)\n\n    # Add the experiment to the study\n    study.add_experiment(\n        experiment_name=f\"{model}/fold_{fold}\",  # The name of the experiment group and experiment\n        confusion_matrix=confusion_matrix,  # The confusion matrix\n        prevalence_prior=\"ones\",\n        confusion_prior=\"zeros\",\n    )\n</pre> from pathlib import Path  from prob_conf_mat.io import load_csv  # Iterate over all found csv files for file_path in sorted(Path(\"./mnist_digits\").glob(\"*.csv\")):     # Split the file name to recover the model and fold     model, fold = file_path.stem.split(\"_\")      # Load in the confusion matrix using the utility function     confusion_matrix = load_csv(location=file_path)      # Add the experiment to the study     study.add_experiment(         experiment_name=f\"{model}/fold_{fold}\",  # The name of the experiment group and experiment         confusion_matrix=confusion_matrix,  # The confusion matrix         prevalence_prior=\"ones\",         confusion_prior=\"zeros\",     )  <p>When we inspect the study, we see that all the necessary experiments and metrics have been introduced.</p> In\u00a0[4]: Copied! <pre>study\n</pre> study  Out[4]: <pre>Study(experiments=['mlp/fold_0', 'mlp/fold_1', 'mlp/fold_2', 'mlp/fold_3', 'mlp/fold_4', 'svm/fold_0', 'svm/fold_1', 'svm/fold_2', 'svm/fold_3', 'svm/fold_4']), metrics=MetricCollection([Metric(acc), Metric(f1), AveragedMetric(f1@weighted), AveragedMetric(f1@macro), Metric(mcc)]))</pre> In\u00a0[5]: Copied! <pre>report_1 = study.report_aggregated_metric_summaries(metric=\"f1@macro\")\n\nreport_1\n</pre> report_1 = study.report_aggregated_metric_summaries(metric=\"f1@macro\")  report_1  Out[5]: Group    Median  Mode             HDI    MU  Kurtosis   Skew  Var. Within  Var. Between    I2 mlp      0.85840.8578[0.8488, 0.8680]0.0192    0.0050 0.0067       0.0001        0.000136.65% svm      0.85440.8527[0.8446, 0.8639]0.0192    0.0055-0.0298       0.0001        0.000135.08% <p>Well, it seems like the MLP is slightly better on average, but there is quite a bit of overlap between the HDIs. When inspecting each experiment in isolation, we also see this:</p> In\u00a0[6]: Copied! <pre>study.plot_forest_plot(metric=\"f1@macro\");\n</pre> study.plot_forest_plot(metric=\"f1@macro\");  <p>The real question though, is this difference significant or not? How certain can we be that the MLP outperforms the SVM? For this, we can request comparisons between the experiment aggregates. Let's say that any difference smaller than $0.005$ is deemed practically equivalent (i.e., we assume that's a difference of 0).</p> In\u00a0[7]: Copied! <pre>print(\n    study.report_pairwise_comparison(\n        metric=\"f1@macro\",\n        experiment_a=\"mlp/aggregated\",\n        experiment_b=\"svm/aggregated\",\n        min_sig_diff=0.005,\n    ),\n)\n</pre> print(     study.report_pairwise_comparison(         metric=\"f1@macro\",         experiment_a=\"mlp/aggregated\",         experiment_b=\"svm/aggregated\",         min_sig_diff=0.005,     ), )  <pre>Experiment mlp/aggregated's f1@macro being greater than svm/aggregated could be considered 'dubious'* (Median \u0394=0.0041, 95.00% HDI=[-0.0100, 0.0171], p_direction=72.15%).\n\nThere is a 54.06% probability that this difference is bidirectionally significant (ROPE=[-0.0050, 0.0050], p_ROPE=45.94%).\n\nBidirectional significance could be considered 'undecided'*.\n\nThere is a 44.29% probability that this difference is significantly positive (p_pos=44.29%, p_neg=9.77%).\n\nRelative to two random models (p_ROPE,random=60.21%) significance is 1.3106 times more likely.\n\n* These interpretations are based off of loose guidelines, and should change according to the application.\n</pre> In\u00a0[8]: Copied! <pre>study.plot_pairwise_comparison(\n    metric=\"f1@macro\",\n    experiment_a=\"mlp/aggregated\",\n    experiment_b=\"svm/aggregated\",\n    min_sig_diff=0.005,\n);\n</pre> study.plot_pairwise_comparison(     metric=\"f1@macro\",     experiment_a=\"mlp/aggregated\",     experiment_b=\"svm/aggregated\",     min_sig_diff=0.005, );  <pre>/home/runner/work/prob_conf_mat/prob_conf_mat/src/prob_conf_mat/study.py:1856: UserWarning: Parameter `plot_obs_point` is True, but one of the experiments has no observation (i.e. aggregated). As a result, no observed difference will be shown.\n  warnings.warn(\n</pre> <p>Well, it turns out that the MLP is not really that much better. With the present amount of information, the probability of it being the better model is worse than a coin flip. To be sure of anything, we'll have to repeat the experiment with (much) more data.</p> In\u00a0[9]: Copied! <pre>from pprint import pprint\n\nstudy_config = study.to_dict()\n\npprint(study_config, indent=2, width=100, depth=3, sort_dicts=False)\n</pre> from pprint import pprint  study_config = study.to_dict()  pprint(study_config, indent=2, width=100, depth=3, sort_dicts=False)  <pre>{ 'seed': 0,\n  'num_samples': 10000,\n  'ci_probability': 0.95,\n  'experiments': { 'mlp': { 'fold_0': {...},\n                            'fold_1': {...},\n                            'fold_2': {...},\n                            'fold_3': {...},\n                            'fold_4': {...}},\n                   'svm': { 'fold_0': {...},\n                            'fold_1': {...},\n                            'fold_2': {...},\n                            'fold_3': {...},\n                            'fold_4': {...}}},\n  'metrics': { 'acc': {'aggregation': 'fe_gaussian'},\n               'f1': {'aggregation': 'fe_gaussian'},\n               'f1@weighted': {'aggregation': 'fe_gaussian'},\n               'f1@macro': {'aggregation': 'fe_gaussian'},\n               'mcc': {'aggregation': 'beta'}}}\n</pre> <p>This is a standard Python <code>dict</code>, and can be saved however you like (e.g., as a human readable YAML or TOML file, as a pickle object, as a JSON file, etc.). This can then be shared with others to replicate your work.</p> <p>Specifically, we can recreate a study by using the <code>.from_dict</code> classmethod:</p> In\u00a0[10]: Copied! <pre>new_study = pcm.Study.from_dict(study_config)\n</pre> new_study = pcm.Study.from_dict(study_config)  <p>The new study will be configured in the same way as before:</p> In\u00a0[11]: Copied! <pre>new_study.report_aggregated_metric_summaries(metric=\"f1@macro\")\n</pre> new_study.report_aggregated_metric_summaries(metric=\"f1@macro\")  Out[11]: Group    Median  Mode             HDI    MU  Kurtosis   Skew  Var. Within  Var. Between    I2 mlp      0.85840.8578[0.8488, 0.8680]0.0192    0.0050 0.0067       0.0001        0.000136.65% svm      0.85440.8527[0.8446, 0.8639]0.0192    0.0055-0.0298       0.0001        0.000135.08% <p>Please note that the state of RNG is not saved. This means that generally speaking, the results will not be exactly the same. This is because the order in which experiments and metrics are added to the study, as well as the order in which report elements are generated will influence the RNG's state. In this case, the order in which different experiments and metrics were added, and which report elements we requested when is the same as before, the output is same as well.</p> In\u00a0[12]: Copied! <pre>report_1\n</pre> report_1  Out[12]: Group    Median  Mode             HDI    MU  Kurtosis   Skew  Var. Within  Var. Between    I2 mlp      0.85840.8578[0.8488, 0.8680]0.0192    0.0050 0.0067       0.0001        0.000136.65% svm      0.85440.8527[0.8446, 0.8639]0.0192    0.0055-0.0298       0.0001        0.000135.08% <p>Generally, this will not be the same. However, with a large enough <code>num_samples</code> parameter, this should not affect the study's outcomes.</p>"},{"location":"Getting%20Started/04_interfacing_with_the_filesystem.html#04-interfacing-with-the-filesystem","title":"04. Interfacing with the Filesystem\u00b6","text":"<p>Note: the appearance of this notebook will depend on the environment and screen size you're using. If the tables are being clipped or the figures look off, consider trying Google Colab or Github via the buttons below. This notebook was created in VSCode, and will likely look best locally.</p>"},{"location":"Getting%20Started/04_interfacing_with_the_filesystem.html#setup","title":"Setup\u00b6","text":"<p>So far, we've created all of the confusion matrices on the fly, directly in the notebook. This is not really realistic; most of the time a confusion matrix is created in some validation loop in a different environment than the one we'll be using to do analysis on the results. In this notebook, we'll load in some confusion matrices from the filesystem, and we'll save the report configuration to the filesystem for easy reproducibility.</p> <p>As an example, let's say we conducted an experiment with the MNIST digits dataset, where we compare the performance of an MLP against that of an SVM. We performed 5 fold cross-validation, and save each produced test split confusion matrix as '{$MODEL}_{$FOLD}.csv' files.</p>"},{"location":"Getting%20Started/04_interfacing_with_the_filesystem.html#analysis","title":"Analysis\u00b6","text":"<p>And now, for a bit of analysis. If you have gone through the previous tutorials, this should be repetitive for you.</p> <p>Without further ado, let's look at the average performance of the models.</p>"},{"location":"Getting%20Started/04_interfacing_with_the_filesystem.html#saving-the-study","title":"Saving the Study\u00b6","text":"<p>Now let's say you wanted to share your analysis with a colleague. You could share all of the necessary files as a directory, or equivalently, you could just provide the study's configuration. We can access the configuration as a Python dict using the <code>.to_dict()</code> method.</p>"},{"location":"Getting%20Started/04_interfacing_with_the_filesystem.html#next-steps","title":"Next Steps\u00b6","text":"<p>This was the last tutorial. You should now be able to use <code>prob_conf_mat</code> with some confidence. The other documentation sections will help deepen you understanding of the library, when you're ready for it.</p> <p>For more on IO:</p> <ul> <li>Check out the reference documentation on implemented IO methods</li> </ul> <p>For more advanced material:</p> <ul> <li>Check out the how-to guide on extending the library with your own metrics or your own averaging, experiment aggregation, or IO methods</li> <li>Check out the replication case-study for some advanced use-cases and custom plots</li> </ul>"},{"location":"How%20To%20Guides/choosing_a_prior.html","title":"Priors","text":"<p>tldr: few classes, use 1 as a prior value. Many classes and enough data, use 0.</p> <p>In <code>prob_conf_mat</code>, we use a probabilistic model of the confusion matrix to sample synthetic confusion matrices. Specifically, we use the product of several Dirichlet-Categorical distributions to sample counterfactuals. These distributions are constrained by the data in the true confusion matrix, and in Bayesian statistics would be called a posterior distribution; the distribution that arises when we combine knowledge we know a priori with evidence.</p> <p>The knowledge we know a priori is summarized as a prior distribution, and represents the knowledge contained in the model before any evidence is taken into account.</p> <p>In <code>prob_conf_mat</code>, each experiment is equipped with two priors:</p> <ol> <li>the <code>prevalence_prior</code> (\\(\\text{num_classes}\\)), which tells us how often we expect each condition to appear in the data</li> <li>the <code>confusion_prior</code> (\\(\\text{num_classes}\\times \\text{num_classes}\\)), which tells us how often we expect the model predict each class, given the ground-truth condition</li> </ol> <p>These can be useful to inject external knowledge into our analyses. For example, if we know that a certain disease is present in 1% of the population, but our disease detector is evaluated on balanced data (50/50) then we can use the prior distribution to take the true disease prevalence into account. As a result, our performance metrics automatically shift to reflect performance under the true occurrence rate.</p> <p>Most of the time, however, we don't have any prior information. In this case, we want to choose a prior that has minimal effect on our analysis, while still regularizing the posterior a little. In Bayesian statistics terms, we want an uninformative prior. What does or does not constitute an uninformative prior is a matter of continued debate, but values of all 0.0, 0.5 or 1.0 are common choices for the Beta-Binomial model (the univariate special case of the Dirichlet-Categorical model).</p> <p>To set priors in <code>prob_conf_mat</code>, you can pass either:</p> <ol> <li>a <code>str</code> corresponding to a registered prior strategy</li> <li>a scalar numeric (<code>float | int</code>), in which all parameters will be set to that value</li> <li>an <code>ArrayLike</code> of numerics, in which the prior parameters are copied from the passed <code>ArrayLike</code></li> </ol> <p>See <code>prob_conf_mat.stats.dirichlet_distribution.dirichlet_prior</code> for more information.</p>"},{"location":"How%20To%20Guides/choosing_a_prior.html#effect-of-uninformative-priors-on-metric-samples","title":"Effect of Uninformative Priors on Metric Samples","text":"<p>It is a feature of the confusion matrix model that the priors of the metric distributions are generally not flat, despite uninformative priors being used. Intuitively, this makes sense. A model that is equally likely to predict any class, regardless of the ground-truth condition, is likely not a good model.</p> <p>This can be seen in the following figure. It displays the empirical distribution of some metrics given a prior of all 1s, across various different number of classes.</p> <p></p> <p>In all cases, the distributions are centered around the value corresponding to random performance, although this differs for each metric. As, the number of classes increases, however, most of the prior distributions become increasingly narrow. This means, even an uninformative prior can have undue influence on the eventual posterior distribution in higher dimensions.</p> <p>This is similar to using a prior with a much larger constant value (implying a greater degree of prior information). The following figure shows this:</p> <p></p> <p>Like before, the metric prior distributions are all centered around the random performance value (the number of classes is held constant at 2), and the priors become more narrow as the prior value (\\(N\\)) increases.</p> <p>Creating a truly uninformative prior (i.e., a flat distribution) for each metric is likely not possible with the synthetic confusion matrix sampling procedure <code>bayes_conf_mat</code> uses. For experiments with relatively few classes, we can get away with a standard prior, like the Bayes-Laplace prior of all 1s. For experiments with large number of classes, however, any value we choose will lead to a narrow and peaked metric prior, and will have a large effect on our analysis. Hence, we recommend the following guidelines:</p> <ol> <li>If you have very few classes (\\(&lt;4\\)), set prior parameters like 0.5 or 1. These will regularize the posterior metric distributions a bit, but not by an undue amount</li> <li>If you have many classes, and a lot of data, use 0 as your prior value. The posterior will not be regularized, but we expect the data to dominate anyway</li> <li>If you have many classes, but only a small amount of data, either set a small prior value, or use 0 and interpret your analysis with a grain of salt</li> </ol>"},{"location":"How%20To%20Guides/comparing_many_experiments.html","title":"Compare Many Experiments","text":"<p>In Getting Started/02 Comparing Experiments, we discussed comparing two experiments against each other using a variety of methods. This provided lots of information about the probability of existence and significance, as well as visual intuition for how different the metric distributions of two different experiments are.</p> <p>Repeating this process for many experiments is tedious, however: the number of pairwise combinations scales factorially with the number of experiments!</p> <p>One setting where we can expect having to compare many experiments at once is in an open-source competition (e.g., Kaggle). This is exactly a scenario tested by T\u00f6tsch &amp; Hoffmann (2020)<sup>1</sup>, where they computed the expected winnings for the top-10 participants in the 'Recursion Cellular Image Classification' challenge, based on the probability that their classifier exceeded all other participants.</p>"},{"location":"How%20To%20Guides/comparing_many_experiments.html#competition-winnings","title":"Competition Winnings","text":"<p>While the test set confusion matrices are hidden, from the accuracy scores and the size of the test set we can recreate a confusion matrix that would have produced those accuracy scores. The table for the top 10 participants might look like this:</p> Rank TeamId Score TP+TN FP+FN TP FN FP TN 1 3467175 0.99763 15087 36 7544 18 18 7544 2 3394520 0.99672 15073 50 7537 25 25 7537 3 3338942 0.99596 15062 61 7531 31 31 7531 4 3339018 0.99512 15049 74 7525 37 37 7525 5 3338836 0.99498 15047 76 7524 38 38 7524 6 3429037 0.99380 15029 94 7515 47 47 7515 7 3346448 0.99296 15017 106 7509 53 53 7509 8 3338664 0.99296 15017 106 7509 53 53 7509 9 3338358 0.99282 15014 109 7507 55 55 7507 10 3339624 0.99240 15008 115 7504 58 58 7504 <p>Using the <code>Study.report_listwise_comparison</code> we can request a table with the probability that each competitor's accuracy score achieved a certain rank:</p> <pre><code>study.report_listwise_comparison(metric=\"acc\")\n</code></pre> Group Experiment Rank 1 Rank 2 Rank 3 Rank 4 Rank 5 Rank 6 Rank 7 Rank 8 Rank 9 Rank 10 1 3467175 0.9297 0.0677 0.0025 2 3394520 0.0678 0.7916 0.1264 0.0090 0.0053 3 3338942 0.0024 0.1254 0.6522 0.1282 0.0902 0.0016 4 3339018 0.0137 0.1659 0.4415 0.3568 0.0191 0.0012 0.0013 0.0004 5 3338836 0.0016 0.0502 0.3609 0.4572 0.0997 0.0120 0.0124 0.0049 0.0010 6 3429037 0.0026 0.0518 0.0755 0.5209 0.1282 0.1281 0.0688 0.0241 8 3338664 0.0001 0.0070 0.0121 0.2024 0.2622 0.2572 0.1764 0.0825 7 3346448 0.0014 0.0024 0.0964 0.2524 0.2540 0.2381 0.1552 9 3338358 0.0002 0.0006 0.0445 0.2099 0.2116 0.2734 0.2598 10 3339624 0.0154 0.1341 0.1353 0.2379 0.4773 <p>While we see no rank inversions, for many of the ranks, there is considerable ambiguity in many ranks. This indicates, that despite a large test set of \\(N=15.1k\\) images, the margins between the top competitors are so narrow that it's difficult to say definitively which team achieved which rank (especially when \\(r&gt;2\\)).</p> <p>The competition organizers offered a $10,000 prize for 1st place, a $2,000 prize for 2nd place and a $1,000 prize for 3rd place. Using these probabilities we can compute that expected prize money each competitor should have received in a fair division of the competition winnings. Using the study, we can compute this by calling <code>study.report_expected_reward(metric='acc', rewards=[10000, 2000, 1000])</code>:</p> Group Experiment E[Reward] 1 3467175 9435.19 2 3394520 2385.68 3 3338942 930.13 4 3339018 146.50 5 3338836 100.89 6 3429037 1.57 8 3338664 0.03 7 3346448 0.01 9 3338358 0.00 10 3339624 0.00 <p>So while ranks 1 &amp; 2 clearly did deserve the lion's share of the competition winnings, rank 4 &amp; 5 comparatively deserved substantially more than the 0 they received.</p> <p>The following figure (source code can be found in Explanation/A Replication of T\u00f6tsch, N. &amp; Hoffmann, D. (2020). 'Classifier uncertainty: evidence, potential impact, and probabilistic treatment' ) summarizes the situation:</p> <p> </p> <ol> <li> <p>T\u00f6tsch, N. &amp; Hoffmann, D. (2020). 'Classifier uncertainty: evidence, potential impact, and probabilistic treatment'\u00a0\u21a9</p> </li> </ol>"},{"location":"How%20To%20Guides/configuration.html","title":"Configuration","text":"<p>All interaction with experiments should ideally go through the <code>Study</code> class. It controls all sampling and experiment aggregation, and can produce reports about samples.</p> <p>Behind the scenes, the configuration of a study is controlled by its parent class: <code>Config</code>. It has 5 properties which can be set by the users, each of which is validated upon instantiation, and whenever it is set.</p>"},{"location":"How%20To%20Guides/configuration.html#seed","title":"<code>seed</code>","text":"<p>Type: <code>int | None</code></p> <p>This is the random seed used to initialize the RNG. If passing <code>None</code>, it defaults to the current time, in fractional seconds. For the purpose of reproducibility, however, it is recommended that you set this value yourself.</p> <p>It must be a positive integer.</p>"},{"location":"How%20To%20Guides/configuration.html#num_samples","title":"<code>num_samples</code>","text":"<p>Type: <code>int | None</code></p> <p>This is the number of synthetic confusion matrices to sample for each experiment. A higher value is better, since it reduces the amount of variation between different runs of the same study, but also more computationally expensive. Keep in mind that confusion matrices are square by definition, such that the computational and storage costs scales as,</p> \\[ \\mathcal{O}(\\mathtt{num\\_samples}\\cdot \\mathtt{num\\_classes}^2) \\] <p>Defaults to 10000, the minimum recommended value,</p> <p>It must be a strictly positive integer, so 0 is not allowed. If you want to use the input confusion matrix, you can 'sample' it using the <code>Study.get_metric_samples</code> method, with <code>sampling_method=\"input\"</code>.</p>"},{"location":"How%20To%20Guides/configuration.html#ci_probability","title":"<code>ci_probability</code>","text":"<p>Type: <code>float | None</code></p> <p>This is the size of the credibility intervals to compute. Defaults to 0.95, which is an arbitrary value, and should be carefully considered. For more information about the size of the credible interval, see this bayestestR article.</p> <p>It must be a float in the range \\((0, 1)\\). The extremes are not permitted, as these can lead to infinitesimal or infinite credible intervals, respectively.</p>"},{"location":"How%20To%20Guides/configuration.html#experiments","title":"<code>experiments</code>","text":"<p>Type: <code>dict[str, dict[str, dict[str, Any]]]</code></p> <p>This represents the experiments and experiment groups that comprise the study. These must be passed as a double nested <code>dict</code> that contains (1) the experiment group name, (2) the experiment name, (3) and finally any IO or prior hyperparameters. Defaults to an empty dict, in which case there are no experiments in the study.</p> <p>Given that this is a fairly convoluted data structure, the <code>Study.add_experiment</code> is provided, and should be much easier to use.</p> <p>An example of a valid <code>experiments</code> value is:</p> <pre><code>{\n    \"EXPERIMENT_GROUP_NAME\": {\n        \"EXPERIMENT_NAME\": {\n            # Int[ArrayLike, \"num_classes num_classes\"]\n            \"confusion_matrix\": [[...], ...],\n            \"confusion_prior\": 0,\n            \"prevalence_prior\": 1,\n        }\n    }\n}\n</code></pre> <p>The innermost <code>dict</code> is additionally checked to make sure it yields valid <code>Experiment</code> instances:</p> <ul> <li>there must be a valid confusion matrix under the <code>'confusion_matrix'</code> key</li> <li>the values under <code>'confusion_prior'</code> and <code>'prevalence_prior'</code> must be a valid Dirichlet prior strategy (see the <code>dirichlet_prior</code> method)</li> </ul>"},{"location":"How%20To%20Guides/configuration.html#metrics","title":"<code>metrics</code>","text":"<p>Type: <code>dict[str, dict[str, Any]]</code></p> <p>Finally, the metrics are represented as a nested <code>dict</code> that contains (1) the metric as metric syntax strings, (2) and any metric aggregation parameters. Defaults to an empty dict, in which case no metrics are added to the study.</p> <p>Similar to the <code>experiments</code> configuration, since this is a convoluted data structure, a more convenient <code>Study.add_metric</code> method is provided.</p> <p>An example of a valid <code>metrics</code> value is:</p> <pre><code>{\n    \"METRIC_SYNTAX_STRING\": {\n        \"aggregation\": \"AGGREGATION_NAME\",\n    }\n}\n</code></pre> <p>The innermost <code>dict</code> contains the necessary parameters to instantiate the <code>ExperimentAggregation</code> method. It can be empty, as long as there is only one experiment per experiment group. Otherwise, it must contain an <code>'aggregation'</code> key and value.</p>"},{"location":"How%20To%20Guides/configuration.html#accessing-the-configuration","title":"Accessing the configuration","text":"<p>The complete configuration can be parsed and written to a <code>dict</code> containing only built-in types. This is useful for serialization using JSON, YAML or TOML, for example. To access this config <code>dict</code>, simply use the <code>Study.to_dict</code> method.</p> <p>See for example, the <code>dict</code> produced in the Interfacing with the Filesystem notebook.</p> <pre><code>&gt;&gt;&gt; study.to_dict()\n{ 'seed': 0,\n  'num_samples': 10000,\n  'ci_probability': 0.95,\n  'experiments': { 'mlp': { 'fold_0': {...},\n                            'fold_1': {...},\n                            'fold_2': {...},\n                            'fold_3': {...},\n                            'fold_4': {...}},\n                   'svm': { 'fold_0': {...},\n                            'fold_1': {...},\n                            'fold_2': {...},\n                            'fold_3': {...},\n                            'fold_4': {...}}},\n  'metrics': { 'acc': {'aggregation': 'fe_gaussian'},\n               'f1': {'aggregation': 'fe_gaussian'},\n               'f1@weighted': {'aggregation': 'fe_gaussian'},\n               'f1@macro': {'aggregation': 'fe_gaussian'},\n               'mcc': {'aggregation': 'beta'}}}\n</code></pre> <p>Similarly, the study can be instantiated using the <code>Study.from_dict</code> method. Keep in mind that this is a classmethod.</p> <pre><code>study = Study.from_dict(config)\n</code></pre> <p>While this creates a study instance with the same configuration, its output will generally not be identical, since this depends on the order of operations. However, the performing the same operations in the same order with identical study instances should yield the same output.</p>"},{"location":"How%20To%20Guides/extending_the_library.html","title":"Extend the Library","text":"<p>While we aim to keep <code>prob_conf_mat</code> comprehensive enough to cover most use-cases, it's not possible for the library to be complete. For that reason, we have made it possible to easily extend the library with your own metrics, averaging methods and experiment aggregators.</p> <p>We do this with a metaclass system that:</p> <ol> <li>Enforces which methods and properties all subclasses should possess</li> <li>Automatically registers subclasses when these are defined</li> </ol> <p>In this guide, we outline some basic steps to help you implement these yourself.</p>"},{"location":"How%20To%20Guides/extending_the_library.html#metrics-averaging","title":"Metrics &amp; Averaging","text":""},{"location":"How%20To%20Guides/extending_the_library.html#metric","title":"Metric","text":"<ol> <li> <p>First import the base class, <code>Metric</code>, as:</p> <pre><code>from prob_conf_mat.metrics.abc import Metric\n</code></pre> </li> <li> <p>Then define your class:</p> <pre><code>from prob_conf_mat.metrics.abc import Metric\n\nclass FowlkesMallows(Metric):\n</code></pre> </li> <li> <p>Define the required class properties:</p> <ol> <li><code>full_name (str)</code>: the full, human-readable name</li> <li><code>is_multiclass (bool)</code>: whether the metric is defined only for binary classification, in which a result is return per class, or if it is also defined for multi-class classification, in which case only a single value is returned</li> <li><code>bounds (tuple[float, float])</code>: the minimum and maximum value. Use <code>float(inf)</code> to specify infinite values. Used for cross-experiment aggregation and plotting</li> <li><code>dependencies (tuple[str, ...])</code>: the name of any dependencies your metric might need. Make sure the dependencies have been implemented already (or implement them yourself). Used to build the computation graph. If there are no dependencies, leave an empty tuple</li> <li><code>sklearn_equivalent (str | None)</code>: the name of the sklearn equivalent function. Used for documentation and unit testing</li> <li><code>aliases (list[str])</code>: any aliases your metric might go by. Each alias must be unique, and should not be used by another metric</li> </ol> <p>For example:</p> <pre><code>from prob_conf_mat.metrics.abc import Metric\n\nclass FowlkesMallows(Metric):\n    full_name = \"Fowlkes Mallows Index\"\n    is_multiclass = False\n    bounds = (0.0, 1.0)\n    dependencies = (\"ppv\", \"tpr\")\n    sklearn_equivalent = \"fowlkes_mallows_index\"\n    aliases = [\"fowlkes_mallows\", \"fm\"]\n</code></pre> </li> <li> <p>Finally, implement how the method should be computed using the <code>compute_metric</code> method. The output should always have a dimensionality of <code>Float[ndarray, \" num_samples num_classes\"]</code> if it is binary, or <code>Float[ndarray, \" num_samples 1\"]</code> if it is multi-class.</p> <p>The Fowlkes-Mallows index is defined as the square root of the product of the precision and recall, so we would define it as follows:</p> <pre><code>from prob_conf_mat.metrics.abc import Metric\n\nclass FowlkesMallows(Metric):\n    full_name = \"Fowlkes Mallows Index\"\n    is_multiclass = False\n    bounds = (0.0, 1.0)\n    dependencies = (\"ppv\", \"tpr\")\n    sklearn_equivalent = \"fowlkes_mallows_index\"\n    aliases = [\"fowlkes_mallows\", \"fm\"]\n\n    def compute_metric(\n        self,\n        ppv: jtyping.Float[np.ndarray, \" num_samples num_classes\"],\n        tpr: jtyping.Float[np.ndarray, \" num_samples num_classes\"],\n    ) -&gt; jtyping.Float[np.ndarray, \" num_samples num_classes\"]:\n        return np.sqrt(ppv * tpr)\n</code></pre> <p>Make sure that the arguments in the signature of the <code>.compute_metric</code> method matches the dependencies. These are automatically fetched and assigned.</p> </li> </ol> <p>Once defined, the metric can also be automatically found using the metric syntax interface. The following is now completely valid:</p> <pre><code>study.add_metric(\"fowlkes_mallows\")\n</code></pre>"},{"location":"How%20To%20Guides/extending_the_library.html#metric-averaging","title":"Metric Averaging","text":"<ol> <li> <p>First import the base class, <code>Averaging</code>, as:</p> <pre><code>from prob_conf_mat.metrics.abc import Averaging\n</code></pre> </li> <li> <p>Define your class:</p> <pre><code>from prob_conf_mat.metrics.abc import Averaging\n\nclass Take2ndClass(Averaging):\n</code></pre> </li> <li> <p>Define the required class properties:</p> <ol> <li><code>full_name (str)</code>: the full, human-readable name</li> <li><code>dependencies (Tuple[str, ...])</code>: the name of any (metric) dependencies your averaging method might need. Make sure the dependencies have been implemented already (or implement them yourself). Used to build the computation graph. If there are no dependencies, leave an empty tuple</li> <li><code>sklearn_equivalent (str | None)</code>: the name of the sklearn equivalent averaging option. Used for documentation and unit testing</li> <li><code>aliases (list[str])</code>: any aliases your averaging method might go by. Each alias must be unique, and should not conflict with an alias used by another metric or averaging method</li> </ol> <p>For example:</p> <pre><code>from prob_conf_mat.metrics.abc import Averaging\n\nclass Take2ndClass(Averaging):\n    full_name = \"Takes 2nd Class Value\"\n    dependencies = ()\n    sklearn_equivalent = \"binary, with positive_class=1\"\n    aliases = [\"2nd_class\", \"two\"]\n</code></pre> </li> <li> <p>Finally, implement the <code>compute_average</code> method. Note that the input is always an array of <code>Float[ndarray, \" num_samples num_classes\"]</code>, and it should output an array of dimensions <code>jtyping.Float[np.ndarray, \" num_samples 1\"]</code>:</p> <pre><code>from prob_conf_mat.metrics.abc import Averaging\n\nclass Take2ndClass(Averaging):\n    full_name = \"Takes 2nd Class Value\"\n    dependencies = ()\n    sklearn_equivalent = \"binary, with positive_class=1\"\n    aliases = [\"2nd_class\", \"two\"]\n\n    def compute_average(self, metric_values):\n        scalar_array = metric_values[:, 1]\n\n        return scalar_array\n</code></pre> </li> </ol> <p>Just like with implementing your own metric, the averaging method can now be automatically found. You can use this averaging method with any pre-defined metric, for example:</p> <pre><code>study.add_metric(\"acc@two\")\n</code></pre> <p>Or, if you implemented the Fowlkes-Mallows index as above, the following is also completely valid:</p> <pre><code>study.add_metric(\"fmi@two\")\n</code></pre>"},{"location":"How%20To%20Guides/extending_the_library.html#additional-parameters","title":"Additional Parameters","text":"<p>If you want to add additional parameters, or introduce some notion of state into the metric or averaging method, you need to define an <code>__init__</code> method. For example,</p> <pre><code>class FooBar(Metric):\n    full_name = \"FooBar Index\"\n    ...\n    aliases = [\"foobar\"]\n\n    def __init__(self, foo: bool = False, bar: int = 1) -&gt; None:\n        super().__init__()\n\n        self.foo = foo\n        self.bar = bar\n</code></pre> <p>This metric can now be called using the following metric syntax string:</p> <pre><code>study.add_metric(\"foobar+foo=True+bar=2\")\n</code></pre> <p>Make sure to call <code>super().__init__()</code> first though.</p>"},{"location":"How%20To%20Guides/extending_the_library.html#experiment-aggregation","title":"Experiment Aggregation","text":"<p>A similar pattern was used in defining experiment aggregation methods.</p> <ol> <li> <p>First, import the base class</p> <pre><code>from prob_conf_mat.experiment_aggregation.abc import ExperimentAggregator\n</code></pre> </li> <li> <p>Define your class:</p> <pre><code>from prob_conf_mat.experiment_aggregation.abc import ExperimentAggregator\n\nclass Take1stExperiment(ExperimentAggregator):\n</code></pre> </li> <li> <p>Define the required class properties:</p> <ol> <li><code>full_name (str)</code>: the full, human-readable name</li> <li><code>aliases (list[str])</code>: any aliases your averaging method might go by. Each alias must be unique, and should not conflict with an alias used by another experiment aggregator</li> </ol> <p>For example:</p> <pre><code>from prob_conf_mat.experiment_aggregation.abc import ExperimentAggregator\n\nclass Take1stExperiment(ExperimentAggregator):\n    full_name: str = \"Always Takes 1st Experiment Result as Aggregate\"\n    aliases: list[str] = (\"first\", \"1st\")\n</code></pre> </li> <li> <p>Finally, implement the <code>aggregate</code> method. The first argument, <code>experiment_samples</code>, is always an array of <code>Float[ndarray, \" num_samples num_experiments\"]</code>, and it should output an array of dimensions <code>jtyping.Float[np.ndarray, \" num_samples\"]</code>. The signature should also take a <code>bounds: tuple[float, float]</code> argument, to allow for resampling. So, for example:</p> <pre><code>from prob_conf_mat.experiment_aggregation.abc import ExperimentAggregator\n\nclass Take1stExperiment(ExperimentAggregator):\n    full_name: str = \"Always Takes 1st Experiment Result as Aggregate\"\n    aliases: list[str] = (\"first\", \"1st\")\n\n    def aggregate(\n        self,\n        experiment_samples: jtyping.Float[np.ndarray, \" num_samples num_experiments\"],\n        bounds: tuple[float, float],\n    ) -&gt; jtyping.Float[np.ndarray, \" num_samples\"]:\n        return experiment_samples[:, 1]\n</code></pre> </li> </ol> <p>Exactly like before, as soon as you define the method, it is possible to use this experiment aggregation method. For example, the following is completely valid code:</p> <pre><code>study.add_metric(\"acc\", aggregation=\"first\")\n</code></pre>"},{"location":"How%20To%20Guides/extending_the_library.html#additional-parameters_1","title":"Additional Parameters","text":"<p>If you have additional parameters you need to define, or you want the experiment aggregation method to track some form of state, you will need to define an <code>__init__</code> method.</p> <p>Unlike before, the parent class has a defined <code>__init__</code> method that you will need to adhere to. Specifically, the first argument should always be the RNG, and this should be passed to <code>super()</code>. For example,</p> <pre><code>from prob_conf_mat.experiment_aggregation.abc import ExperimentAggregator\nfrom prob_conf_mat.utils.rng import RNG\n\nclass Take1stExperiment(ExperimentAggregator):\n    ...\n\n    def __init__(self, rng: RNG, foo: bool = True, bar: int = 1)  -&gt; None::\n        super().__init__(rng=rng)\n\n        self.foo = foo\n        self.bar = bar\n</code></pre> <p>You can now pass these extra parameters as additional keyword arguments in the <code>study.add_metric</code> method. For example:</p> <pre><code>study.add_metric(\"acc\", aggregation=\"first\", foo=False, bar=2)\n</code></pre>"},{"location":"How%20To%20Guides/extending_the_library.html#notes","title":"Notes","text":"<p>Once a class is registered, it cannot be unregistered. If you need to make changes to a custom class, either define it with completely new aliases, or restart your Python environment.</p>"},{"location":"How%20To%20Guides/metric_syntax.html","title":"Metric Syntax","text":"<p>TODO: rewrite this to reflect the updated library</p> <p>The metric syntax provides a handy interface to creating unique metric functions not hard-coded into the library. Any (binary) metric function can be combined with an aggregation function. Some metric functions and aggregation functions also require arguments. Rather than having the user search for the required metric and aggregation function, one only needs to pass a single string and have the library return the required function.</p> <p>A valid metric syntax string consists of (in order):</p> <ol> <li>The metric identifier</li> <li>Any keyword arguments that need to be passed to the metric function</li> <li>Optionally, an <code>@</code> symbol</li> <li>Optionally, the aggregation function identifier</li> <li>Optionally, any keyword arguments that need to be passed to the metric function</li> </ol> <p>No spaces should be used. Instead, keywords arguments start with a <code>+</code> prepended to the key, followed by a <code>=</code> and the value.</p> <p>All together:</p> <pre><code>metric_identifier+arg1=2+arg2=foo@aggregation_identifier+arg1=None+arg2=2.0\n</code></pre> <p>Only the metric function identifier is necessary, all other aspects are optional.</p>"},{"location":"How%20To%20Guides/metric_syntax.html#properties","title":"Properties","text":"<ul> <li>The metric name should be one of the registered functions</li> <li>Only keyword arguments are allowed</li> <li>Keywords have keys specified using a <code>+</code> their value using a prepended <code>=</code></li> <li>Only numeric (float, int) or string arguments are accepted. The string \"None\" maps to <code>None</code></li> <li>Any keyword arguments before the aggregation symbol <code>@</code> are passed to the metric function, and any after to the aggregation function</li> <li>The order of the keyword arguments does not matter, as long as they appear in the correct block</li> </ul>"},{"location":"How%20To%20Guides/metric_syntax.html#examples","title":"Examples","text":"<p>The are meant to illustrate the flexibility of the metric syntax. The defined metrics are not necessarily useful</p> <ol> <li> <p>The MCC score</p> <pre><code>mcc\n</code></pre> </li> <li> <p>The F3-score</p> <pre><code>fbeta+beta=3.0@binary+positive_class=2\n</code></pre> </li> <li> <p>Macro-averaged precision</p> <pre><code>ppv@macro\n</code></pre> </li> <li> <p>The geometric mean of the P4 scores</p> <pre><code>p4@geometric\n</code></pre> </li> <li> <p>The DOR for the third class only</p> <pre><code>dor@binary+positive_class=2\n</code></pre> </li> <li> <p>The F2-score for the 1st class only</p> <pre><code>fbeta+beta=2.0@binary+positive_class=1\n</code></pre> </li> </ol>"},{"location":"Reference/index.html","title":"Reference","text":""},{"location":"Reference/Experiment.html","title":"Experiment","text":""},{"location":"Reference/Experiment.html#experiment","title":"Experiment","text":""},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment","title":"<code>Experiment</code>","text":"<p>A single experiment, characterized by a confusion matrix.</p> <p>It is responsible for generating synthetic confusion matrices, and computing metrics.</p> <p>It is typically part of an ExperimentGroup, which in turn is part of a Study. The Study and ExperimentGroup are responsible for passing properly validated initialization parameters.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>the name of this experiment</p> </li> <li> <code>rng</code>               (<code>RNG</code>)           \u2013            <p>the RNG used to control randomness</p> </li> <li> <code>confusion_matrix</code>               (<code>Int[ndarray, 'num_classes num_classes']</code>)           \u2013            <p>the confusion matrix for this experiment.</p> </li> </ul>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment-attributes","title":"Attributes","text":""},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment.num_classes","title":"<code>num_classes</code>  <code>property</code>","text":"<p>The number of classes in this experiment.</p>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment.num_predictions","title":"<code>num_predictions</code>  <code>property</code>","text":"<p>The total number of predictions in the observed confusion matrix.</p>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment.confusion_matrix","title":"<code>confusion_matrix = confusion_matrix</code>  <code>instance-attribute</code>","text":""},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment.prevalence_prior","title":"<code>prevalence_prior = dirichlet_prior(strategy=prevalence_prior, shape=(self.num_classes,))</code>  <code>instance-attribute</code>","text":""},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment.confusion_prior","title":"<code>confusion_prior = dirichlet_prior(strategy=confusion_prior, shape=(self.num_classes, self.num_classes))</code>  <code>instance-attribute</code>","text":""},{"location":"Reference/Experiment.html#sampling","title":"Sampling","text":""},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment.sample","title":"<code>sample</code>","text":"<p>Sample synthetic confusion matrices for this experiment.</p> <p>Parameters:</p> <ul> <li> <code>sampling_method</code>               (<code>SamplingMethod</code>)           \u2013            <p>the sampling method used to generate the metric values. Must a member of the SamplingMethod enum</p> </li> <li> <code>num_samples</code>               (<code>int</code>)           \u2013            <p>the number of synthetic confusion matrices to sample</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[MetricLike, ExperimentResult]</code>           \u2013            <p>dict[MetricLike, ExperimentResult]: a dictionary of RootMetric instances</p> </li> </ul>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.Experiment.sample_metrics","title":"<code>sample_metrics</code>","text":"<p>Computes metrics over the synthetic confusion matrices.</p> <p>Parameters:</p> <ul> <li> <code>metrics</code>               (<code>MetricCollection</code>)           \u2013            <p>the metrics needed to be computed on the synthetic confusion matrices</p> </li> <li> <code>sampling_method</code>               (<code>SamplingMethod</code>)           \u2013            <p>the sampling method used to generate the metric values. Must a member of the SamplingMethod enum</p> </li> <li> <code>num_samples</code>               (<code>int</code>)           \u2013            <p>the number of synthetic confusion matrices to sample</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[MetricLike, ExperimentResult]</code>           \u2013            <p>dict[MetricLike, ExperimentResult]: a mapping from metric instance to an <code>ExperimentResult</code> instance</p> </li> </ul>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.ExperimentResult","title":"<code>ExperimentResult</code>  <code>dataclass</code>","text":"<p>A simple wrapper class for results from an experiment.</p> <p>Essentially just combines the metadata of an Experiment with a Metric, and stores its output.</p> <p>Parameters:</p> <ul> <li> <code>experiment</code>               (<code>Experiment</code>)           \u2013            <p>the experiment which produced these results</p> </li> <li> <code>metric</code>               (<code>MetricLike</code>)           \u2013            <p>the metric instance that produced these results</p> </li> <li> <code>values</code>               (<code>Float[ndarray, ' num_samples #num_classes']</code>)           \u2013            <p>the actual produced values</p> </li> </ul>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.ExperimentResult-attributes","title":"Attributes","text":""},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.ExperimentResult.is_multiclass","title":"<code>is_multiclass</code>  <code>property</code>","text":"<p>Whether the metric that produced this result is binary or multiclass.</p>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.ExperimentResult.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>The minimum and maximum possible values of the metric that produced this result.</p>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.ExperimentResult.num_classes","title":"<code>num_classes</code>  <code>property</code>","text":"<p>The number of classes in the experiment that poduced this result.</p>"},{"location":"Reference/Experiment.html#prob_conf_mat.experiment.ExperimentResult.num_samples","title":"<code>num_samples</code>  <code>property</code>","text":"<p>How many confusion matrices were sampled.</p>"},{"location":"Reference/ExperimentGroup.html","title":"ExperimentGroup","text":""},{"location":"Reference/ExperimentGroup.html#experiment-group","title":"Experiment Group","text":""},{"location":"Reference/ExperimentGroup.html#prob_conf_mat.experiment_group.ExperimentGroup","title":"<code>ExperimentGroup</code>","text":"<p>This class represents a group of related Experiments.</p> <p>The results across experiments can be aggregated to give an average result for the group.</p> <p>For example, this could represent the same model evaluated across different folds of the same dataset. Or they could be results on te same dataset from models with different weight initializations.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>the name of this experiment group</p> </li> <li> <code>rng</code>               (<code>RNG</code>)           \u2013            <p>the RNG used to control randomness</p> </li> </ul>"},{"location":"Reference/ExperimentGroup.html#prob_conf_mat.experiment_group.ExperimentGroup-attributes","title":"Attributes","text":""},{"location":"Reference/ExperimentGroup.html#prob_conf_mat.experiment_group.ExperimentGroup.num_experiments","title":"<code>num_experiments</code>  <code>property</code>","text":"<p>The number of experiments in this ExperimentGroup.</p>"},{"location":"Reference/ExperimentGroup.html#configuration","title":"Configuration","text":""},{"location":"Reference/ExperimentGroup.html#prob_conf_mat.experiment_group.ExperimentGroup.add_experiment","title":"<code>add_experiment</code>","text":"<p>Adds an Experiment to this ExperimentGroup.</p> <p>Each experiment is characterized by a single confusion matrix.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>the name of this experiment</p> </li> <li> <code>confusion_matrix</code>               (<code>Int[ndarray, 'num_classes num_classes']</code>)           \u2013            <p>the confusion matrix for this experiment.</p> </li> <li> <code>prevalence_prior</code>               (<code>str | float | Float[ArrayLike, ' num_classes']</code>, default:                   <code>0</code> )           \u2013            <p>the prior over the prevalence counts for this experiments. Defaults to 0, Haldane's prior.</p> </li> <li> <code>confusion_prior</code>               (<code>str | float | Float[ArrayLike, ' num_classes num_classes']</code>, default:                   <code>0</code> )           \u2013            <p>the prior over the confusion counts for this experiments. Defaults to 0, Haldane's prior.</p> </li> </ul>"},{"location":"Reference/ExperimentGroup.html#prob_conf_mat.experiment_group.ExperimentGroup.__getitem__","title":"<code>__getitem__</code>","text":"<p>Gets an experiment under this ExperimentGroup by its name.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>the experiment name</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Experiment</code> (              <code>Experiment</code> )          \u2013            <p>description</p> </li> </ul>"},{"location":"Reference/ExperimentGroup.html#prob_conf_mat.experiment_group.ExperimentGroupResult","title":"<code>ExperimentGroupResult</code>  <code>dataclass</code>","text":"<p>A wrapper class for the output of an ExperimentGroup.</p>"},{"location":"Reference/IO.html","title":"IO","text":"<p>A module dedicated to various IO operations.</p> <p>Users are encouraged to use this when trying to import confusion matrices from disk.</p>"},{"location":"Reference/IO.html#file-formats","title":"File Formats","text":""},{"location":"Reference/IO.html#prob_conf_mat.io.load_csv","title":"load_csv","text":"<pre><code>load_csv(\n    location: str | Path,\n    encoding: str = \"utf-8\",\n    newline: str = \"\\n\",\n    dialect: str = \"excel\",\n    delimiter: str = \",\",\n    lineterminator: str = \"\\r\\n\",\n    dtype: DTypeLike = int64,\n) -&gt; Int[ndarray, \" num_classes num_classes\"]\n</code></pre> <p>Loads a CSV file into memory, and parses it as if it were a valid confusion matrix.</p> <p>Parameters:</p> <ul> <li> <code>location</code>               (<code>str | Path</code>)           \u2013            <p>the location of csv file containing the confusion matrix</p> </li> <li> <code>encoding</code>               (<code>str</code>, default:                   <code>'utf-8'</code> )           \u2013            <p>the encoding of the confusion matrix file</p> </li> <li> <code>newline</code>               (<code>str</code>, default:                   <code>'\\n'</code> )           \u2013            <p>the newline character used in the confusion matrix file</p> </li> <li> <code>dialect</code>               (<code>str</code>, default:                   <code>'excel'</code> )           \u2013            <p>the csv dialect, passed to <code>csv.reader</code></p> </li> <li> <code>delimiter</code>               (<code>str</code>, default:                   <code>','</code> )           \u2013            <p>the csv delimiter character, passed to <code>csv.reader</code></p> </li> <li> <code>lineterminator</code>               (<code>str</code>, default:                   <code>'\\r\\n'</code> )           \u2013            <p>the csv lineterminator character, passed to <code>csv.reader</code></p> </li> <li> <code>dtype</code>               (<code>DTypeLike</code>, default:                   <code>int64</code> )           \u2013            <p>the desired dtype of the numpy array. Defaults to int64.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Int[ndarray, ' num_classes num_classes']</code>           \u2013            <p>Int[ndarray, 'num_classes num_classes']: the parsed confusion matrix</p> </li> </ul>"},{"location":"Reference/IO.html#utilities","title":"Utilities","text":""},{"location":"Reference/IO.html#prob_conf_mat.io.validate_confusion_matrix","title":"validate_confusion_matrix","text":"<pre><code>validate_confusion_matrix(\n    confusion_matrix: Int[ArrayLike, \" num_classes num_classes\"],\n    dtype: DTypeLike = int64,\n) -&gt; Int[ndarray, \" num_classes num_classes\"]\n</code></pre> <p>Validates a confusion matrix to prevent any future funny business.</p> <p>For a confusion matrix to be valid, it:     1. Must be square matrix (i.e., arrays with 2 dimensions)     2. Must contain only positive integers     3. Must contain at least 2 classes     4. Must have at least one record for each ground-truth class     5. Should have at least one record for each prediction</p> <p>Parameters:</p> <ul> <li> <code>confusion_matrix</code>               (<code>Int[ndarray, 'num_classes num_classes']</code>)           \u2013            <p>the confusion matrix</p> </li> <li> <code>dtype</code>               (<code>DTypeLike</code>, default:                   <code>int64</code> )           \u2013            <p>the desired dtype of the numpy array. Defaults to int64.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Int[ndarray, ' num_classes num_classes']</code>           \u2013            <p>Int[ndarray, 'num_classes num_classes']: the validated confusion matrix as a numpy ndarray</p> </li> </ul>"},{"location":"Reference/IO.html#prob_conf_mat.io.pred_cond_to_confusion_matrix","title":"pred_cond_to_confusion_matrix","text":"<pre><code>pred_cond_to_confusion_matrix(\n    pred_cond: Int[ndarray, \" num_samples 2\"], *, pred_first: bool = True\n) -&gt; Int[ndarray, \" num_classes num_classes\"]\n</code></pre> <p>Converts an array-like of model prediction, ground truth pairs into an unnormalized confusion matrix.</p> <p>Confusion matrix always has predictions on the columns, condition on the rows.</p> <p>Parameters:</p> <ul> <li> <code>pred_cond</code>               (<code>Int[ndarray, ' num_samples 2']</code>)           \u2013            <p>the arraylike collection of predictions</p> </li> <li> <code>pred_first</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether the model prediction is on the first column, or the ground truth label. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Int[ndarray, ' num_classes num_classes']</code>           \u2013            <p>jtyping.Int[np.ndarray, ' num_classes num_classes']</p> </li> </ul>"},{"location":"Reference/IO.html#prob_conf_mat.io.confusion_matrix_to_pred_cond","title":"confusion_matrix_to_pred_cond","text":"<pre><code>confusion_matrix_to_pred_cond(\n    confusion_matrix: Int[ndarray, \" num_classes num_classes\"],\n    *,\n    pred_first: bool = True,\n) -&gt; Int[ndarray, \" num_samples 2\"]\n</code></pre> <p>Converts an unnormalized confusion matrix into an array of model prediction, ground truth pairs.</p> <p>Assumes predictions on the columns, condition on the rows of the confusion matrix.</p> <p>Parameters:</p> <ul> <li> <code>confusion_matrix</code>               (<code>Int[ndarray, ' num_classes num_classes']</code>)           \u2013            <p>the unnormalized confusion matrix</p> </li> <li> <code>pred_first</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether the model prediction should be on the first column, or the ground truth label. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Int[ndarray, ' num_samples 2']</code>           \u2013            <p>jtyping.Int[np.ndarray, ' num_samples 2']</p> </li> </ul>"},{"location":"Reference/Statistics.html","title":"Statistics","text":"<p>A module dedicated to generating samples and computing summary statistics from samples.</p>"},{"location":"Reference/Statistics.html#batched-averages","title":"Batched Averages","text":"<p>Vectorized computation of various averages using a consistent interface.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.batched_averaging.numpy_batched_arithmetic_mean","title":"numpy_batched_arithmetic_mean","text":"<pre><code>numpy_batched_arithmetic_mean(\n    array: Float[ndarray, \"... axis ...\"],\n    axis: int = -1,\n    *,\n    keepdims: bool = True,\n) -&gt; Float[ndarray, \"... 1 ...\"]\n</code></pre> <p>Computes the arithmetic mean over an axis.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.batched_averaging.numpy_batched_convex_combination","title":"numpy_batched_convex_combination","text":"<pre><code>numpy_batched_convex_combination(\n    array: Float[ndarray, \"... axis ...\"],\n    axis: int = -1,\n    *,\n    convex_weights: Float[ndarray, \" num_samples final_dim\"],\n    keepdims: bool = True,\n) -&gt; Float[ndarray, \"... 1 ...\"]\n</code></pre> <p>Computes a weighted mean over an axis.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.batched_averaging.numpy_batched_harmonic_mean","title":"numpy_batched_harmonic_mean","text":"<pre><code>numpy_batched_harmonic_mean(\n    array: Float[ndarray, \"... axis ...\"],\n    axis: int = -1,\n    *,\n    keepdims: bool = True,\n) -&gt; Float[ndarray, \"... 1 ...\"]\n</code></pre> <p>Computes the harmonic mean over an axis.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.batched_averaging.numpy_batched_geometric_mean","title":"numpy_batched_geometric_mean","text":"<pre><code>numpy_batched_geometric_mean(\n    array: Float[ndarray, \"... axis ...\"],\n    axis: int = -1,\n    *,\n    keepdims: bool = True,\n) -&gt; Float[ndarray, \"... 1 ...\"]\n</code></pre> <p>Computes the weighted mean over an axis.</p>"},{"location":"Reference/Statistics.html#dirichlet-distribution","title":"Dirichlet Distribution","text":"<p>Optimized sampling of batched samples from independent Dirichlet distributions.</p> <p>These functions tend to be a performance bottleneck, and should be as optimized as much as possible.</p> <p>Creates a prior array for a Dirichlet distribution.</p> <p>Returns:</p> <ul> <li> <code>Float[ndarray, ' ...']</code>           \u2013            <p>jtyping.Float[np.ndarray, \" ...\"]: the prior vector</p> </li> </ul>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.dirichlet_distribution.dirichlet_prior","title":"dirichlet_prior","text":"<pre><code>dirichlet_prior(\n    strategy: str | float | int | Float[ArrayLike, \" ...\"],\n    shape: tuple[int, ...],\n) -&gt; Float[ndarray, \" ...\"]\n</code></pre> <p>Creates a prior array for a Dirichlet distribution.</p> <p>Returns:</p> <ul> <li> <code>Float[ndarray, ' ...']</code>           \u2013            <p>jtyping.Float[np.ndarray, \" ...\"]: the prior vector</p> </li> </ul>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.dirichlet_distribution.dirichlet_sample","title":"dirichlet_sample","text":"<pre><code>dirichlet_sample(\n    rng: RNG, alphas: Float[ndarray, \" ...\"], num_samples: int\n) -&gt; Float[ndarray, \" num_samples ...\"]\n</code></pre> <p>Generate Dirichlet distributed samples from an array of Gamma distributions.</p> <p>A Dirichlet distribution can be constructed by dividing a set of Gamma distributions by their sum.</p> <p>For some reason the Numpy implementation of the Dirichlet distribution is not vectorized, while the implementation of the Gamma distribution. is.</p> <p>Adapted from StackOverflow.</p> <p>This function is the performance bottleneck for this package. Need to make sure it's performant.</p> <p>Parameters:</p> <ul> <li> <code>rng</code>               (<code>RNG</code>)           \u2013            <p>the random number generator</p> </li> <li> <code>alphas</code>               (<code>Float[ndarray, '...']</code>)           \u2013            <p>the Dirichlet parameters</p> </li> <li> <code>num_samples</code>               (<code>int</code>)           \u2013            <p>the number of samples to retrieve</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ndarray, ' num_samples ...']</code>           \u2013            <p>jtyping.Float[np.ndarray, \" num_samples ...\"]: samples from the specified Dirichlet distribution</p> </li> </ul>"},{"location":"Reference/Statistics.html#hdi-estimation","title":"HDI Estimation","text":"<p>Tries to find the Highest Density Interval (HDI) of a posterior distribution from its samples.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.hdi_estimation.hdi_estimator","title":"hdi_estimator","text":"<pre><code>hdi_estimator(\n    samples: Float[ndarray, \" num_samples\"], prob: float\n) -&gt; tuple[float | Float[ndarray, \"\"], float | Float[ndarray, \"\"]]\n</code></pre> <p>Computes the highest density interval (HDI) of an array of samples for a given probability.</p> <p>Adapted from arviz.</p> <p>Guaranteed to contain the median if <code>prob &gt; 0.5</code>, and if the distribution is unimodal, also contains the mode.</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>Float[ndarray, ' num_samples']</code>)           \u2013            <p>the array of samples</p> </li> <li> <code>prob</code>               (<code>float</code>)           \u2013            <p>the probability</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float | Float[ndarray, ''], float | Float[ndarray, '']]</code>           \u2013            <p>tuple[float, float]: the lower and upper bound of the HDI</p> </li> </ul>"},{"location":"Reference/Statistics.html#mode-estimation","title":"Mode Estimation","text":"<p>Tries to find the mode of a distribution from its samples.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.mode_estimation.histogram_mode_estimator","title":"histogram_mode_estimator","text":"<pre><code>histogram_mode_estimator(\n    samples: Float[ndarray, \" num_samples\"],\n    bounds: tuple[float, float] | None = None,\n) -&gt; float\n</code></pre> <p>\"Tries to estimate the mode of a distribution from its samples.</p>"},{"location":"Reference/Statistics.html#summary-statistics","title":"Summary Statistics","text":"<p>Computes various summary statistics about a distribution from its samples.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.summary.PosteriorSummary","title":"PosteriorSummary  <code>dataclass</code>","text":"<p>Summary statistics of some probability distribution.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.summary.PosteriorSummary.metric_uncertainty","title":"metric_uncertainty  <code>property</code>","text":"<pre><code>metric_uncertainty: float\n</code></pre> <p>The metric uncertainty (MU), defined as the size of the HDI.</p> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>the MU</p> </li> </ul>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.summary.PosteriorSummary.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: list[str]\n</code></pre> <p>The column headers.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.summary.PosteriorSummary.as_dict","title":"as_dict","text":"<pre><code>as_dict() -&gt; dict[str, float | tuple[float, float]]\n</code></pre> <p>Returns the dict representation of the statistics.</p> <p>Useful for coverting to a table.</p> <p>Returns:</p> <ul> <li> <code>dict[str, float | tuple[float, float]]</code>           \u2013            <p>dict[str, float | tuple[float, float]]</p> </li> </ul>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.summary.summarize_posterior","title":"summarize_posterior","text":"<pre><code>summarize_posterior(\n    posterior_samples: Float[ndarray, \" num_samples\"], ci_probability: float\n) -&gt; PosteriorSummary\n</code></pre> <p>Summarizes a distribution, assumed to be a posterior, based on samples from it.</p> <p>Parameters:</p> <ul> <li> <code>posterior_samples</code>               (<code>Float[ndarray, ' num_samples']</code>)           \u2013            <p>samples from the posterior.</p> </li> <li> <code>ci_probability</code>               (<code>float</code>)           \u2013            <p>the probability under the HDI.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PosteriorSummary</code> (              <code>PosteriorSummary</code> )          \u2013            <p>the summary statistics.</p> </li> </ul>"},{"location":"Reference/Statistics.html#truncated-sampling","title":"Truncated Sampling","text":"<p>Draws bounded samples from unbounded Scipy distributions.</p> <p>This is necessary when making parametric assumptions about the distribution of metrics that have minimum and maximum values.</p>"},{"location":"Reference/Statistics.html#prob_conf_mat.stats.truncated_sampling.truncated_sample","title":"truncated_sample","text":"<pre><code>truncated_sample(\n    sampling_distribution: rv_continuous,\n    bounds: tuple[float, float],\n    rng: RNG,\n    num_samples: int,\n) -&gt; Float[ndarray, \" num_samples\"]\n</code></pre> <p>Generates a bounded sample from an unbouded continuous Scipy distribution.</p> <p>Uses inverse transform sampling to draw samples from the unbounded distribution.</p> <p>The quantiles sampled uniformly are bounded, such that their transform is also implicitly bounded.</p> <p>Parameters:</p> <ul> <li> <code>sampling_distribution</code>               (<code>rv_continuous</code>)           \u2013            <p>the unbouded continuous Scipy distribution</p> </li> <li> <code>bounds</code>               (<code>tuple[float, float]</code>)           \u2013            <p>the bounds</p> </li> <li> <code>rng</code>               (<code>RNG</code>)           \u2013            <p>the random number generator</p> </li> <li> <code>num_samples</code>               (<code>int</code>)           \u2013            <p>the number of samples to draw</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ndarray, ' num_samples']</code>           \u2013            <p>jtyping.Float[np.ndarray, \" num_samples\"]: the samples from the bounded distribution</p> </li> </ul>"},{"location":"Reference/Study.html","title":"Study","text":""},{"location":"Reference/Study.html#study","title":"Study","text":""},{"location":"Reference/Study.html#prob_conf_mat.study.Study","title":"<code>Study</code>","text":"<p>               Bases: <code>Config</code></p> <p>This class represents a study, a collection of related experiments and experiment groups.</p> <p>It handles all lower level operations for you.</p> <p>You can use it to organize your experiments, compute and cache metrics, request analyses or figures, etc.</p> <p>Experiment groups should be directly comparable across groups.</p> <p>For example, a series of different models evaluated on the same dataset.</p> <p>Parameters:</p> <ul> <li> <code>seed</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>the random seed used to initialise the RNG. Defaults to the current time, in fractional seconds.</p> </li> <li> <code>num_samples</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>the number of syntehtic confusion matrices to sample. A higher value is better, but more computationally expensive. Defaults to 10000, the minimum recommended value.</p> </li> <li> <code>ci_probability</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>the size of the credibility intervals to compute. Defaults to 0.95, which is an arbitrary value, and should be carefully considered.</p> </li> <li> <code>experiments</code>               (<code>dict[str, dict[str, dict[str, Any]]]</code>, default:                   <code>{}</code> )           \u2013            <p>a nested dict that contains (1) the experiment group name, (2) the experiment name, (3) and finally any IO/prior hyperparameters. Defaults to an empty dict.</p> </li> <li> <code>metrics</code>               (<code>dict[str, dict[str, Any]]</code>, default:                   <code>{}</code> )           \u2013            <p>a nested dict that contains (1) the metric as metric syntax strings, (2) and any metric aggregation parameters. Defaults to an empty dict.</p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study-attributes","title":"Attributes","text":""},{"location":"Reference/Study.html#prob_conf_mat.study.Study.num_classes","title":"<code>num_classes</code>  <code>property</code>","text":"<p>Returns the number of classes used in experiments in this study.</p>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.num_experiment_groups","title":"<code>num_experiment_groups</code>  <code>property</code>","text":"<p>Returns the number of ExperimentGroups in this Study.</p>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.num_experiments","title":"<code>num_experiments</code>  <code>property</code>","text":"<p>Returns the total number of Experiments in this Study.</p>"},{"location":"Reference/Study.html#configuration","title":"Configuration","text":""},{"location":"Reference/Study.html#prob_conf_mat.study.Study.to_dict","title":"<code>to_dict</code>","text":"<p>Returns the configuration of this Study as a Pythonic dict.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, typing.Any]: the configuration dict, necessary to recreate this Study</p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.from_dict","title":"<code>from_dict</code>  <code>classmethod</code>","text":"<p>Creates a Study from a dictionary.</p> <p>Keys and values should match pattern of output from Study.to_dict.</p> <p>Parameters:</p> <ul> <li> <code>config_dict</code>               (<code>dict[str, Any]</code>)           \u2013            <p>the dictionary representation of the study configuration.</p> </li> <li> <code>kwargs</code>           \u2013            <p>any additional keyword arguments typically passed to Study's <code>.__init__</code> method</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>typing.Self: an instance of a study</p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.add_experiment","title":"<code>add_experiment</code>","text":"<p>Adds an experiment to this study.</p> <p>Parameters:</p> <ul> <li> <code>experiment_name</code>               (<code>str</code>)           \u2013            <p>the name of the experiment and experiment group. Should be written as 'experiment_group/experiment'. If the experiment group name is omitted,     the experiment gets added to a new experiment group of the same name.</p> </li> <li> <code>confusion_matrix</code>               (<code>Int[ArrayLike, 'num_classes num_classes']</code>)           \u2013            <p>the confusion matrix for this experiment</p> </li> <li> <code>prevalence_prior</code>               (<code>str | float | Float[ArrayLike, ' num_classes']</code>, default:                   <code>None</code> )           \u2013            <p>the prior over the prevalence counts for this experiments. Defaults to 0, Haldane's prior.</p> </li> <li> <code>confusion_prior</code>               (<code>str | float | Float[ArrayLike, ' num_classes num_classes']</code>, default:                   <code>None</code> )           \u2013            <p>the prior over the confusion counts for this experiments. Defaults to 0, Haldane's prior.</p> </li> <li> <code>**io_kwargs</code>           \u2013            <p>any additional keyword arguments that are needed for confusion matrix I/O</p> </li> </ul> <p>Examples:</p> <p>Add an experiment named 'test_a' to experiment group 'test'</p> <pre><code>&gt;&gt;&gt; self.add_experiment(\n...     name=\"test/test_a\",\n...     confusion_matrix=[[1, 0], [0, 1]],\n... )\n</code></pre> <p>Add an experiment named 'test_a' to experiment group 'test', with some specific prior.</p> <pre><code>&gt;&gt;&gt; self.add_experiment(\n...     name=\"test/test_a\",\n...     confusion_matrix=[[1, 0], [0, 1]],\n...     prevalence_prior=[1, 1],\n...     confusion_prior=\"half\",\n... )\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.__getitem__","title":"<code>__getitem__</code>","text":"<p>Gets an ExperimentGroup or Experiment by name.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>the name of the ExperimentGroup or the Experiment. Experiment names must be in the '{EXPERIMENT_GROUP}/{EXPERIMENT}' format</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Experiment | ExperimentGroup</code>           \u2013            <p>Experiment | ExperimentGroup: description</p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.add_metric","title":"<code>add_metric</code>","text":"<p>Adds a metric to the study.</p> <p>If there are more than one Experiment in an ExperimentGroup, an aggregation method is required.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str | MetricLike</code>)           \u2013            <p>the metric to be added</p> </li> <li> <code>aggregation</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>the name of the aggregation method. Defaults to None.</p> </li> <li> <code>aggregation_kwargs</code>           \u2013            <p>keyword arguments passed to the <code>get_experiment_aggregator</code> function</p> </li> </ul>"},{"location":"Reference/Study.html#estimating-uncertainty","title":"Estimating Uncertainty","text":"<p>Examples:</p> <p>Plot a distribution of metric values</p> <pre><code>study.plot_metric_summaries(\n    metric=\"acc\",\n    class_label=0,\n)\n</code></pre> <p> </p>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.get_metric_samples","title":"<code>get_metric_samples</code>","text":"<p>Loads or computes samples for a metric, belonging to an experiment.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str | MetricLike</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>experiment_name</code>               (<code>str</code>)           \u2013            <p>the name of the experiment. You can also pass 'experiment_group/aggregated' to retrieve the aggregated metric values.</p> </li> <li> <code>sampling_method</code>               (<code>str</code>)           \u2013            <p>the sampling method used to generate the metric values. Must a member of the SamplingMethod enum</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ExperimentResult | ExperimentAggregationResult</code>           \u2013            <p>typing.Union[ExperimentResult, ExperimentAggregationResult]</p> </li> </ul> <p>Examples:</p> <p>Get the accuracy scores for experiment 'test/test_a' for synthetic confusion matrices sampled from the posterior predictive distribution.</p> <pre><code>&gt;&gt;&gt; experiment_result = self.get_metric_samples(\n...     metric=\"accuracy\",\n...     sampling_method=\"posterior\",\n...     experiment_name=\"test/test_a\",\n... )\nExperimentResult(experiment=ExperimentGroup(test_a), metric=Metric(accuracy))\n</code></pre> <p>Similarly, get the accuracy scores, but now aggregated across an entire ExperimentGroup</p> <pre><code>&gt;&gt;&gt; experiment_result = self.get_metric_samples(\n...     metric=\"accuracy\",\n...     sampling_method=\"posterior\",\n...     experiment_name=\"test/aggregated\",\n... )\nExperimentAggregationResult(\n    experiment_group=ExperimentGroup(test),\n    metric=Metric(accuracy),\n    aggregator=ExperimentAggregator(fe_gaussian)\n    )\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.report_metric_summaries","title":"<code>report_metric_summaries</code>","text":"<p>Generates a table with summary statistics for all experiments.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Leave 0 or None if using a multiclass metric. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>table_fmt</code>               (<code>str</code>)           \u2013            <p>the format of the table. If 'records', the raw list of values is returned. If 'pandas' or 'pd', a Pandas DataFrame is returned. In all other cases, it is passed to tabulate. Defaults to tabulate's \"html\".</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the required precision of the presented numbers. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>list | DataFrame | str</code> )          \u2013            <p>the table as a string</p> </li> </ul> <p>Examples:</p> <p>Return the a table with summary statistics of the metric distribution</p> <pre><code>&gt;&gt;&gt; print(\n...     study.report_metric_summaries(\n...         metric=\"acc\", class_label=0, table_fmt=\"github\"\n...     )\n... )\n</code></pre> <pre><code>| Group   | Experiment   |   Observed |   Median |   Mode |        95.0% HDI |     MU |   Skew |    Kurt |\n|---------|--------------|------------|----------|--------|------------------|--------|--------|---------|\n| GROUP   | EXPERIMENT   |     0.5000 |   0.4999 | 0.4921 | [0.1863, 0.8227] | 0.6365 | 0.0011 | -0.5304 |\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.report_random_metric_summaries","title":"<code>report_random_metric_summaries</code>","text":"<p>Provides a table with metric results from a simulated random classifier.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Leave 0 or None if using a multiclass metric. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>table_fmt</code>               (<code>str</code>)           \u2013            <p>the format of the table, passed to tabulate. Defaults to \"html\".</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the required precision of the presented numbers. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>list | DataFrame | str</code> )          \u2013            <p>the table as a string</p> </li> </ul> <p>Examples:</p> <p>Return the a table with summary statistics of the metric distribution</p> <pre><code>&gt;&gt;&gt; print(\n...     study.report_random_metric_summaries(\n...         metric=\"acc\", class_label=0, table_fmt=\"github\"\n...     )\n... )\n</code></pre> <pre><code>| Group   | Experiment   |   Median |   Mode |        95.0% HDI |     MU |    Skew |    Kurt |\n|---------|--------------|----------|--------|------------------|--------|---------|---------|\n| GROUP   | EXPERIMENT   |   0.4994 | 0.5454 | [0.1778, 0.8126] | 0.6348 | -0.0130 | -0.5623 |\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.plot_metric_summaries","title":"<code>plot_metric_summaries</code>","text":"<p>Plots the distrbution of sampled metric values for a metric and class combination.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>method</code>               (<code>str</code>)           \u2013            <p>the method for displaying a histogram, provided by Seaborn. Can be either a histogram or KDE. Defaults to \"kde\".</p> </li> <li> <code>bandwidth</code>               (<code>float</code>)           \u2013            <p>the bandwith parameter for the KDE. Corresponds to Seaborn's <code>bw_adjust</code> parameter. Defaults to 1.0.</p> </li> <li> <code>bins</code>               (<code>int | list[int] | str</code>)           \u2013            <p>the number of bins to use in the histrogram. Corresponds to numpy's <code>bins</code> parameter. Defaults to \"auto\".</p> </li> <li> <code>normalize</code>               (<code>bool</code>)           \u2013            <p>if normalized, each distribution will be scaled to [0, 1]. Otherwise, uses a shared y-axis. Defaults to False.</p> </li> <li> <code>figsize</code>               (<code>tuple[float, float]</code>)           \u2013            <p>the figure size, in inches. Corresponds to matplotlib's <code>figsize</code> parameter. Defaults to None, in which case a decent default value will be approximated.</p> </li> <li> <code>fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the experiment name labels. Defaults to 9.</p> </li> <li> <code>axis_fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the x-axis ticklabels. Defaults to None, in which case the fontsize will be used.</p> </li> <li> <code>edge_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the histogram or KDE edge. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>area_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the histogram or KDE filled area. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"gray\".</p> </li> <li> <code>area_alpha</code>               (<code>float</code>)           \u2013            <p>the opacity of the histogram or KDE filled area. Corresponds to matplotlib's <code>alpha</code> parameter. Defaults to 0.5.</p> </li> <li> <code>plot_median_line</code>               (<code>bool</code>)           \u2013            <p>whether to plot the median line. Defaults to True.</p> </li> <li> <code>median_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the median line. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>median_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the median line. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"--\".</p> </li> <li> <code>plot_hdi_lines</code>               (<code>bool</code>)           \u2013            <p>whether to plot the HDI lines. Defaults to True.</p> </li> <li> <code>hdi_lines_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the HDI lines. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>hdi_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the HDI lines. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>plot_obs_point</code>               (<code>bool</code>)           \u2013            <p>whether to plot the observed value as a marker. Defaults to True.</p> </li> <li> <code>obs_point_marker</code>               (<code>str</code>)           \u2013            <p>the marker type of the observed value. Corresponds to matplotlib's <code>marker</code> parameter. Defaults to \"D\".</p> </li> <li> <code>obs_point_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the observed marker. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>obs_point_size</code>               (<code>float</code>)           \u2013            <p>the size of the observed marker. Defaults to None.</p> </li> <li> <code>plot_extrema_lines</code>               (<code>bool</code>)           \u2013            <p>whether to plot small lines at the distribution extreme values. Defaults to True.</p> </li> <li> <code>extrema_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the extrema lines. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>extrema_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the extrema lines. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>extrema_line_height</code>               (<code>float</code>)           \u2013            <p>the maximum height of the extrema lines. Defaults to 12.</p> </li> <li> <code>extrema_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the extrema line. Defaults to 1.</p> </li> <li> <code>plot_base_line</code>               (<code>bool</code>)           \u2013            <p>whether to plot a line at the base of the distribution. Defaults to True.</p> </li> <li> <code>base_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the base line. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>base_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the base line. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>base_line_width</code>               (<code>int</code>)           \u2013            <p>the width of the base line. Defaults to 1.</p> </li> <li> <code>plot_experiment_name</code>               (<code>bool</code>)           \u2013            <p>whether to plot the experiment names as labels. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>matplotlib.figure.Figure: the completed figure of the distribution plot</p> </li> </ul>"},{"location":"Reference/Study.html#comparing-2-experiments","title":"Comparing 2 Experiments","text":"<p>Examples:</p> <p>Plot the distribution of the difference of two metrics</p> <pre><code>study.plot_pairwise_comparison(\n    metric=\"acc\",\n    class_label=0,\n    experiment_a=\"GROUP/EXPERIMENT_A\",\n    experiment_b=\"GROUP/EXPERIMENT_B\",\n    min_sig_diff=0.03,\n)\n</code></pre> <p> </p>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.get_pairwise_comparison","title":"<code>get_pairwise_comparison</code>","text":"<p>Generates a <code>PairwiseComparisonResult</code> between two experiments in this study.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>)           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>experiment_a</code>               (<code>str</code>)           \u2013            <p>the name of an experiment in the '{EXPERIMENT_NAME}/{EXPERIMENT}' format. To compare an ExperimentGroup, use 'aggregated' as the experiment name</p> </li> <li> <code>experiment_b</code>               (<code>str</code>)           \u2013            <p>the name of an experiment in the '{EXPERIMENT_NAME}/{EXPERIMENT}' format. To compare an ExperimentGroup, use 'aggregated' as the experiment name</p> </li> <li> <code>min_sig_diff</code>               (<code>float | None</code>)           \u2013            <p>the minimal difference which is considered significant. Defaults to 0.1 * std.</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the precision of floats used when printing. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PairwiseComparisonResult</code> (              <code>PairwiseComparisonResult</code> )          \u2013            <p>the unformatted <code>PairwiseComparisonResult</code></p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.report_pairwise_comparison","title":"<code>report_pairwise_comparison</code>","text":"<p>Reports on the comparison between two Experiments or ExperimentGroups.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>experiment_a</code>               (<code>str</code>)           \u2013            <p>the name of an experiment in the '{EXPERIMENT_NAME}/{EXPERIMENT}' format. To compare an ExperimentGroup, use 'aggregated' as the experiment name</p> </li> <li> <code>experiment_b</code>               (<code>str</code>)           \u2013            <p>the name of an experiment in the '{EXPERIMENT_NAME}/{EXPERIMENT}' format. To compare an ExperimentGroup, use 'aggregated' as the experiment name</p> </li> <li> <code>min_sig_diff</code>               (<code>float | None</code>)           \u2013            <p>the minimal difference which is considered significant. Defaults to 0.1 * std.</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the precision of floats used when printing. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>a description of the significance of the difference between <code>experiment_a</code> and <code>experiment_b</code></p> </li> </ul> <p>Examples:</p> <p>Report on the difference in accuracy between experiments 'EXPERIMENT_A' and 'EXPERIMENT_B', with a minimum significance difference of 0.03.</p> <pre><code>&gt;&gt;&gt; study.report_pairwise_comparison(\n...     metric=\"acc\",\n...     class_label=0,\n...     experiment_a=\"GROUP/EXPERIMENT_A\",\n...     experiment_b=\"GROUP/EXPERIMENT_B\",\n...     min_sig_diff=0.03,\n... )\n</code></pre> <pre><code>Experiment GROUP/EXPERIMENT_A's acc being lesser than GROUP/EXPERIMENT_B could be considered 'dubious'* (Median \u0394=-0.0002, 95.00% HDI=[-0.0971, 0.0926], p_direction=50.13%).\n\nThere is a 53.11% probability that this difference is bidirectionally significant (ROPE=[-0.0300, 0.0300], p_ROPE=46.89%).\n\nBidirectional significance could be considered 'undecided'*.\n\nThere is a 26.27% probability that this difference is significantly negative (p_pos=26.84%, p_neg=26.27%).\n\nRelative to two random models (p_ROPE,random=36.56%) significance is 1.2825 times less likely.\n\n* These interpretations are based off of loose guidelines, and should change according to the application.\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.report_pairwise_comparison_to_random","title":"<code>report_pairwise_comparison_to_random</code>","text":"<p>Reports on the comparison between an Experiment or ExperimentGroup and a simulated random classifier.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>experiment</code>               (<code>str</code>)           \u2013            <p>the name of an experiment in the '{EXPERIMENT_NAME}/{EXPERIMENT}' format. To compare an ExperimentGroup, use 'aggregated' as the experiment name</p> </li> <li> <code>min_sig_diff</code>               (<code>float | None</code>)           \u2013            <p>the minimal difference which is considered significant. Defaults to 0.1 * std.</p> </li> <li> <code>table_fmt</code>               (<code>str</code>)           \u2013            <p>the format of the table. If 'records', the raw list of values is returned. If 'pandas' or 'pd', a Pandas DataFrame is returned. In all other cases, it is passed to tabulate. Defaults to tabulate's \"html\".</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the precision of floats used when printing. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>list | DataFrame | str</code> )          \u2013            <p>a description of the significance of the difference between <code>experiment_a</code> and <code>experiment_b</code></p> </li> </ul> <p>Examples:</p> <p>Report on the difference in accuracy to that of a random classifier</p> <pre><code>&gt;&gt;&gt; print(\n...     study.report_pairwise_comparison_to_random(\n...         metric=\"acc\",\n...         class_label=0,\n...         table_fmt=\"github\",\n...     )\n... )\n</code></pre> <pre><code>| Group   | Experiment   |   Median \u0394 |   p_direction |              ROPE |   p_ROPE |   p_sig |\n|---------|--------------|------------|---------------|-------------------|----------|---------|\n| GROUP   | EXPERIMENT_A |     0.3235 |        1.0000 | [-0.0056, 0.0056] |   0.0000 |  1.0000 |\n| GROUP   | EXPERIMENT_B |     0.3231 |        1.0000 | [-0.0056, 0.0056] |   0.0000 |  1.0000 |\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.plot_pairwise_comparison","title":"<code>plot_pairwise_comparison</code>","text":"<p>Plots the distribution of the difference between two experiments.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>experiment_a</code>               (<code>str</code>)           \u2013            <p>the name of an experiment in the '{EXPERIMENT_NAME}/{EXPERIMENT}' format. To compare an ExperimentGroup, use 'aggregated' as the experiment name</p> </li> <li> <code>experiment_b</code>               (<code>str</code>)           \u2013            <p>the name of an experiment in the '{EXPERIMENT_NAME}/{EXPERIMENT}' format. To compare an ExperimentGroup, use 'aggregated' as the experiment name</p> </li> <li> <code>min_sig_diff</code>               (<code>float | None</code>)           \u2013            <p>the minimal difference which is considered significant. Defaults to 0.1 * std.</p> </li> <li> <code>method</code>               (<code>str</code>)           \u2013            <p>the method for displaying a histogram, provided by Seaborn. Can be either a histogram or KDE. Defaults to \"kde\".</p> </li> <li> <code>bandwidth</code>               (<code>float</code>)           \u2013            <p>the bandwith parameter for the KDE. Corresponds to Seaborn's <code>bw_adjust</code> parameter. Defaults to 1.0.</p> </li> <li> <code>bins</code>               (<code>int | list[int] | str</code>)           \u2013            <p>the number of bins to use in the histrogram. Corresponds to numpy's <code>bins</code> parameter. Defaults to \"auto\".</p> </li> <li> <code>figsize</code>               (<code>tuple[float, float]</code>)           \u2013            <p>the figure size, in inches. Corresponds to matplotlib's <code>figsize</code> parameter. Defaults to None, in which case a decent default value will be approximated.</p> </li> <li> <code>fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the experiment name labels. Defaults to 9.</p> </li> <li> <code>axis_fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the x-axis ticklabels. Defaults to None, in which case the fontsize will be used.</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the required precision of the presented numbers. Defaults to 4.</p> </li> <li> <code>edge_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the histogram or KDE edge. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>plot_min_sig_diff_lines</code>               (<code>bool</code>)           \u2013            <p>whether to plot the borders of the ROPE, the lines of minimal significance. Defaults to True.</p> </li> <li> <code>min_sig_diff_lines_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the lines of minimal significance. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>min_sig_diff_lines_format</code>               (<code>str</code>)           \u2013            <p>the format of the lines of minimal significance. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>rope_area_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the ROPE area. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"gray\".</p> </li> <li> <code>rope_area_alpha</code>               (<code>float</code>)           \u2013            <p>the opacity of the ROPE area. Corresponds to matplotlib's <code>alpha</code> parameter. Defaults to 0.5.</p> </li> <li> <code>neg_sig_diff_area_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the negatively significant area. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"red\".</p> </li> <li> <code>neg_sig_diff_area_alpha</code>               (<code>float</code>)           \u2013            <p>the opacity of the negatively significant area. Corresponds to matplotlib's <code>alpha</code> parameter. Defaults to 0.5.</p> </li> <li> <code>pos_sig_diff_area_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the positively significant area. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"green\".</p> </li> <li> <code>pos_sig_diff_area_alpha</code>               (<code>float</code>)           \u2013            <p>the opacity of the positively significant area. Corresponds to matplotlib's <code>alpha</code> parameter. Defaults to 0.5.</p> </li> <li> <code>plot_obs_point</code>               (<code>bool</code>)           \u2013            <p>whether to plot the observed value as a marker. Defaults to True.</p> </li> <li> <code>obs_point_marker</code>               (<code>str</code>)           \u2013            <p>the marker type of the observed value. Corresponds to matplotlib's <code>marker</code> parameter. Defaults to \"D\".</p> </li> <li> <code>obs_point_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the observed marker. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>obs_point_size</code>               (<code>float</code>)           \u2013            <p>the size of the observed marker. Defaults to None.</p> </li> <li> <code>plot_median_line</code>               (<code>bool</code>)           \u2013            <p>whether to plot the median line. Defaults to True.</p> </li> <li> <code>median_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the median line. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>median_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the median line. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"--\".</p> </li> <li> <code>plot_extrema_lines</code>               (<code>bool</code>)           \u2013            <p>description. Defaults to True.</p> </li> <li> <code>plot_extrema_lines</code>               (<code>bool</code>)           \u2013            <p>whether to plot small lines at the distribution extreme values. Defaults to True.</p> </li> <li> <code>extrema_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the extrema lines. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>extrema_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the extrema lines. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>extrema_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the extrema lines. Defaults to 1.</p> </li> <li> <code>extrema_line_height</code>               (<code>float</code>)           \u2013            <p>the maximum height of the extrema lines. Defaults to 12.</p> </li> <li> <code>plot_base_line</code>               (<code>bool</code>)           \u2013            <p>whether to plot a line at the base of the distribution. Defaults to True.</p> </li> <li> <code>base_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the base line. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>base_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the base line. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>base_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the base line. Defaults to 1.</p> </li> <li> <code>plot_proportions</code>               (<code>bool</code>)           \u2013            <p>whether to plot the proportions of the data under the three areas as text. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>matplotlib.figure.Figure: the Matplotlib Figure represenation of the plot</p> </li> </ul>"},{"location":"Reference/Study.html#comparing-many-experiments","title":"Comparing Many Experiments","text":""},{"location":"Reference/Study.html#prob_conf_mat.study.Study.get_listwise_comparsion_result","title":"<code>get_listwise_comparsion_result</code>","text":"<p>Generates a <code>ListwiseComparisonResult</code> comparing all Experiments in this study.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ListwiseComparisonResult</code> (              <code>ListwiseComparisonResult</code> )          \u2013            <p>the unformatted <code>ListwiseComparisonResult</code></p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.report_listwise_comparison","title":"<code>report_listwise_comparison</code>","text":"<p>Reports the probability for an experiment achieving a rank when compared to all other experiments on the same metric.</p> <p>Any probability values smaller than the precision are discarded.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Leave 0 or None if using a multiclass metric. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>table_fmt</code>               (<code>str</code>)           \u2013            <p>the format of the table. If 'records', the raw list of values is returned. If 'pandas' or 'pd', a Pandas DataFrame is returned. In all other cases, it is passed to tabulate. Defaults to tabulate's \"html\".</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the required precision of the presented numbers. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>list | DataFrame | str</code> )          \u2013            <p>the table as a string</p> </li> </ul> <p>Examples:</p> <p>Prints the probability of all experiments achieving a particular rank when compared against all others.</p> <pre><code>&gt;&gt;&gt; print(\n...     study.report_listwise_comparison(\n...         metric=\"acc\",\n...         class_label=0,\n...         table_fmt=\"github\",\n...     ),\n... )\n</code></pre> <pre><code>| Group   | Experiment   |   Rank 1 |   Rank 2 |\n|---------|--------------|----------|----------|\n| GROUP   | EXPERIMENT_B |   0.5013 |   0.4987 |\n| GROUP   | EXPERIMENT_A |   0.4987 |   0.5013 |\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.report_expected_reward","title":"<code>report_expected_reward</code>","text":"<p>Computes the expected reward each experiments should receive.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Leave 0 or None if using a multiclass metric. Defaults to None.</p> </li> <li> <code>rewards</code>               (<code>Float[ArrayLike, 'num_rewards']</code>, default:                   <code>[1.0]</code> )           \u2013            <p>the rewards each rank earns. Should be an ArrayLike with as many entries as there are total number of experiments in this study. If there are fewer rewards, the rewards array is padded with 0s. If there are more rewards than experiments, a ValueError is raised.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>table_fmt</code>               (<code>str</code>)           \u2013            <p>the format of the table. If 'records', the raw list of values is returned. If 'pandas' or 'pd', a Pandas DataFrame is returned. In all other cases, it is passed to tabulate. Defaults to tabulate's \"html\".</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the precision of floats used when printing. Defaults to 2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list | DataFrame | str</code>           \u2013            <p>dict[str, float]: a mapping of experiment name to reward</p> </li> </ul>"},{"location":"Reference/Study.html#aggregating-experiments","title":"Aggregating Experiments","text":"<p>Examples:</p> <p>Plot the distributions and the aggregated distribution for ExperimentGroup 'GROUP'.</p> <pre><code>study.plot_experiment_aggregation(\n    metric=\"acc\",\n    class_label=0,\n    experiment_group=\"GROUP\",\n)\n</code></pre> <p> </p> <p>Examples:</p> <p>Plot the distributions and the aggregated distribution for all ExperimentGroups as a forest plot.</p> <pre><code>study.plot_forest_plot(\n    metric=\"acc\",\n    class_label=0,\n)\n</code></pre> <p> </p>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.report_aggregated_metric_summaries","title":"<code>report_aggregated_metric_summaries</code>","text":"<p>Reports on the aggregation of Experiments in all ExperimentGroups.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>table_fmt</code>               (<code>str</code>)           \u2013            <p>the format of the table. If 'records', the raw list of values is returned. If 'pandas' or 'pd', a Pandas DataFrame is returned. In all other cases, it is passed to tabulate. Defaults to tabulate's \"html\".</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the precision of floats used when printing. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>list | DataFrame | str</code> )          \u2013            <p>the table with experiment aggregation statistics as a string</p> </li> </ul> <p>Examples:</p> <p>Report on the aggregated accuracy scores for each ExperimentGroup in this Study</p> <pre><code>&gt;&gt;&gt; print(\n...     study.report_aggregated_metric_summaries(\n...         metric=\"acc\",\n...         class_label=0,\n...         table_fmt=\"github\",\n...     ),\n... )\n</code></pre> <pre><code>| Group   |   Median |   Mode |              HDI |     MU |   Kurtosis |    Skew |   Var. Within |   Var. Between |     I2 |\n|---------|----------|--------|------------------|--------|------------|---------|---------------|----------------|--------|\n| GROUP   |   0.7884 | 0.7835 | [0.7413, 0.8411] | 0.0997 |    -0.0121 | -0.0161 |        0.0013 |         0.0019 | 59.04% |\n</code></pre>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.plot_experiment_aggregation","title":"<code>plot_experiment_aggregation</code>","text":"<p>Plots the distrbution of sampled metric values for a specific experiment group, with the aggregated distribution, for a particular metric and class combination.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>experiment_group</code>               (<code>str</code>)           \u2013            <p>the name of the experiment group</p> </li> <li> <code>observed_values</code>               (<code>dict[str, ExperimentResult]</code>)           \u2013            <p>the observed metric values</p> </li> <li> <code>sampled_values</code>               (<code>dict[str, ExperimentResult]</code>)           \u2013            <p>the sampled metric values</p> </li> <li> <code>metric</code>               (<code>Metric | AveragedMetric</code>)           \u2013            <p>the metric</p> </li> <li> <code>method</code>               (<code>str</code>)           \u2013            <p>the method for displaying a histogram, provided by Seaborn. Can be either a histogram or KDE. Defaults to \"kde\".</p> </li> <li> <code>bandwidth</code>               (<code>float</code>)           \u2013            <p>the bandwith parameter for the KDE. Corresponds to Seaborn's <code>bw_adjust</code> parameter. Defaults to 1.0.</p> </li> <li> <code>bins</code>               (<code>int | list[int] | str</code>)           \u2013            <p>the number of bins to use in the histrogram. Corresponds to numpy's <code>bins</code> parameter. Defaults to \"auto\".</p> </li> <li> <code>normalize</code>               (<code>bool</code>)           \u2013            <p>if normalized, each distribution will be scaled to [0, 1]. Otherwise, uses a shared y-axis. Defaults to False.</p> </li> <li> <code>figsize</code>               (<code>tuple[float, float]</code>)           \u2013            <p>the figure size, in inches. Corresponds to matplotlib's <code>figsize</code> parameter. Defaults to None, in which case a decent default value will be approximated.</p> </li> <li> <code>fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the experiment name labels. Defaults to 9.</p> </li> <li> <code>axis_fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the x-axis ticklabels. Defaults to None, in which case the fontsize will be used.</p> </li> <li> <code>edge_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the histogram or KDE edge. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>area_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the histogram or KDE filled area. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"gray\".</p> </li> <li> <code>area_alpha</code>               (<code>float</code>)           \u2013            <p>the opacity of the histogram or KDE filled area. Corresponds to matplotlib's <code>alpha</code> parameter. Defaults to 0.5.</p> </li> <li> <code>plot_median_line</code>               (<code>bool</code>)           \u2013            <p>whether to plot the median line. Defaults to True.</p> </li> <li> <code>median_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the median line. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>median_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the median line. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"--\".</p> </li> <li> <code>plot_hdi_lines</code>               (<code>bool</code>)           \u2013            <p>whether to plot the HDI lines. Defaults to True.</p> </li> <li> <code>hdi_lines_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the HDI lines. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>hdi_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the HDI lines. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>plot_obs_point</code>               (<code>bool</code>)           \u2013            <p>whether to plot the observed value as a marker. Defaults to True.</p> </li> <li> <code>obs_point_marker</code>               (<code>str</code>)           \u2013            <p>the marker type of the observed value. Corresponds to matplotlib's <code>marker</code> parameter. Defaults to \"D\".</p> </li> <li> <code>obs_point_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the observed marker. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>obs_point_size</code>               (<code>float</code>)           \u2013            <p>the size of the observed marker. Defaults to None.</p> </li> <li> <code>plot_extrema_lines</code>               (<code>bool</code>)           \u2013            <p>whether to plot small lines at the distribution extreme values. Defaults to True.</p> </li> <li> <code>extrema_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the extrema lines. Defaults to \"black\".</p> </li> <li> <code>extrema_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the extrema lines. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>extrema_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the extrema lines. Defaults to 1.</p> </li> <li> <code>extrema_line_height</code>               (<code>float</code>)           \u2013            <p>the maximum height of the extrema lines. Defaults to 12.</p> </li> <li> <code>plot_base_line</code>               (<code>bool</code>)           \u2013            <p>whether to plot a line at the base of the distribution. Defaults to True.</p> </li> <li> <code>base_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the base line. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>base_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the base line. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>base_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the base line. Defaults to 1.</p> </li> <li> <code>plot_experiment_name</code>               (<code>bool</code>)           \u2013            <p>whether to plot the experiment names as labels. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>matplotlib.figure.Figure: the completed figure of the distribution plot</p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.study.Study.plot_forest_plot","title":"<code>plot_forest_plot</code>","text":"<p>Plots the distributions for a metric for each Experiment and aggregated ExperimentGroup.</p> <p>Uses a forest plot format.</p> <p>The median and HDIs of individual Experiment distributions are plotted as squares, and the aggregate distribution is plotted as a diamond below it. Also provides summary statistics bout each distribution, and the aggregation.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>str</code>)           \u2013            <p>the name of the metric</p> </li> <li> <code>class_label</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>the class label. Defaults to None.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>figsize</code>               (<code>tuple[float, float]</code>)           \u2013            <p>the figure size, in inches. Corresponds to matplotlib's <code>figsize</code> parameter. Defaults to None, in which case a decent default value will be approximated.</p> </li> <li> <code>fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the experiment name labels. Defaults to 9.</p> </li> <li> <code>axis_fontsize</code>               (<code>float</code>)           \u2013            <p>fontsize for the x-axis ticklabels. Defaults to None, in which case the fontsize will be used.</p> </li> <li> <code>fontname</code>               (<code>str</code>)           \u2013            <p>the name of the font used. Corresponds to matplotlib's font <code>family</code> parameter. Defaults to \"monospace\".</p> </li> <li> <code>median_marker</code>               (<code>str</code>)           \u2013            <p>the marker type of the median value marker of the individual Experiment distributions. Corresponds to matplotlib's <code>marker</code> parameter. Defaults to \"s\".</p> </li> <li> <code>median_marker_edge_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the individual Experiment median markers' edges. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>median_marker_face_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the individual Experiment median markers. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"white\".</p> </li> <li> <code>median_marker_size</code>               (<code>float</code>)           \u2013            <p>the size of the individual Experiment median markers. Defaults to None.</p> </li> <li> <code>median_marker_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the aggregated median line. Defaults to 1.5.</p> </li> <li> <code>agg_offset</code>               (<code>int</code>)           \u2013            <p>the number of empty rows between the last Experiment and the aggregated row. Defaults to 1.</p> </li> <li> <code>agg_median_marker</code>               (<code>str</code>)           \u2013            <p>the marker type of the median value marker of the aggregated distribution. Corresponds to matplotlib's <code>marker</code> parameter. Defaults to \"D\".</p> </li> <li> <code>agg_median_marker_edge_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the aggregated median markers' edges. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>agg_median_marker_face_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the aggregated median marker. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"white\".</p> </li> <li> <code>agg_median_marker_size</code>               (<code>float</code>)           \u2013            <p>the size of the individual aggregated median marker. Defaults to 10.</p> </li> <li> <code>agg_median_marker_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the aggregated median marker. Defaults to 1.5.</p> </li> <li> <code>hdi_lines_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the HDI lines. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>hdi_lines_format</code>               (<code>str</code>)           \u2013            <p>the format of the HDI lines. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"-\".</p> </li> <li> <code>hdi_lines_width</code>               (<code>int</code>)           \u2013            <p>the width of the HDI lines. Defaults to 1.</p> </li> <li> <code>plot_agg_median_line</code>               (<code>bool</code>)           \u2013            <p>whether to plot the a line through the aggregated median through all other Experiments in the ExperimentGroup. Defaults to True.</p> </li> <li> <code>agg_median_line_colour</code>               (<code>str</code>)           \u2013            <p>the colour of the aggregated median line. Corresponds to matplotlib's <code>color</code> parameter. Defaults to \"black\".</p> </li> <li> <code>agg_median_line_format</code>               (<code>str</code>)           \u2013            <p>the format of the aggregated median line. Corresponds to matplotlib's <code>linestyle</code> parameter. Defaults to \"--\".</p> </li> <li> <code>agg_median_line_width</code>               (<code>float</code>)           \u2013            <p>the width of the aggregated median line. Defaults to 1.0.</p> </li> <li> <code>plot_experiment_name</code>               (<code>bool</code>)           \u2013            <p>whether to plot the name of the individual Experiments. Defaults to True.</p> </li> <li> <code>experiment_name_padding</code>               (<code>int</code>)           \u2013            <p>the padding between the experiment names and the forest plot. Defaults to 0.</p> </li> <li> <code>plot_experiment_info</code>               (<code>bool</code>)           \u2013            <p>whether to plot statistics of the individual and aggregated distributions. Defaults to True.</p> </li> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>the required precision of the presented numbers. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>matplotlib.figure.Figure: the Matplotlib Figure represenation of the forest plot</p> </li> </ul>"},{"location":"Reference/Study.html#prob_conf_mat.config.Config","title":"<code>Config</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config-attributes","title":"Attributes","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config.fingerprint","title":"<code>fingerprint</code>  <code>property</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config.seed","title":"<code>seed</code>  <code>property</code> <code>writable</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config.num_samples","title":"<code>num_samples</code>  <code>property</code> <code>writable</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config.ci_probability","title":"<code>ci_probability</code>  <code>property</code> <code>writable</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config.experiments","title":"<code>experiments</code>  <code>property</code> <code>writable</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config.metrics","title":"<code>metrics</code>  <code>property</code> <code>writable</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config-functions","title":"Functions","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config._validate_seed","title":"<code>_validate_seed</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config._validate_num_samples","title":"<code>_validate_num_samples</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config._validate_ci_probability","title":"<code>_validate_ci_probability</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config._validate_experiments","title":"<code>_validate_experiments</code>","text":""},{"location":"Reference/Study.html#prob_conf_mat.config.Config._validate_metrics","title":"<code>_validate_metrics</code>","text":""},{"location":"Reference/Experiment%20Aggregation/index.html","title":"Experiment Aggregators","text":""},{"location":"Reference/Experiment%20Aggregation/index.html#prob_conf_mat.experiment_aggregation.aggregators.SingletonAggregator","title":"<code>SingletonAggregator</code>","text":"<p>               Bases: <code>ExperimentAggregator</code></p> <p>An aggregation to apply to an ExperimentGroup that needs no aggregation.</p> <p>For example, the ExperimentGroup only contains one Experiment.</p> <p>Essentially just the identity function:</p> \\[f(x)=x\\]"},{"location":"Reference/Experiment%20Aggregation/index.html#prob_conf_mat.experiment_aggregation.aggregators.BetaAggregator","title":"<code>BetaAggregator</code>","text":"<p>               Bases: <code>ExperimentAggregator</code></p> <p>Samples from the beta-conflated distribution.</p> <p>Specifically, the aggregate distribution \\(\\text{Beta}(\\tilde{\\alpha}, \\tilde{\\beta})\\) is estimated as:</p> \\[\\begin{aligned}     \\tilde{\\alpha}&amp;=\\left[\\sum_{i=1}^{M}\\alpha_{i}\\right]-\\left(M-1\\right) \\     \\tilde{\\beta}&amp;=\\left[\\sum_{i=1}^{M}\\beta_{i}\\right]-\\left(M-1\\right) \\end{aligned}\\] <p>where \\(M\\) is the total number of experiments.</p> <p>Uses <code>scipy.stats.beta</code> class to fit beta-distributions.</p> <ul> <li>the individual experiment distributions are beta distributed</li> <li>the metrics are bounded, although the range need not be (0, 1)</li> </ul> Read more: <ol> <li>Hill, T. P. (2008). Conflations Of Probability Distributions: An Optimal Method For     Consolidating Data From Different Experiments.</li> <li>Hill, T. P., &amp; Miller, J. (2011). How to combine independent data sets for the same quantity.</li> <li>'Beta distribution' on Wikipedia</li> </ol> <p>Parameters:</p> <ul> <li> <code>estimation_method</code>               (<code>str</code>, default:                   <code>'mle'</code> )           \u2013            <p>method for estimating the parameters of the individual experiment distributions. Options are 'mle' for maximum-likelihood estimation, or 'mome' for the method of moments estimator. MLE tends be more efficient but is difficult to estimate</p> </li> </ul>"},{"location":"Reference/Experiment%20Aggregation/index.html#prob_conf_mat.experiment_aggregation.aggregators.GammaAggregator","title":"<code>GammaAggregator</code>","text":"<p>               Bases: <code>ExperimentAggregator</code></p> <p>Samples from the Gamma-conflated distribution.</p> <p>Specifically, the aggregate distribution \\(\\text{Gamma}(\\tilde{\\alpha}, \\tilde{\\beta})\\) (\\(\\alpha\\) is the shape, \\(\\beta\\) the rate parameter) is estimated as:</p> \\[\\begin{aligned}     \\tilde{\\alpha}&amp;=\\left[\\sum_{i}^{M}\\alpha_{i}\\right]-(M-1) \\     \\tilde{\\beta}&amp;=\\dfrac{1}{\\sum_{i}^{M}\\beta_{i}^{-1}} \\end{aligned}\\] <p>where \\(M\\) is the total number of experiments.</p> <p>An optional <code>shifted: bool</code> argument exists to dynamically estimate the support for the distribution. Can help fit to individual experiments, but likely minimally impacts the aggregate distribution.</p> <ul> <li>the individual experiment distributions are gamma distributed</li> </ul> Read more: <ol> <li>Hill, T. (2008). Conflations Of Probability Distributions: An Optimal Method For Consolidating Data From Different Experiments.</li> <li>Hill, T., &amp; Miller, J. (2011). How to combine independent data sets for the same quantity.</li> <li>'Gamma distribution' on Wikipedia</li> </ol>"},{"location":"Reference/Experiment%20Aggregation/index.html#prob_conf_mat.experiment_aggregation.aggregators.FEGaussianAggregator","title":"<code>FEGaussianAggregator</code>","text":"<p>               Bases: <code>ExperimentAggregator</code></p> <p>Samples from the Gaussian-conflated distribution.</p> <p>This is equivalent to the fixed-effects meta-analytical estimator.</p> <p>Uses the inverse variance weighted mean and standard errors. Specifically, the aggregate distribution \\(\\mathcal{N}(\\tilde{\\mu}, \\tilde{\\sigma})\\) is estimated as:</p> \\[\\begin{aligned}     w_{i}&amp;=\\dfrac{\\sigma_{i}^{-2}}{\\sum_{j}^{M}\\sigma_{j}^{-2}} \\\\     \\tilde{\\mu}&amp;=\\sum_{i}^{M}w_{i}\\mu_{i} \\\\     \\tilde{\\sigma^2}&amp;=\\dfrac{1}{\\sum_{i}^{M}\\sigma_{i}^{-2}} \\end{aligned}\\] <p>where \\(M\\) is the total number of experiments.</p> <ul> <li>the individual experiment distributions are normally (Gaussian) distributed</li> <li>there is no inter-experiment heterogeneity present</li> </ul> Read more: <ol> <li>Hill, T. (2008). Conflations Of Probability Distributions: An Optimal Method For Consolidating Data From Different Experiments.</li> <li>Hill, T., &amp; Miller, J. (2011). How to combine independent data sets for the same quantity.</li> <li>Higgins, J., &amp; Thomas, J. (Eds.). (2023). Cochrane handbook for systematic reviews of interventions.</li> <li>Borenstein et al. (2021). Introduction to meta-analysis.</li> <li>'Meta-analysis' on Wikipedia</li> </ol>"},{"location":"Reference/Experiment%20Aggregation/index.html#prob_conf_mat.experiment_aggregation.aggregators.REGaussianAggregator","title":"<code>REGaussianAggregator</code>","text":"<p>               Bases: <code>ExperimentAggregator</code></p> <p>Samples from the Random Effects Meta-Analytical Estimator.</p> <p>First uses the standard the inverse variance weighted mean and standard errors as model parameters, before debiasing the weights to incorporate inter-experiment heterogeneity. As a result, studies with larger standard errors will be upweighted relative to the fixed-effects model.</p> <p>Specifically, starting with a Fixed-Effects model \\(\\mathcal{N}(\\tilde{\\mu_{\\text{FE}}}, \\tilde{\\sigma_{\\text{FE}}})\\),</p> \\[\\begin{aligned}     w_{i}&amp;=\\dfrac{\\left(\\sigma_{i}^2+\\tau^2\\right)^{-1}}{\\sum_{j}^{M}\\left(\\sigma_{j}^2+\\tau^2\\right)^{-1}} \\\\     \\tilde{\\mu}&amp;=\\sum_{i}^{M}w_{i}\\mu_{i} \\\\     \\tilde{\\sigma^2}&amp;=\\dfrac{1}{\\sum_{i}^{M}\\sigma_{i}^{-2}} \\end{aligned}\\] <p>where \\(\\tau\\) is the estimated inter-experiment heterogeneity, and \\(M\\) is the total number of experiments.</p> <p>Uses the Paule-Mandel iterative heterogeneity estimator, which does not make a parametric assumption. The more common (but biased) DerSimonian-Laird estimator can also be used by setting <code>paule_mandel_heterogeneity: bool = False</code>.</p> <p>If <code>hksj_sampling_distribution: bool = True</code>, the aggregated distribution is a more conservative \\(t\\)-distribution, with degrees of freedom equal to \\(M-1\\). This is especially more conservative when there are only a few experiments available, and can substantially increase the aggregated distribution's variance.</p> <ul> <li>the individual experiment distributions are normally (Gaussian) distributed</li> <li>there is inter-experiment heterogeneity present</li> </ul> Read more: <ol> <li>Higgins, J., &amp; Thomas, J. (Eds.). (2023). Cochrane handbook for systematic reviews of interventions.</li> <li>Borenstein et al. (2021). Introduction to meta-analysis.</li> <li>'Meta-analysis' on Wikipedia</li> <li>IntHout, J., Ioannidis, J. P., &amp; Borm, G. F. (2014). The Hartung-Knapp-Sidik-Jonkman method for random effects meta-analysis is straightforward and considerably outperforms the standard DerSimonian-Laird method.</li> <li>Langan et al. (2019). A comparison of heterogeneity variance estimators in simulated random\u2010effects meta\u2010analyses.</li> </ol> <p>Parameters:</p> <ul> <li> <code>paule_mandel_heterogeneity</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to use the Paule-Mandel method for estimating inter-experiment heterogeneity, or fallback to the DerSimonian-Laird estimator. Defaults to True.</p> </li> <li> <code>hksj_sampling_distribution</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use the Hartung-Knapp-Sidik-Jonkman corrected \\(t\\)-distribition as the aggregate sampling distribution. Defaults to False.</p> </li> </ul>"},{"location":"Reference/Experiment%20Aggregation/index.html#prob_conf_mat.experiment_aggregation.aggregators.HistogramAggregator","title":"<code>HistogramAggregator</code>","text":"<p>               Bases: <code>ExperimentAggregator</code></p> <p>Samples from a histogram approximate conflation distribution.</p> <p>First bins all individual experiment groups, and then computes the product of the probability masses across individual experiments.</p> <p>Unlike other methods, this does not make a parametric assumption. However, the resulting distribution can 'look' unnatural, and requires overlapping supports within the sample. If any experiment assigns 0 probability mass to any bin, the conflated bin will also contain 0 probability mass.</p> <p>As such, inter-experiment heterogeneity can be a significant problem.</p> <p>Uses numpy.histogram_bin_edges to estimate the number of bin edges needed per experiment, and takes the smallest across all experiments for the aggregate distribution.</p> <ul> <li>the individual experiment distributions' supports overlap</li> </ul> Read more: <ol> <li>Hill, T. (2008). Conflations Of Probability Distributions: An Optimal Method For Consolidating Data From Different Experiments.</li> <li>Hill, T., &amp; Miller, J. (2011). How to combine independent data sets for the same quantity.</li> </ol>"},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html","title":"Heterogeneity","text":"<p>With experiment aggregation, <code>prob_conf_mat</code> tries to estimate the distribution of the average experiment in an ExperimentGroup. Implicitly, this assumes that all experiments come from the same distribution, and that all between experiment variance can be explained by random noise.</p> <p>This is not always the case. For example, if the experiments represent the same model tested on different benchmarks (or different models tested on the same benchmark). In these cases, inter-experiment heterogeneity can exist.</p> <p>Heterogeneity can lead to large inter- (or between) experiment variance, which in turn can make estimating an aggregate difficult. The methods in this module try to estimate the degree of heterogeneity present, so users are better informed as to the quality of the experiment aggregation.</p> <p>See the guide on experiment aggregation for more details.</p>"},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html#prob_conf_mat.experiment_aggregation.heterogeneity.HeterogeneityResult","title":"<code>HeterogeneityResult</code>  <code>dataclass</code>","text":"<p>The output of a heterogeneity computation.</p>"},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html#prob_conf_mat.experiment_aggregation.heterogeneity.HeterogeneityResult-functions","title":"Functions","text":""},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html#prob_conf_mat.experiment_aggregation.heterogeneity.HeterogeneityResult.template_sentence","title":"<code>template_sentence</code>","text":"<p>Fills a template string with some standard summary statistics.</p>"},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html#prob_conf_mat.experiment_aggregation.heterogeneity.heterogeneity_dl","title":"<code>heterogeneity_dl</code>","text":"<p>Compute the DerSimonian-Laird estimate of between-experiment heterogeneity.</p> <p>Parameters:</p> <ul> <li> <code>means</code>               (<code>Float[ndarray, 'num_experiments']</code>)           \u2013            <p>the experiment means</p> </li> <li> <code>variances</code>               (<code>Float[ndarray, 'num_experiments']</code>)           \u2013            <p>the experiment variances</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>estimate of the between-experiment heterogeneity</p> </li> </ul>"},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html#prob_conf_mat.experiment_aggregation.heterogeneity.heterogeneity_pm","title":"<code>heterogeneity_pm</code>","text":"<p>Compute the Paule-Mandel estimate of between-experiment heterogeneity.</p> <p>Based on the <code>_fit_tau_iterative</code> function from <code>stats_models</code>.</p> <p>Original implementation is based on Appendix A of [1]</p> We make two modifications <ol> <li>instead of stopping iteration if F(tau_2) &lt; 0, we back-off to the midpoint     between the current and previous estimate</li> <li>optionally, we apply the Viechtbauer correction to the root. Instead of     converging to the mean, converge to the median</li> </ol> Read More: <ol> <li>DerSimonian, R., &amp; Kacker, R. (2007). Random-effects model for meta-analysis     of clinical trials: an update. Contemporary clinical trials, 28(2), 105-114.</li> </ol> <p>Parameters:</p> <ul> <li> <code>means</code>               (<code>Float[ndarray, ' num_experiments']</code>)           \u2013            <p>the experiment means</p> </li> <li> <code>variances</code>               (<code>Float[ndarray, ' num_experiments']</code>)           \u2013            <p>the experiment variances</p> </li> <li> <code>init_tau2</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>the inital tau2 estimate. Defaults to 0.0.</p> </li> <li> <code>atol</code>               (<code>float</code>, default:                   <code>1e-05</code> )           \u2013            <p>when to assume convergence. Defaults to 1e-5.</p> </li> <li> <code>maxiter</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>the maximum number of iterations needed. Defaults to 50.</p> </li> <li> <code>use_viechtbauer_correction</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use the Viechtbauer correction. Very new. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>estimate of the between-experiment heterogeneity</p> </li> </ul>"},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html#prob_conf_mat.experiment_aggregation.heterogeneity.estimate_i2","title":"<code>estimate_i2</code>","text":"<p>Estimates a generalised \\(I^2\\) metric, as suggested by Bowden et al. [1].</p> <p>It measures the amount of variance attributable to within-experiment variance vs. between-experiment variance. The between experiment variance is estimated using a Paule-Mandel tau2 estimator.</p> Read more: <ol> <li>Bowden, J., Tierney, J. F., Copas, A. J., &amp; Burdett, S. (2011). Quantifying,     displaying and accounting for heterogeneity in the meta-analysis of RCTs     using standard and generalised Qstatistics. BMC medical research     methodology, 11(1), 1-12.</li> </ol> <p>Parameters:</p> <ul> <li> <code>individual_samples</code>               (<code>Float[ndarray, 'num_samples num_experiments']</code>)           \u2013            <p>the samples from individual experiments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>HeterogeneityResult</code> )          \u2013            <p>the \\(I^2\\) estimate</p> </li> </ul>"},{"location":"Reference/Experiment%20Aggregation/heterogeneity.html#prob_conf_mat.experiment_aggregation.heterogeneity.interpret_i2","title":"<code>interpret_i2</code>","text":"<p>Interprets \\(I^2\\) values using prescribed guidelines [1].</p> Read More: <ol> <li>Higgins, J. P., &amp; Green, S. (Eds.). (2008). Cochrane handbook for     systematic reviews of interventions.</li> </ol> <p>Parameters:</p> <ul> <li> <code>i2_score</code>               (<code>float</code>)           \u2013            <p>the I2 estimate</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>Literal['insignificant heterogeneity', 'borderline moderate heterogeneity', 'moderate heterogeneity', 'borderline substantial heterogeneity', 'borderline considerable heterogeneity', 'considerable heterogeneity', 'unknown']</code> )          \u2013            <p>a rough interpretation of the magnitude of I2</p> </li> </ul>"},{"location":"Reference/Metrics/index.html","title":"Metrics and Averaging","text":"<p>Metrics, within the scope of this project, summarize a model's performance on some test set. It does so by comparing the model's class predictions against a paired set of condition labels (i.e. the ground truth class). The value a metric function spits out, should tell you something about the model's classification performance, whether it's good, bad or something in between.</p> <p>Metrics can be either:</p> <ol> <li>multiclass, in which case they spit out a single value that combines all classes in one go</li> <li>binary, in which case they compute a value for each class individually</li> </ol> <p>Usually, the former is a better indication for the overall performance of the model, whereas the latter provides more (usually supporting) fine-grained detail. To convert a binary metric into a multiclass metric, it can be composed with an averaging method. The averaging method takes in the \\(k\\) dimensional array of metric values (where \\(k\\) the number of classes), and yields a scalar value that combines all the per-class values.</p>"},{"location":"Reference/Metrics/index.html#interface","title":"Interface","text":"<p>Usually, you will not be interacting with the metrics themselves. Instead, this library provides users with high-level methods for defining metrics and collections of metrics. The easiest method for constructing metrics is by passing a metric syntax string.</p> <p>A valid metric syntax string consists of (in order):</p> <ol> <li>[Required] A registered metric alias (see below)</li> <li>[Required] Any keyword arguments that need to be passed to the metric function</li> <li>[Optional] An <code>@</code> symbol</li> <li>[Optional] A registered averaging method alias (see below)</li> <li>[Optional] Any keyword arguments that need to be passed to the averaging function</li> </ol> <p>No spaces should be used. Instead, keywords arguments start with a <code>+</code> prepended to the key, followed by a <code>=</code> and the value. All together:</p> <pre><code>&lt;metric-alias&gt;+&lt;arg-key&gt;=&lt;arg-val&gt;@&lt;avg-method-alias&gt;+&lt;arg-key&gt;=&lt;arg-value&gt;\n</code></pre> <p>Only numeric (float, int) or string arguments are accepted. The strings \"None\", \"True\" and \"False\" are converted to their Pythonic counterpart. The order of the keyword arguments does not matter, as long as they appear in the correct block.</p>"},{"location":"Reference/Metrics/index.html#examples","title":"Examples","text":"<ol> <li><code>f1</code>: the F1 score</li> <li><code>mcc</code>: the MCC score</li> <li><code>ppv</code>: the Positive Predictive Value</li> <li><code>precision</code>: also Positive Predictive Value, as its a registered alias (see below)</li> <li><code>fbeta+beta=3.0</code>: the F3 score</li> <li><code>f1@macro</code>: the macro averaged F1 score</li> <li><code>ba+adjusted=True@binary+positive_class=2</code>: the chance-correct balanced accuracy score, but only for class 2 (starting at 0)</li> <li><code>p4@geometric</code>: the geometric mean of the P4 scores</li> <li><code>mcc@harmonic</code>: the MCC score, since it's already a multiclass metric, the averaging is ignored</li> </ol>"},{"location":"Reference/Metrics/index.html#metrics","title":"Metrics","text":"<p>The following lists all implemented metrics, by alias</p> Alias Metric Multiclass sklearn 'acc' <code>Accuracy</code> True accuracy_score 'accuracy' <code>Accuracy</code> True accuracy_score 'ba' <code>BalancedAccuracy</code> True balanced_accuracy_score 'balanced_accuracy' <code>BalancedAccuracy</code> True balanced_accuracy_score 'bookmaker_informedness' <code>Informedness</code> False 'cohen_kappa' <code>CohensKappa</code> True cohen_kappa_score 'critical_success_index' <code>JaccardIndex</code> False jaccard_score 'delta_p' <code>Markedness</code> False 'diag_mass' <code>DiagMass</code> False 'diagnostic_odds_ratio' <code>DiagnosticOddsRatio</code> False 'dor' <code>DiagnosticOddsRatio</code> False 'f1' <code>F1</code> False f1_score 'fall-out' <code>FalsePositiveRate</code> False 'fall_out' <code>FalsePositiveRate</code> False 'false_discovery_rate' <code>FalseDiscoveryRate</code> False 'false_negative_rate' <code>FalseNegativeRate</code> False 'false_omission_rate' <code>FalseOmissionRate</code> False 'false_positive_rate' <code>FalsePositiveRate</code> False 'fbeta' <code>FBeta</code> False fbeta_score 'fdr' <code>FalseDiscoveryRate</code> False 'fnr' <code>FalseNegativeRate</code> False 'for' <code>FalseOmissionRate</code> False 'fpr' <code>FalsePositiveRate</code> False 'hit_rate' <code>TruePositiveRate</code> False 'informedness' <code>Informedness</code> False 'jaccard' <code>JaccardIndex</code> False jaccard_score 'jaccard_index' <code>JaccardIndex</code> False jaccard_score 'kappa' <code>CohensKappa</code> True cohen_kappa_score 'ldor' <code>LogDiagnosticOddsRatio</code> False 'lnlr' <code>LogNegativeLikelihoodRatio</code> False class_likelihood_ratios 'log_diagnostic_odds_ratio' <code>LogDiagnosticOddsRatio</code> False 'log_dor' <code>LogDiagnosticOddsRatio</code> False 'log_negative_likelihood_ratio' <code>LogNegativeLikelihoodRatio</code> False class_likelihood_ratios 'log_nlr' <code>LogNegativeLikelihoodRatio</code> False class_likelihood_ratios 'log_plr' <code>LogPositiveLikelihoodRatio</code> False class_likelihood_ratios 'log_positive_likelihood_ratio' <code>LogPositiveLikelihoodRatio</code> False class_likelihood_ratios 'lplr' <code>LogPositiveLikelihoodRatio</code> False class_likelihood_ratios 'markedness' <code>Markedness</code> False 'matthews_corrcoef' <code>MatthewsCorrelationCoefficient</code> True matthews_corrcoef 'matthews_correlation_coefficient' <code>MatthewsCorrelationCoefficient</code> True matthews_corrcoef 'mcc' <code>MatthewsCorrelationCoefficient</code> True matthews_corrcoef 'miss_rate' <code>FalseNegativeRate</code> False 'model_bias' <code>ModelBias</code> False 'negative_likelihood_ratio' <code>NegativeLikelihoodRatio</code> False class_likelihood_ratios 'negative_predictive_value' <code>NegativePredictiveValue</code> False 'nlr' <code>NegativeLikelihoodRatio</code> False class_likelihood_ratios 'npv' <code>NegativePredictiveValue</code> False 'p4' <code>P4</code> False 'phi' <code>MatthewsCorrelationCoefficient</code> True matthews_corrcoef 'phi_coefficient' <code>MatthewsCorrelationCoefficient</code> True matthews_corrcoef 'plr' <code>PositiveLikelihoodRatio</code> False class_likelihood_ratios 'positive_likelihood_ratio' <code>PositiveLikelihoodRatio</code> False class_likelihood_ratios 'positive_predictive_value' <code>PositivePredictiveValue</code> False 'ppv' <code>PositivePredictiveValue</code> False 'precision' <code>PositivePredictiveValue</code> False 'prev_thresh' <code>PrevalenceThreshold</code> False 'prevalence' <code>Prevalence</code> False 'prevalence_threshold' <code>PrevalenceThreshold</code> False 'pt' <code>PrevalenceThreshold</code> False 'recall' <code>TruePositiveRate</code> False 'selectivity' <code>TrueNegativeRate</code> False 'sensitivity' <code>TruePositiveRate</code> False 'specificity' <code>TrueNegativeRate</code> False 'threat_score' <code>JaccardIndex</code> False jaccard_score 'tnr' <code>TrueNegativeRate</code> False 'tpr' <code>TruePositiveRate</code> False 'true_negative_rate' <code>TrueNegativeRate</code> False 'true_positive_rate' <code>TruePositiveRate</code> False 'youden_j' <code>Informedness</code> False 'youdenj' <code>Informedness</code> False"},{"location":"Reference/Metrics/index.html#averaging","title":"Averaging","text":"<p>The following lists all implemented metric averaging methods, by alias</p> Alias Metric sklearn 'binary' <code>SelectPositiveClass</code> binary 'geom' <code>GeometricMean</code> 'geometric' <code>GeometricMean</code> 'harm' <code>HarmonicMean</code> 'harmonic' <code>HarmonicMean</code> 'macro' <code>MacroAverage</code> macro 'macro_average' <code>MacroAverage</code> macro 'mean' <code>MacroAverage</code> macro 'micro' <code>WeightedAverage</code> weighted 'micro_average' <code>WeightedAverage</code> weighted 'select' <code>SelectPositiveClass</code> binary 'select_positive' <code>SelectPositiveClass</code> binary 'weighted' <code>WeightedAverage</code> weighted 'weighted_average' <code>WeightedAverage</code> weighted"},{"location":"Reference/Metrics/Averaging.html","title":"Averaging","text":""},{"location":"Reference/Metrics/Averaging.html#abstract-base-class","title":"Abstract Base Class","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.abc.Averaging","title":"<code>Averaging</code>","text":"<p>The abstract base class for metric averaging.</p> <p>Properties should be implemented as class attributes in derived metrics.</p> <p>The <code>compute_average</code> method needs to be implemented</p>"},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.abc.Averaging-attributes","title":"Attributes","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.abc.Averaging.dependencies","title":"<code>dependencies</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.abc.Averaging.sklearn_equivalent","title":"<code>sklearn_equivalent</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.abc.Averaging.aliases","title":"<code>aliases</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.abc.Averaging-functions","title":"Functions","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.abc.Averaging.compute_average","title":"<code>compute_average</code>  <code>abstractmethod</code>","text":""},{"location":"Reference/Metrics/Averaging.html#metric-instances","title":"Metric Instances","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.MacroAverage","title":"<code>MacroAverage</code>","text":"<p>               Bases: <code>Averaging</code></p> <p>Computes the arithmetic mean over all classes, also known as macro-averaging.</p>"},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.MacroAverage.aliases","title":"<code>aliases = ['macro', 'macro_average', 'mean']</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.MacroAverage.dependencies","title":"<code>dependencies = ()</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.MacroAverage.sklearn_equivalent","title":"<code>sklearn_equivalent = 'macro'</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.WeightedAverage","title":"<code>WeightedAverage</code>","text":"<p>               Bases: <code>Averaging</code></p> <p>Computes the class prevalence weighted mean over all classes, also known as weighted averaging.</p>"},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.WeightedAverage.aliases","title":"<code>aliases = ['weighted', 'weighted_average', 'micro', 'micro_average']</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.WeightedAverage.dependencies","title":"<code>dependencies = ('p_condition',)</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.WeightedAverage.sklearn_equivalent","title":"<code>sklearn_equivalent = 'weighted'</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.SelectPositiveClass","title":"<code>SelectPositiveClass</code>","text":"<p>               Bases: <code>Averaging</code></p> <p>Selects only the positive class, also known as binary averaging.</p>"},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.SelectPositiveClass.aliases","title":"<code>aliases = ['select_positive', 'binary', 'select']</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.SelectPositiveClass.dependencies","title":"<code>dependencies = ()</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.SelectPositiveClass.sklearn_equivalent","title":"<code>sklearn_equivalent = 'binary'</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.HarmonicMean","title":"<code>HarmonicMean</code>","text":"<p>               Bases: <code>Averaging</code></p> <p>Computes the harmonic mean over all classes.</p>"},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.HarmonicMean.aliases","title":"<code>aliases = ['harmonic', 'harm']</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.HarmonicMean.dependencies","title":"<code>dependencies = ()</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.HarmonicMean.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.GeometricMean","title":"<code>GeometricMean</code>","text":"<p>               Bases: <code>Averaging</code></p> <p>Computes the geometric mean over all classes.</p>"},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.GeometricMean.aliases","title":"<code>aliases = ['geometric', 'geom']</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.GeometricMean.dependencies","title":"<code>dependencies = ()</code>","text":""},{"location":"Reference/Metrics/Averaging.html#prob_conf_mat.metrics.averaging.GeometricMean.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html","title":"Metrics","text":""},{"location":"Reference/Metrics/Metrics.html#abstract-base-class","title":"Abstract Base Class","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric","title":"<code>Metric</code>","text":"<p>The abstract base class for metrics.</p> <p>Properties should be implemented as class attributes in derived metrics</p> <p>The <code>compute_metric</code> method needs to be implemented</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric-attributes","title":"Attributes","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric.full_name","title":"<code>full_name</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":"<p>A human-readable name for this metric.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric.is_multiclass","title":"<code>is_multiclass</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":"<p>Whether or not this metric computes a value for each class individually, or for all classes at once.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric.bounds","title":"<code>bounds</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":"<p>A tuple of the minimum and maximum possible value for this metric to take.</p> <p>Can be infinite.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric.dependencies","title":"<code>dependencies</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":"<p>All metrics upon which this metric depends.</p> <p>Used to generate a computation schedule, such that no metric is calculated before its dependencies. The dependencies must match the <code>compute_metric</code> signature. This is checked during class definition.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric.sklearn_equivalent","title":"<code>sklearn_equivalent</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":"<p>The <code>sklearn</code> equivalent function, if applicable</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric.aliases","title":"<code>aliases</code>  <code>abstractmethod</code> <code>instance-attribute</code> <code>property</code>","text":"<p>A list of all valid aliases for this metric. Can be used when creating metric syntax strings.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric-functions","title":"Functions","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.Metric.compute_metric","title":"<code>compute_metric</code>  <code>abstractmethod</code>","text":"<p>Computes the metric values from its dependencies.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.AveragedMetric","title":"<code>AveragedMetric</code>","text":"<p>The abstract base class for the composition of any instance of <code>Metric</code> with any instance of <code>Averaging</code>.</p> <p>Parameters:</p> <ul> <li> <code>metric</code>               (<code>Metric</code>)           \u2013            <p>a binary metric</p> </li> <li> <code>averaging</code>               (<code>Averaging</code>)           \u2013            <p>an averaging method</p> </li> </ul>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.AveragedMetric-attributes","title":"Attributes","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.AveragedMetric.aliases","title":"<code>aliases</code>  <code>property</code>","text":"<p>A list of all valid aliases for this metric.</p> <p>Constructed from the product of the all aliases of the Metric and Averaging methods.</p> <p>Can be used when creating metric syntax strings.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.AveragedMetric.is_multiclass","title":"<code>is_multiclass</code>  <code>property</code>","text":"<p>Whether or not this metric computes a value for each class individually, or for all classes at once.</p> <p>An AveragedMetric is always multiclass.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.AveragedMetric.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>A tuple of the minimum and maximum possible value for this metric to take. Can be infinite.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.AveragedMetric.dependencies","title":"<code>dependencies</code>  <code>property</code>","text":"<p>All metrics upon which this AveragedMetric depends.</p> <p>Constructed from the union of all Metric and AveragingMethod dependencies.</p> <p>Used to generate a computation schedule, such that no metric is calculated before its dependencies.</p> <p>The dependencies must match the <code>compute_metric</code> signature.</p> <p>This is checked during class definition.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics.abc.AveragedMetric.sklearn_equivalent","title":"<code>sklearn_equivalent</code>  <code>property</code>","text":"<p>The <code>sklearn</code> equivalent function, if applicable</p>"},{"location":"Reference/Metrics/Metrics.html#metric-instances","title":"Metric Instances","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagMass","title":"<code>DiagMass</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the mass on the diagonal of the normalized confusion matrix.</p> <p>It is defined as the rate of true positives to all entries:</p> \\[\\mathtt{diag}(\\mathbf{CM})=TP / N\\] <p>where \\(TP\\) are the true positives, and \\(N\\) are the total number of predictions.</p> <p>This is a metric primarily used as a intermediate value for other metrics, and says relatively little on its own.</p> <p>Not to be confused with the True Positive Rate.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagMass.aliases","title":"<code>aliases = ['diag_mass']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagMass.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagMass.dependencies","title":"<code>dependencies = ('norm_confusion_matrix',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagMass.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagMass.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Prevalence","title":"<code>Prevalence</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the marginal distribution of condition occurence. Also known as the prevalence.</p> <p>It can be defined as the rate of positives to all predictions:</p> \\[\\mathtt{Prev}=P / N\\] <p>where \\(P\\) is the count of condition positives, and \\(N\\) are the total number of predictions.</p> <p>This is a metric primarily used as a intermediate value for other metrics, and say relatively little on its own.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Prevalence.aliases","title":"<code>aliases = ['prevalence']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Prevalence.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Prevalence.dependencies","title":"<code>dependencies = ('p_condition',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Prevalence.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Prevalence.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.ModelBias","title":"<code>ModelBias</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the marginal distribution of prediction occurence. Also known as the model bias.</p> <p>It can be defined as the rate of predicted positives to all predictions:</p> \\[\\mathtt{Bias}=PP / N\\] <p>where \\(PP\\) is the count of predicted positives, and \\(N\\) are the total number of predictions.</p> <p>This is a metric primarily used as a intermediate value for other metrics, and say relatively little on its own.</p>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.ModelBias.aliases","title":"<code>aliases = ['model_bias']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.ModelBias.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.ModelBias.dependencies","title":"<code>dependencies = ('p_pred',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.ModelBias.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.ModelBias.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TruePositiveRate","title":"<code>TruePositiveRate</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the True Positive Rate, also known as recall, sensitivity.</p> <p>It is defined as the ratio of correctly predited positives to all condition positives:</p> \\[\\mathtt{TPR}=TP / P\\] <p>where \\(TP\\) are the true positives, and \\(TN\\) are true negatives and \\(N\\) the number of predictions.</p> <p>Essentially, out of all condition positives, how many were correctly predicted. Can be seen as a metric measuring retrieval.</p> <p>Examples:</p> <ul> <li><code>tpr</code></li> <li><code>recall@macro</code></li> </ul> Read more: <ol> <li>scikit-learn</li> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TruePositiveRate.aliases","title":"<code>aliases = ['true_positive_rate', 'sensitivity', 'recall', 'hit_rate', 'tpr']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TruePositiveRate.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TruePositiveRate.dependencies","title":"<code>dependencies = ('p_pred_given_condition',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TruePositiveRate.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TruePositiveRate.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseNegativeRate","title":"<code>FalseNegativeRate</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the False Negative Rate, also known as the miss-rate.</p> <p>It is defined as the ratio of false negatives to condition positives:</p> \\[\\mathtt{FNR}=FN / (TP + FN)\\] <p>where \\(TP\\) are the true positives, and \\(FN\\) are the false negatives.</p> <p>Examples:</p> <ul> <li><code>fnr</code></li> <li><code>false_negative_rate@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseNegativeRate.aliases","title":"<code>aliases = ['false_negative_rate', 'miss_rate', 'fnr']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseNegativeRate.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseNegativeRate.dependencies","title":"<code>dependencies = ('true_positive_rate',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseNegativeRate.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseNegativeRate.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositivePredictiveValue","title":"<code>PositivePredictiveValue</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the Positive Predictive Value, also known as precision.</p> <p>It is defined as the ratio of true positives to predicted positives:</p> \\[\\mathtt{PPV}=TP / (TP + FP)\\] <p>where \\(TP\\) is the count of true positives, and \\(FP\\) the count falsely predicted positives.</p> <p>It is the complement of the False Discovery Rate, \\(PPV=1-FDR\\).</p> <p>Examples:</p> <ul> <li><code>ppv</code></li> <li><code>precision@macro</code></li> </ul> Read more: <ol> <li>scikit-learn</li> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositivePredictiveValue.aliases","title":"<code>aliases = ['positive_predictive_value', 'precision', 'ppv']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositivePredictiveValue.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositivePredictiveValue.dependencies","title":"<code>dependencies = ('p_condition_given_pred',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositivePredictiveValue.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositivePredictiveValue.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseDiscoveryRate","title":"<code>FalseDiscoveryRate</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the False Discovery Rate.</p> <p>It is defined as the ratio of falsely predicted positives to predicted positives:</p> \\[\\mathtt{FDR}=FP / (TP + FP)\\] <p>where \\(TP\\) is the count of true positives, and \\(FP\\) the count of falsely predicted positives.</p> <p>It is the complement of the Positive Predictve Value, \\(FDR=1-PPV\\).</p> <p>Examples:</p> <ul> <li><code>fdr</code></li> <li><code>false_discovery_rate@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.aliases","title":"<code>aliases = ['false_discovery_rate', 'fdr']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.dependencies","title":"<code>dependencies = ('positive_predictive_value',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalsePositiveRate","title":"<code>FalsePositiveRate</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the False Positive Rate, the probability of false alarm.</p> <p>Also known as the fall-out.</p> <p>It is defined as the ratio of falsely predicted positives to condition negatives:</p> \\[\\mathtt{FPR}=FP / (TN + FP)\\] <p>where \\(TN\\) is the count of true negatives, and \\(FP\\) the count of falsely predicted positives.</p> <p>It is the complement of the True Negative Rate, \\(FPR=1-TNR\\).</p> <p>Examples:</p> <ul> <li><code>fpr</code></li> <li><code>fall-out@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalsePositiveRate.aliases","title":"<code>aliases = ['false_positive_rate', 'fall-out', 'fall_out', 'fpr']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalsePositiveRate.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalsePositiveRate.dependencies","title":"<code>dependencies = ('diag_mass', 'p_pred', 'p_condition')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalsePositiveRate.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalsePositiveRate.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TrueNegativeRate","title":"<code>TrueNegativeRate</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the True Negative Rate, i.e. specificity, selectivity.</p> <p>It is defined as the ratio of true predicted negatives to condition negatives:</p> \\[\\mathtt{TNR}=TN / (TN + FP)\\] <p>where \\(TN\\) is the count of true negatives, and FP the count of falsely predicted positives.</p> <p>It is the complement of the False Positive Rate, \\(TNR=1-FPR\\).</p> <p>Examples:</p> <ul> <li><code>tnr</code></li> <li><code>selectivity@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TrueNegativeRate.aliases","title":"<code>aliases = ['true_negative_rate', 'specificity', 'selectivity', 'tnr']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TrueNegativeRate.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TrueNegativeRate.dependencies","title":"<code>dependencies = ('false_positive_rate',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TrueNegativeRate.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.TrueNegativeRate.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseOmissionRate","title":"<code>FalseOmissionRate</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the False Omission Rate.</p> <p>It is defined as the ratio of falsely predicted negatives to all predicted negatives:</p> \\[\\mathtt{FOR}=FN / (TN + FN)\\] <p>where \\(\\(TN\\)\\) is the count of true negatives, and \\(\\(FN\\)\\) the count of falsely predicted negatives.</p> <p>It is the complement of the Negative Predictive Value, \\(FOR=1-NPV\\).</p> <p>Examples:</p> <ul> <li><code>for</code></li> <li><code>false_omission_rate@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseOmissionRate.aliases","title":"<code>aliases = ['false_omission_rate', 'for']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseOmissionRate.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseOmissionRate.dependencies","title":"<code>dependencies = ('p_condition', 'p_pred', 'diag_mass')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseOmissionRate.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FalseOmissionRate.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativePredictiveValue","title":"<code>NegativePredictiveValue</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the Negative Predicitive Value.</p> <p>It is defined as the ratio of true negatives to all predicted negatives:</p> \\[\\mathtt{NPV}=TN / (TN + FN)\\] <p>where TN are the true negatives, and FN are the falsely predicted negatives.</p> <p>It is the complement of the False Omission Rate, \\(NPV=1-FOR\\).</p> <p>Examples:</p> <ul> <li><code>npv</code></li> <li><code>negative_predictive_value@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativePredictiveValue.aliases","title":"<code>aliases = ['negative_predictive_value', 'npv']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativePredictiveValue.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativePredictiveValue.dependencies","title":"<code>dependencies = ('false_omission_rate',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativePredictiveValue.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativePredictiveValue.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the multiclass accuracy score.</p> <p>It is defined as the rate of correct classifications to all classifications:</p> \\[\\mathtt{Acc}=(TP + TN) / N\\] <p>where \\(TP\\) are the true positives, \\(TN\\) the true negatives and \\(N\\) the total number of predictions.</p> <p>Possible values lie in the range [0.0, 1.0], with larger values denoting better performance. The value of a random classifier is dependent on the label distribution, which makes accuracy especially susceptible to class imbalance. It is also not directly comparable across datasets.</p> <p>Examples:</p> <ul> <li><code>acc</code></li> <li><code>accuracy@macro</code></li> </ul> Read more: <ol> <li>scikit-learn</li> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Accuracy.aliases","title":"<code>aliases = ['acc', 'accuracy']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Accuracy.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Accuracy.dependencies","title":"<code>dependencies = ('diag_mass',)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Accuracy.is_multiclass","title":"<code>is_multiclass = True</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Accuracy.sklearn_equivalent","title":"<code>sklearn_equivalent = 'accuracy_score'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.BalancedAccuracy","title":"<code>BalancedAccuracy</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the balanced accuracy score.</p> <p>It is defined as the the arithmetic average of the per-class true-positive rate:</p> \\[\\mathtt{BA}=\\frac{1}{|C|}\\sum TPR_{c}\\] <p>where \\(TPR\\) is the true positive rate (precision).</p> <p>Possible values lie in the range [0.0, 1.0], with larger values denoting better performance. Unlike accuracy, balanced accuracy can be 'chance corrected', such that random performance is yield a score of 0.0. This can be achieved by setting <code>adjusted=True</code>.</p> <p>Examples:</p> <ul> <li><code>ba</code></li> <li><code>balanced_accuracy@macro</code></li> <li><code>ba+adjusted=True</code></li> </ul> Read more: <ol> <li>scikit-learn</li> </ol> <p>Parameters:</p> <ul> <li> <code>adjusted</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether the chance-corrected variant is computed. Defaults to <code>False</code>.</p> </li> </ul>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.BalancedAccuracy.aliases","title":"<code>aliases = ['ba', 'balanced_accuracy']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.BalancedAccuracy.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.BalancedAccuracy.dependencies","title":"<code>dependencies = ('tpr', 'p_condition')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.BalancedAccuracy.is_multiclass","title":"<code>is_multiclass = True</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.BalancedAccuracy.sklearn_equivalent","title":"<code>sklearn_equivalent = 'balanced_accuracy_score'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient","title":"<code>MatthewsCorrelationCoefficient</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the multiclass Matthew's Correlation Coefficient (MCC), also known as the phi coefficient.</p> <p>Goes by a variety of names, depending on the application scenario.</p> <p>A metric that holistically combines many different classification metrics.</p> <p>A perfect classifier scores 1.0, a random classifier 0.0. Values smaller than 0 indicate worse than random performance.</p> <p>It's absolute value is proportional to the square root of the Chi-square test statistic.</p> <p>Quoting Wikipedia:</p> <p>Some scientists claim the Matthews correlation coefficient to be the most informative single score to establish the quality of a binary classifier prediction in a confusion matrix context.</p> <p>Examples:</p> <ul> <li><code>mcc</code></li> <li><code>phi</code></li> </ul> Read more: <ol> <li>scikit-learn</li> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.aliases","title":"<code>aliases = ['mcc', 'matthews_corrcoef', 'matthews_correlation_coefficient', 'phi', 'phi_coefficient']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.bounds","title":"<code>bounds = (-1.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.dependencies","title":"<code>dependencies = ('diag_mass', 'p_condition', 'p_pred')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.is_multiclass","title":"<code>is_multiclass = True</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.sklearn_equivalent","title":"<code>sklearn_equivalent = 'matthews_corrcoef'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.CohensKappa","title":"<code>CohensKappa</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the multiclass Cohen's Kappa coefficient.</p> <p>Commonly used to quantify inter-annotator agreement, Cohen's kappa can also be used to quantify the quality of a predictor.</p> <p>It is defined as</p> \\[\\kappa=\\frac{p_o-p_e}{1-p_e}\\] <p>where \\(p_o\\) is the observed agreement and \\(p_e\\) the expected agreement due to chance. Perfect agreement yields a score of 1, with a score of 0 corresponding to random performance. Several guidelines exist to interpret the magnitude of the score.</p> <p>Examples:</p> <ul> <li><code>kappa</code></li> <li><code>cohen_kappa</code></li> </ul> Read more: <ol> <li>sklearn</li> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.CohensKappa.aliases","title":"<code>aliases = ['kappa', 'cohen_kappa']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.CohensKappa.bounds","title":"<code>bounds = (-1.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.CohensKappa.dependencies","title":"<code>dependencies = ('diag_mass', 'p_condition', 'p_pred')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.CohensKappa.is_multiclass","title":"<code>is_multiclass = True</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.CohensKappa.sklearn_equivalent","title":"<code>sklearn_equivalent = 'cohen_kappa_score'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.F1","title":"<code>F1</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the univariate \\(F_{1}\\)-score.</p> <p>It is defined as:</p> \\[\\mathtt{F}_{1}=2\\dfrac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\\] <p>or simply put, the harmonic mean between precision (PPV) and recall (TPR).</p> <p>It is an exceedingly common metric used to evaluate machine learning performance. It is closely related to the Precision-Recall curve, an anlysis with varying thresholds.</p> <p>The 1 in the name from an unseen \\(\\beta\\) parameter that weights precision and recall. See the <code>FBeta</code> metric.</p> <p>The \\(F_{1}\\)-score is susceptible to class imbalance. Values fall in the range [0, 1]. A random classifier which predicts a class with a probability \\(p\\), achieves a performance of,</p> \\[2\\dfrac{\\text{prevalence}\\cdot p}{\\text{prevalence}+p}.\\] <p>Since this value is maximized for \\(p=1\\), Flach &amp; Kull recommend comparing performance not to a random classifier, but the 'always-on' classifier (perfect recall but poor precision). See the <code>F1Gain</code> metric.</p> <p>Examples:</p> <ul> <li><code>f1</code></li> <li><code>f1@macro</code></li> </ul> Read more: <ol> <li>sklearn</li> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.F1.aliases","title":"<code>aliases = ['f1']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.F1.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.F1.dependencies","title":"<code>dependencies = ('ppv', 'tpr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.F1.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.F1.sklearn_equivalent","title":"<code>sklearn_equivalent = 'f1_score'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FBeta","title":"<code>FBeta</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the univariate \\(F_{\\beta}\\)-score.</p> <p>Commonly used to quantify inter-annotator agreement, Cohen's kappa can also be used to quantify the quality of a predictor.</p> <p>It is defined as:</p> \\[\\mathtt{F}_{\\beta}=(1+\\beta^2)\\dfrac{\\text{precision} \\cdot \\text{recall}}{\\beta^2\\cdot\\text{precision} + \\text{recall}}\\] <p>or simply put, the weighted harmonic mean between precision (PPV) and recall (TPR).</p> <p>The value of \\(\\beta\\) determines to which degree a user deems recall more important than precision. Larger values (x &gt; 1) weight recall more, whereas lower values weight precision more. A value of 1 corresponds to equal weighting, see the <code>F1</code> metric.</p> <p>The \\(F_{\\beta}\\)-score is susceptible to class imbalance. Values fall in the range [0, 1]. A random classifier which predicts a class with a probability \\(p\\), achieves a performance of,</p> \\[(1+\\beta^2)\\dfrac{\\text{prevalence}\\cdot p}{\\beta^2\\cdot\\text{prevalence}+p}.\\] <p>Since this value is maximized for \\(p=1\\), Flach &amp; Kull recommend comparing performance not to a random classifier, but the 'always-on' classifier (perfect recall but poor precision). See the <code>FBetaGain</code> metric.</p> <p>Examples:</p> <ul> <li><code>fbeta+beta=2</code></li> <li><code>fbeta+beta=0.5@macro</code></li> </ul> Read more: <ol> <li>sklearn</li> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FBeta.aliases","title":"<code>aliases = ['fbeta']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FBeta.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FBeta.dependencies","title":"<code>dependencies = ('ppv', 'tpr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FBeta.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.FBeta.sklearn_equivalent","title":"<code>sklearn_equivalent = 'fbeta_score'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Informedness","title":"<code>Informedness</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the Informedness metric, also known Youden's J.</p> <p>It is defined as:</p> \\[\\mathtt{J}=\\text{sensitivity}+\\text{specificity}-1\\] <p>where sensitivity is the True Positive Rate (TPR), and specificity is the True Negative Rate (TNR).</p> <p>Values fall in the range [-1, 1], with higher values corresponding to better performance and 0 corresponding to random performance.</p> <p>In the binary case, this metric is equivalent to the adjusted balanced accuracy, <code>ba+adj=True</code>.</p> <p>It is commonly used in conjunction with a Reciever-Operator Curve analysis.</p> <p>Examples:</p> <ul> <li><code>informedness</code></li> <li><code>youdenj@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Informedness.aliases","title":"<code>aliases = ['informedness', 'youdenj', 'youden_j', 'bookmaker_informedness', 'bm']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Informedness.bounds","title":"<code>bounds = (-1.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Informedness.dependencies","title":"<code>dependencies = ('tpr', 'tnr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Informedness.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Informedness.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Markedness","title":"<code>Markedness</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the markedness metric, also known as \\(\\Delta p\\).</p> <p>It is defined as:</p> \\[\\Delta p=\\text{precision}+NPV-1\\] <p>where precision is the Positive Predictive Value (PPV).</p> <p>Values fall in the range [-1, 1], with higher values corresponding to better performance and 0 corresponding to random performance.</p> <p>It is commonly used in conjunction with a Reciever-Operator Curve analysis.</p> <p>Examples:</p> <ul> <li><code>markedness</code></li> <li><code>delta_p@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Markedness.aliases","title":"<code>aliases = ['markedness', 'delta_p']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Markedness.bounds","title":"<code>bounds = (-1.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Markedness.dependencies","title":"<code>dependencies = ('ppv', 'npv')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Markedness.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.Markedness.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.P4","title":"<code>P4</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the P4 metric.</p> <p>It is defined as:</p> \\[\\mathtt{P4}=4\\left(\\dfrac{1}{\\text{precision}}+\\dfrac{1}{\\text{recall}}+\\dfrac{1}{\\text{specificity}}+\\dfrac{1}{NPV}\\right)^{-1}\\] <p>where precision corresponds to the Positive Predictive Value (PPV), recall to the True Positive Rate (TPR), and specificity to the True Negative Rate (TNR). Put otherwise, it is the harmonic mean of the 4 listed metrics.</p> <p>Introduced in 2022 by Sitarz, it is meant to extend the properties of the F1, Markedness and Informedness metrics. It is one of few defined metrics that incorporates the Negative Predictive Value.</p> <p>Possible values lie in the range [0, 1], with a score of 0 implying one of the intermediate metrics is 0, and a 1 requiring perfect classification.</p> <p>Relative to MCC, the author notes different behaviour at extreme values, but otherwise the metrics are meant to provide a similar amount of information with a single value.</p> <p>Examples:</p> <ul> <li><code>p4</code></li> <li><code>p4@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.P4.aliases","title":"<code>aliases = ['p4']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.P4.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.P4.dependencies","title":"<code>dependencies = ('ppv', 'tpr', 'tnr', 'npv')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.P4.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.P4.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.JaccardIndex","title":"<code>JaccardIndex</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the Jaccard Index, also known as the threat score.</p> <p>It is defined as:</p> \\[\\mathtt{Jaccard}=\\dfrac{TP}{TP+FP+FN}\\] <p>where \\(TP\\) is the count of true positives, \\(FP\\) the count of false positives and \\(FN\\) the count of false negatives.</p> <p>Alternatively, it may be defined as the area of overlap between predicted and conditions, divided by the area of all predicted and condition positives.</p> <p>Due to the alternative definition, it is commonly used when labels are not readily present, for example in evaluating clustering performance.</p> <p>Examples:</p> <ul> <li><code>jaccard</code></li> <li><code>critical_success_index@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.JaccardIndex.aliases","title":"<code>aliases = ['jaccard', 'jaccard_index', 'threat_score', 'critical_success_index']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.JaccardIndex.bounds","title":"<code>bounds = (0.0, 1.0)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.JaccardIndex.dependencies","title":"<code>dependencies = ('diag_mass', 'p_pred', 'p_condition')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.JaccardIndex.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.JaccardIndex.sklearn_equivalent","title":"<code>sklearn_equivalent = 'jaccard_score'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio","title":"<code>PositiveLikelihoodRatio</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the positive likelihood ratio.</p> <p>It is defined as</p> \\[\\mathtt{LR}^{+}=\\dfrac{\\text{sensitivity}}{1-\\text{specificity}}\\] <p>where sensitivity is the True Positive Rate (TPR), and specificity is the True Negative Rate (TNR).</p> <p>Simply put, it is the ratio of the probabilities of the model predicting a positive when the condition is positive and negative, respectively.</p> <p>Possible values lie in the range [0.0, \\(\\infty\\)], with 0.0 corresponding to no true positives, and infinity corresponding to no false positives. Larger values indicate better performance, with a score of 1 corresponding to random performance.</p> <p>Examples:</p> <ul> <li><code>plr</code></li> <li><code>positive_likelihood_ratio@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.aliases","title":"<code>aliases = ['plr', 'positive_likelihood_ratio']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.bounds","title":"<code>bounds = (0.0, float('inf'))</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.dependencies","title":"<code>dependencies = ('tpr', 'fpr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.sklearn_equivalent","title":"<code>sklearn_equivalent = 'class_likelihood_ratios'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio","title":"<code>LogPositiveLikelihoodRatio</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the positive likelihood ratio.</p> <p>It is defined as</p> \\[\\mathtt{LogLR}{+}=\\log\\dfrac{\\text{sensitivity}}{1-\\text{specificity}}\\] <p>where sensitivity is the True Positive Rate (TPR), and specificity is the True Negative Rate (TNR).</p> <p>Simply put, it is logarithm of the ratio of the probabilities of the model predicting a positive when the condition is positive and negative, respectively.</p> <p>Possible values lie in the range (\\(-\\infty\\), \\(\\infty\\)), with \\(-\\infty\\) corresponding to no true positives, and infinity corresponding to no false positives. Larger values indicate better performance, with a score of 0 corresponding to random performance.</p> <p>Examples:</p> <ul> <li><code>log_plr</code></li> <li><code>lplr</code></li> <li><code>log_positive_likelihood_ratio@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.aliases","title":"<code>aliases = ['log_plr', 'lplr', 'log_positive_likelihood_ratio']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.bounds","title":"<code>bounds = (-float('inf'), float('inf'))</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.dependencies","title":"<code>dependencies = ('tpr', 'fpr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.sklearn_equivalent","title":"<code>sklearn_equivalent = 'class_likelihood_ratios'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio","title":"<code>NegativeLikelihoodRatio</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the negative likelihood ratio.</p> <p>It is defined as</p> \\[\\mathtt{LR}^{-}=\\dfrac{1-\\text{sensitivity}}{\\text{specificity}}\\] <p>where sensitivity is the True Positive Rate (TPR), and specificity is the True Negative Rate(TNR).</p> <p>Simply put, it is the ratio of the probabilities of the model predicting a negative when the condition is positive and negative, respectively.</p> <p>Possible values lie in the range [0.0, \\(\\infty\\)], with 0.0 corresponding to no false negatives, and infinity corresponding to no true negatives. Smaller values indicate better performance, with a score of 1 corresponding to random performance.</p> <p>Examples:</p> <ul> <li><code>nlr</code></li> <li><code>negative_likelihood_ratio@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.aliases","title":"<code>aliases = ['negative_likelihood_ratio', 'nlr']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.bounds","title":"<code>bounds = (0.0, float('inf'))</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.dependencies","title":"<code>dependencies = ('fnr', 'tnr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.sklearn_equivalent","title":"<code>sklearn_equivalent = 'class_likelihood_ratios'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio","title":"<code>LogNegativeLikelihoodRatio</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the negative likelihood ratio.</p> <p>It is defined as</p> \\[\\mathtt{LogLR}{-}=\\log \\dfrac{1-\\text{sensitivity}}{\\text{specificity}}\\] <p>where sensitivity is the True Positive Rate (TPR), and specificity is the True Negative Rate (TNR).</p> <p>Simply put, it is the logarithm of the ratio of the probabilities of the model predicting a negative when the condition is positive and negative, respectively.</p> <p>Possible values lie in the range (\\(-\\infty\\), \\(\\infty\\)), with \\(-\\infty\\) corresponding to no true positives, and infinity corresponding to no true negatives. Smaller values indicate better performance, with a score of 0 corresponding to random performance.</p> <p>Examples:</p> <ul> <li><code>log_nlr</code></li> <li><code>lnlr</code></li> <li><code>log_negative_likelihood_ratio@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.aliases","title":"<code>aliases = ['lnlr', 'log_negative_likelihood_ratio', 'log_nlr']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.bounds","title":"<code>bounds = (-float('inf'), float('inf'))</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.dependencies","title":"<code>dependencies = ('fnr', 'tnr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.sklearn_equivalent","title":"<code>sklearn_equivalent = 'class_likelihood_ratios'</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio","title":"<code>DiagnosticOddsRatio</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the diagnostic odds ratio.</p> <p>It is defined as:</p> \\[\\mathtt{DOR}=\\dfrac{\\mathtt{LR}^{+}=}{\\mathtt{LR}^{-}=}\\] <p>where \\(\\mathtt{LR}^{+}=\\) and \\(\\mathtt{LR}^{-}=\\) are the positive and negative likelihood ratios, respectively.</p> <p>Possible values lie in the range [0.0, \\(\\infty\\)]. Larger values indicate better performance, with a score of 1 corresponding to random performance.</p> <p>To make experiment aggregation easier, you can log transform this metric by specifying <code>log_transform=true</code>. This makes the sampling distribution essentially Gaussian.</p> <p>Examples:</p> <ul> <li><code>dor</code></li> <li><code>diagnostic_odds_ratio@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.aliases","title":"<code>aliases = ['dor', 'diagnostic_odds_ratio']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.bounds","title":"<code>bounds = (0.0, float('inf'))</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.dependencies","title":"<code>dependencies = ('nlr', 'plr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio","title":"<code>LogDiagnosticOddsRatio</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the diagnostic odds ratio.</p> <p>It is defined as:</p> \\[\\mathtt{LogDOR}=\\mathtt{LogLR}^{+}-\\mathtt{LogLR}^{-}\\] <p>where \\(\\mathtt{LR}^{+}\\) and \\(\\mathtt{LR}^{-}=\\) are the positive and negative likelihood ratios, respectively.</p> <p>Possible values lie in the range (-\\(\\infty\\), \\(\\infty\\)). Larger values indicate better performance, with a score of 0 corresponding to random performance.</p> <p>Examples:</p> <ul> <li><code>log_dor</code></li> <li><code>ldor</code></li> <li><code>log_diagnostic_odds_ratio@macro</code></li> </ul> Read more: <ol> <li>Wikipedia</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.aliases","title":"<code>aliases = ['log_dor', 'ldor', 'log_diagnostic_odds_ratio']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.bounds","title":"<code>bounds = (-float('inf'), float('inf'))</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.dependencies","title":"<code>dependencies = ('log_plr', 'log_nlr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PrevalenceThreshold","title":"<code>PrevalenceThreshold</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes the prevalence threshold.</p> <p>It is defined as:</p> \\[\\phi \\mathtt{e}=\\frac{\\sqrt{\\mathtt{TPR}\\cdot(1-\\mathtt{TNR})}+\\mathtt{TNR}-1}{\\mathtt{TPR}+\\mathtt{TNR}-1}\\] <p>where \\(\\mathtt{TPR}\\) and \\(\\mathtt{TNR}\\) are the true positive and negative rates, respectively.</p> <p>Possible values lie in the range (0, 1). Larger values indicate worse performance, with a score of 0 corresponding to perfect classification, and a score of 1 to perfect misclassifcation.</p> <p>It representents the inflection point in a sensitivity and specificity curve (ROC), beyond which a classifiers positive predictive value drops sharply. See Balayla (2020) for more information.</p> <p>Examples:</p> <ul> <li><code>pt</code></li> <li><code>prevalence_threshold</code></li> </ul> Read more: <ol> <li>Balayla, J. (2020). Prevalence threshold (\\(\\phi \\mathtt{e}\\)) and the geometry of screening curves. Plos one, 15(10), e0240215.</li> </ol>"},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PrevalenceThreshold.aliases","title":"<code>aliases = ['prev_thresh', 'pt', 'prevalence_threshold']</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PrevalenceThreshold.bounds","title":"<code>bounds = (0, 1)</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PrevalenceThreshold.dependencies","title":"<code>dependencies = ('tpr', 'tnr')</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PrevalenceThreshold.is_multiclass","title":"<code>is_multiclass = False</code>","text":""},{"location":"Reference/Metrics/Metrics.html#prob_conf_mat.metrics._metrics.PrevalenceThreshold.sklearn_equivalent","title":"<code>sklearn_equivalent = None</code>","text":""}]}