
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Ivo Verhoeven">
      
      
        <link rel="canonical" href="https://github.com/ioverho/prob_conf_mat/Reference/Metrics/Metrics.html">
      
      
        <link rel="prev" href="index.html">
      
      
        <link rel="next" href="Averaging.html">
      
      
      <link rel="icon" href="../../_static/logo_square.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Metrics - Probabilistic Confusion Matrices</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        
        <link rel="stylesheet" href="../../assets/external/fonts.googleapis.com/css.49ea35f2.css">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../_static/css/main.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Metrics - Probabilistic Confusion Matrices" >
      
        <meta  property="og:description"  content="" >
      
        <meta  property="og:image"  content="https://github.com/ioverho/prob_conf_mat/assets/images/social/Reference/Metrics/Metrics.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://github.com/ioverho/prob_conf_mat/Reference/Metrics/Metrics.html" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Metrics - Probabilistic Confusion Matrices" >
      
        <meta  name="twitter:description"  content="" >
      
        <meta  name="twitter:image"  content="https://github.com/ioverho/prob_conf_mat/assets/images/social/Reference/Metrics/Metrics.png" >
      
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#abstract-base-class" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Probabilistic Confusion Matrices" class="md-header__button md-logo" aria-label="Probabilistic Confusion Matrices" data-md-component="logo">
      <img id="logo_light" src="../../_static/logo_rectangle_dark_text.svg" alt="logo">
<img id="logo_dark" src="../../_static/logo_rectangle_light_text.svg" alt="logo">
    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Probabilistic Confusion Matrices
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Metrics
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/ioverho/prob_conf_mat" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ioverho/prob_conf_mat
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Getting%20Started/01_estimating_uncertainty.html" class="md-tabs__link">
          
  
  
    
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../How%20To%20Guides/configuration.html" class="md-tabs__link">
          
  
  
    
  
  How-To

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../Study.html" class="md-tabs__link">
          
  
  
    
  
  Reference

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Explanation/generating_confusion_matrices.html" class="md-tabs__link">
          
  
  
    
  
  Explanation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Probabilistic Confusion Matrices" class="md-nav__button md-logo" aria-label="Probabilistic Confusion Matrices" data-md-component="logo">
      <img id="logo_light" src="../../_static/logo_rectangle_dark_text.svg" alt="logo">
<img id="logo_dark" src="../../_static/logo_rectangle_light_text.svg" alt="logo">
    </a>
    Probabilistic Confusion Matrices
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ioverho/prob_conf_mat" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ioverho/prob_conf_mat
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../Getting%20Started/01_estimating_uncertainty.html" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../How%20To%20Guides/configuration.html" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    How-To
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Study.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Study
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ExperimentGroup.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ExperimentGroup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Experiment.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Experiment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Metrics
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4_4">
            <span class="md-nav__icon md-icon"></span>
            Metrics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Metrics
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="Metrics.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Metrics
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract-base-class" class="md-nav__link">
    <span class="md-ellipsis">
      Abstract Base Class
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Abstract Base Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics.abc.Metric" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Metric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics.abc.AveragedMetric" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;AveragedMetric
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metric-instances" class="md-nav__link">
    <span class="md-ellipsis">
      Metric Instances
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Metric Instances">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.DiagMass" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DiagMass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Prevalence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Prevalence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.ModelBias" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ModelBias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.TruePositiveRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TruePositiveRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalseNegativeRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PositivePredictiveValue
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalseDiscoveryRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalsePositiveRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TrueNegativeRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalseOmissionRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NegativePredictiveValue
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Accuracy" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Accuracy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;BalancedAccuracy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MatthewsCorrelationCoefficient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.CohensKappa" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CohensKappa
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.F1" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;F1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FBeta" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FBeta
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Informedness" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Informedness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Markedness" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Markedness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.P4" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;P4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.JaccardIndex" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;JaccardIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PositiveLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LogPositiveLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NegativeLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LogNegativeLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DiagnosticOddsRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LogDiagnosticOddsRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PrevalenceThreshold
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="Averaging.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Averaging
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../Experiment%20Aggregation/index.html" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Experiment Aggregation
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../IO.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Statistics.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Statistics
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../Explanation/generating_confusion_matrices.html" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Explanation
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract-base-class" class="md-nav__link">
    <span class="md-ellipsis">
      Abstract Base Class
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Abstract Base Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics.abc.Metric" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Metric
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics.abc.AveragedMetric" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;AveragedMetric
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metric-instances" class="md-nav__link">
    <span class="md-ellipsis">
      Metric Instances
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Metric Instances">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.DiagMass" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DiagMass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Prevalence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Prevalence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.ModelBias" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ModelBias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.TruePositiveRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TruePositiveRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalseNegativeRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PositivePredictiveValue
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalseDiscoveryRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalsePositiveRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TrueNegativeRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FalseOmissionRate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NegativePredictiveValue
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Accuracy" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Accuracy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;BalancedAccuracy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MatthewsCorrelationCoefficient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.CohensKappa" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CohensKappa
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.F1" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;F1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.FBeta" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FBeta
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Informedness" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Informedness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.Markedness" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;Markedness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.P4" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;P4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.JaccardIndex" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;JaccardIndex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PositiveLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LogPositiveLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NegativeLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LogNegativeLikelihoodRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DiagnosticOddsRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LogDiagnosticOddsRatio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;PrevalenceThreshold
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Metrics</h1>

<h2 id="abstract-base-class">Abstract Base Class<a class="headerlink" href="#abstract-base-class" title="Permanent link">#</a></h2>


<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics.abc.Metric" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>Metric</code>


<a href="#prob_conf_mat.metrics.abc.Metric" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">


        <p>The abstract base class for metrics.</p>
<p>Properties should be implemented as class attributes in derived metrics</p>
<p>The <code>compute_metric</code> method needs to be implemented</p>










  <div class="doc doc-children">





<h4 id="prob_conf_mat.metrics.abc.Metric-attributes">Attributes<a href="#prob_conf_mat.metrics.abc.Metric-attributes" class="headerlink" title="Permanent link">#</a></h4>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.Metric.full_name" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">full_name</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.Metric.full_name" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>A human-readable name for this metric.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.Metric.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.Metric.is_multiclass" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>Whether or not this metric computes a value for each class individually, or for all classes at once.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.Metric.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.Metric.bounds" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>A tuple of the minimum and maximum possible value for this metric to take.</p>
<p>Can be infinite.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.Metric.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.Metric.dependencies" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>All metrics upon which this metric depends.</p>
<p>Used to generate a computation schedule, such that no metric is calculated before its
dependencies. The dependencies <strong>must</strong> match the <code>compute_metric</code> signature.
This is checked during class definition.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.Metric.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.Metric.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>The <code>sklearn</code> equivalent function, if applicable</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.Metric.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.Metric.aliases" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>A list of all valid aliases for this metric. Can be used when creating metric syntax strings.</p>

    </div>

</div>

<h4 id="prob_conf_mat.metrics.abc.Metric-functions">Functions<a href="#prob_conf_mat.metrics.abc.Metric-functions" class="headerlink" title="Permanent link">#</a></h4>

<div class="doc doc-object doc-function">


<h5 id="prob_conf_mat.metrics.abc.Metric.compute_metric" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">compute_metric</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.Metric.compute_metric" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>Computes the metric values from its dependencies.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics.abc.AveragedMetric" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>AveragedMetric</code>


<a href="#prob_conf_mat.metrics.abc.AveragedMetric" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">


        <p>The abstract base class for the composition of any instance of <code>Metric</code> with any instance of <code>Averaging</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>metric</code></b>
              (<code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code>)
          
          <div class="doc-md-description">
            <p>a binary metric</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>averaging</code></b>
              (<code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Averaging&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Averaging&lt;/code&gt;)" href="Averaging.html#prob_conf_mat.metrics.abc.Averaging">Averaging</a></code>)
          
          <div class="doc-md-description">
            <p>an averaging method</p>
          </div>
        </li>
    </ul>










  <div class="doc doc-children">





<h4 id="prob_conf_mat.metrics.abc.AveragedMetric-attributes">Attributes<a href="#prob_conf_mat.metrics.abc.AveragedMetric-attributes" class="headerlink" title="Permanent link">#</a></h4>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.AveragedMetric.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.AveragedMetric.aliases" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>A list of all valid aliases for this metric.</p>
<p>Constructed from the product of the all aliases of the Metric and Averaging methods.</p>
<p>Can be used when creating metric syntax strings.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.AveragedMetric.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.AveragedMetric.is_multiclass" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>Whether or not this metric computes a value for each class individually, or for all classes at once.</p>
<p>An AveragedMetric is <em>always</em> multiclass.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.AveragedMetric.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.AveragedMetric.bounds" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>A tuple of the minimum and maximum possible value for this metric to take. Can be infinite.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.AveragedMetric.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.AveragedMetric.dependencies" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>All metrics upon which this AveragedMetric depends.</p>
<p>Constructed from the union of all Metric and AveragingMethod dependencies.</p>
<p>Used to generate a computation schedule, such that no metric is calculated before its dependencies.</p>
<p>The dependencies <strong>must</strong> match the <code>compute_metric</code> signature.</p>
<p>This is checked during class definition.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h5 id="prob_conf_mat.metrics.abc.AveragedMetric.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#prob_conf_mat.metrics.abc.AveragedMetric.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h5>


    <div class="doc doc-contents ">

        <p>The <code>sklearn</code> equivalent function, if applicable</p>

    </div>

</div>





  </div>

    </div>

</div><h2 id="metric-instances">Metric Instances<a class="headerlink" href="#metric-instances" title="Permanent link">#</a></h2>


<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.DiagMass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>DiagMass</code>


<a href="#prob_conf_mat.metrics._metrics.DiagMass" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the mass on the diagonal of the normalized confusion matrix.</p>
<p>It is defined as the rate of true positives to all entries:</p>
<div class="arithmatex">\[\mathtt{diag}(\mathbf{CM})=TP / N\]</div>
<p>where <span class="arithmatex">\(TP\)</span> are the true positives, and <span class="arithmatex">\(N\)</span> are the total number of predictions.</p>
<p>This is a metric primarily used as a intermediate value for other metrics, and says relatively
little on its own.</p>
<p>Not to be confused with the True Positive Rate.</p>










  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagMass.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;diag_mass&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagMass.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagMass.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagMass.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagMass.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;norm_confusion_matrix&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagMass.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagMass.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagMass.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagMass.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagMass.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.Prevalence" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>Prevalence</code>


<a href="#prob_conf_mat.metrics._metrics.Prevalence" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the marginal distribution of condition occurence. Also known as the prevalence.</p>
<p>It can be defined as the rate of positives to all predictions:</p>
<div class="arithmatex">\[\mathtt{Prev}=P / N\]</div>
<p>where <span class="arithmatex">\(P\)</span> is the count of condition positives, and <span class="arithmatex">\(N\)</span> are the total number of predictions.</p>
<p>This is a metric primarily used as a intermediate value for other metrics, and say relatively little
on its own.</p>










  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Prevalence.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;prevalence&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.Prevalence.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Prevalence.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Prevalence.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Prevalence.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;p_condition&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Prevalence.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Prevalence.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.Prevalence.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Prevalence.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.Prevalence.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.ModelBias" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>ModelBias</code>


<a href="#prob_conf_mat.metrics._metrics.ModelBias" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the marginal distribution of prediction occurence. Also known as the model bias.</p>
<p>It can be defined as the rate of predicted positives to all predictions:</p>
<div class="arithmatex">\[\mathtt{Bias}=PP / N\]</div>
<p>where <span class="arithmatex">\(PP\)</span> is the count of predicted positives, and <span class="arithmatex">\(N\)</span> are the total number of predictions.</p>
<p>This is a metric primarily used as a intermediate value for other metrics, and say relatively little
on its own.</p>










  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.ModelBias.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;model_bias&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.ModelBias.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.ModelBias.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.ModelBias.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.ModelBias.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;p_pred&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.ModelBias.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.ModelBias.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.ModelBias.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.ModelBias.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.ModelBias.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.TruePositiveRate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>TruePositiveRate</code>


<a href="#prob_conf_mat.metrics._metrics.TruePositiveRate" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the True Positive Rate, also known as recall, sensitivity.</p>
<p>It is defined as the ratio of correctly predited positives to all condition positives:</p>
<div class="arithmatex">\[\mathtt{TPR}=TP / P\]</div>
<p>where <span class="arithmatex">\(TP\)</span> are the true positives, and <span class="arithmatex">\(TN\)</span> are true negatives and <span class="arithmatex">\(N\)</span> the number of predictions.</p>
<p>Essentially, out of all condition positives, how many were correctly predicted. Can be seen as a metric
measuring retrieval.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>tpr</code></li>
<li><code>recall@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html">scikit-learn</a></li>
<li><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TruePositiveRate.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;true_positive_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;sensitivity&#39;</span><span class="p">,</span> <span class="s1">&#39;recall&#39;</span><span class="p">,</span> <span class="s1">&#39;hit_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;tpr&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.TruePositiveRate.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TruePositiveRate.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.TruePositiveRate.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TruePositiveRate.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;p_pred_given_condition&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.TruePositiveRate.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TruePositiveRate.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.TruePositiveRate.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TruePositiveRate.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.TruePositiveRate.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.FalseNegativeRate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>FalseNegativeRate</code>


<a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the False Negative Rate, also known as the miss-rate.</p>
<p>It is defined as the ratio of false negatives to condition positives:</p>
<div class="arithmatex">\[\mathtt{FNR}=FN / (TP + FN)\]</div>
<p>where <span class="arithmatex">\(TP\)</span> are the true positives, and <span class="arithmatex">\(FN\)</span> are the false negatives.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>fnr</code></li>
<li><code>false_negative_rate@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives#Related_terms">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseNegativeRate.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;false_negative_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;miss_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;fnr&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseNegativeRate.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseNegativeRate.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;true_positive_rate&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseNegativeRate.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseNegativeRate.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseNegativeRate.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.PositivePredictiveValue" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>PositivePredictiveValue</code>


<a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the Positive Predictive Value, also known as precision.</p>
<p>It is defined as the ratio of true positives to predicted positives:</p>
<div class="arithmatex">\[\mathtt{PPV}=TP / (TP + FP)\]</div>
<p>where <span class="arithmatex">\(TP\)</span> is the count of true positives, and <span class="arithmatex">\(FP\)</span> the count falsely predicted positives.</p>
<p>It is the complement of the False Discovery Rate, <span class="arithmatex">\(PPV=1-FDR\)</span>.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>ppv</code></li>
<li><code>precision@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html">scikit-learn</a></li>
<li><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositivePredictiveValue.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;positive_predictive_value&#39;</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="s1">&#39;ppv&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositivePredictiveValue.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositivePredictiveValue.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;p_condition_given_pred&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositivePredictiveValue.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositivePredictiveValue.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositivePredictiveValue.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.FalseDiscoveryRate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>FalseDiscoveryRate</code>


<a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the False Discovery Rate.</p>
<p>It is defined as the ratio of falsely predicted positives to predicted positives:</p>
<div class="arithmatex">\[\mathtt{FDR}=FP / (TP + FP)\]</div>
<p>where <span class="arithmatex">\(TP\)</span> is the count of true positives, and <span class="arithmatex">\(FP\)</span> the count of falsely predicted positives.</p>
<p>It is the complement of the Positive Predictve Value, <span class="arithmatex">\(FDR=1-PPV\)</span>.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>fdr</code></li>
<li><code>false_discovery_rate@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/False_discovery_rate">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseDiscoveryRate.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;false_discovery_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;fdr&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseDiscoveryRate.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseDiscoveryRate.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;positive_predictive_value&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseDiscoveryRate.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseDiscoveryRate.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseDiscoveryRate.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.FalsePositiveRate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>FalsePositiveRate</code>


<a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the False Positive Rate, the probability of false alarm.</p>
<p>Also known as the fall-out.</p>
<p>It is defined as the ratio of falsely predicted positives to condition negatives:</p>
<div class="arithmatex">\[\mathtt{FPR}=FP / (TN + FP)\]</div>
<p>where <span class="arithmatex">\(TN\)</span> is the count of true negatives, and <span class="arithmatex">\(FP\)</span> the count of falsely predicted positives.</p>
<p>It is the complement of the True Negative Rate, <span class="arithmatex">\(FPR=1-TNR\)</span>.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>fpr</code></li>
<li><code>fall-out@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/False_positive_rate">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalsePositiveRate.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;false_positive_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;fall-out&#39;</span><span class="p">,</span> <span class="s1">&#39;fall_out&#39;</span><span class="p">,</span> <span class="s1">&#39;fpr&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalsePositiveRate.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalsePositiveRate.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;diag_mass&#39;</span><span class="p">,</span> <span class="s1">&#39;p_pred&#39;</span><span class="p">,</span> <span class="s1">&#39;p_condition&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalsePositiveRate.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalsePositiveRate.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalsePositiveRate.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.TrueNegativeRate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>TrueNegativeRate</code>


<a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the True Negative Rate, i.e. specificity, selectivity.</p>
<p>It is defined as the ratio of true predicted negatives to condition negatives:</p>
<div class="arithmatex">\[\mathtt{TNR}=TN / (TN + FP)\]</div>
<p>where <span class="arithmatex">\(TN\)</span> is the count of true negatives, and FP the count of falsely predicted positives.</p>
<p>It is the complement of the False Positive Rate, <span class="arithmatex">\(TNR=1-FPR\)</span>.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>tnr</code></li>
<li><code>selectivity@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TrueNegativeRate.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;true_negative_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;specificity&#39;</span><span class="p">,</span> <span class="s1">&#39;selectivity&#39;</span><span class="p">,</span> <span class="s1">&#39;tnr&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TrueNegativeRate.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TrueNegativeRate.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;false_positive_rate&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TrueNegativeRate.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.TrueNegativeRate.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.TrueNegativeRate.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.FalseOmissionRate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>FalseOmissionRate</code>


<a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the False Omission Rate.</p>
<p>It is defined as the ratio of falsely predicted negatives to all predicted negatives:</p>
<div class="arithmatex">\[\mathtt{FOR}=FN / (TN + FN)\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(TN\)</span>\)</span> is the count of true negatives, and <span class="arithmatex">\(<span class="arithmatex">\(FN\)</span>\)</span> the count of falsely predicted negatives.</p>
<p>It is the complement of the Negative Predictive Value, <span class="arithmatex">\(FOR=1-NPV\)</span>.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>for</code></li>
<li><code>false_omission_rate@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#false_omission_rate">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseOmissionRate.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;false_omission_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;for&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseOmissionRate.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseOmissionRate.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;p_condition&#39;</span><span class="p">,</span> <span class="s1">&#39;p_pred&#39;</span><span class="p">,</span> <span class="s1">&#39;diag_mass&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseOmissionRate.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FalseOmissionRate.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.FalseOmissionRate.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.NegativePredictiveValue" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>NegativePredictiveValue</code>


<a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the Negative Predicitive Value.</p>
<p>It is defined as the ratio of true negatives to all predicted negatives:</p>
<div class="arithmatex">\[\mathtt{NPV}=TN / (TN + FN)\]</div>
<p>where TN are the true negatives, and FN are the falsely predicted negatives.</p>
<p>It is the complement of the False Omission Rate, <span class="arithmatex">\(NPV=1-FOR\)</span>.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>npv</code></li>
<li><code>negative_predictive_value@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#Negative_predictive_value_(NPV)">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativePredictiveValue.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;negative_predictive_value&#39;</span><span class="p">,</span> <span class="s1">&#39;npv&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativePredictiveValue.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativePredictiveValue.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;false_omission_rate&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativePredictiveValue.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativePredictiveValue.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativePredictiveValue.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.Accuracy" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>Accuracy</code>


<a href="#prob_conf_mat.metrics._metrics.Accuracy" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the multiclass accuracy score.</p>
<p>It is defined as the rate of correct classifications to all classifications:</p>
<div class="arithmatex">\[\mathtt{Acc}=(TP + TN) / N\]</div>
<p>where <span class="arithmatex">\(TP\)</span> are the true positives, <span class="arithmatex">\(TN\)</span> the true negatives and <span class="arithmatex">\(N\)</span> the total number of predictions.</p>
<p>Possible values lie in the range [0.0, 1.0], with larger values denoting better performance. The value
of a random classifier is dependent on the label distribution, which makes accuracy especially susceptible
to class imbalance. It is also not directly comparable across datasets.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>acc</code></li>
<li><code>accuracy@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score">scikit-learn</a></li>
<li><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Accuracy.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.Accuracy.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Accuracy.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Accuracy.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Accuracy.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;diag_mass&#39;</span><span class="p">,)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Accuracy.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Accuracy.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">True</span></code>

<a href="#prob_conf_mat.metrics._metrics.Accuracy.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Accuracy.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;accuracy_score&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.Accuracy.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.BalancedAccuracy" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>BalancedAccuracy</code>


<a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the balanced accuracy score.</p>
<p>It is defined as the the arithmetic average of the per-class true-positive rate:</p>
<div class="arithmatex">\[\mathtt{BA}=\frac{1}{|C|}\sum TPR_{c}\]</div>
<p>where <span class="arithmatex">\(TPR\)</span> is the true positive rate (precision).</p>
<p>Possible values lie in the range [0.0, 1.0], with larger values denoting better performance. Unlike
accuracy, balanced accuracy can be 'chance corrected', such that random performance is yield a score
of 0.0. This can be achieved by setting <code>adjusted=True</code>.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>ba</code></li>
<li><code>balanced_accuracy@macro</code></li>
<li><code>ba+adjusted=True</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score">scikit-learn</a></li>
</ol>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>adjusted</code></b>
              (<code><span title="bool">bool</span></code>, default:
                  <code>False</code>
)
          
          <div class="doc-md-description">
            <p>whether the chance-corrected variant is computed. Defaults to <code>False</code>.</p>
          </div>
        </li>
    </ul>










  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.BalancedAccuracy.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ba&#39;</span><span class="p">,</span> <span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.BalancedAccuracy.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.BalancedAccuracy.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;p_condition&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.BalancedAccuracy.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">True</span></code>

<a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.BalancedAccuracy.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;balanced_accuracy_score&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.BalancedAccuracy.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>MatthewsCorrelationCoefficient</code>


<a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the multiclass Matthew's Correlation Coefficient (MCC), also known as the phi coefficient.</p>
<p>Goes by a variety of names, depending on the application scenario.</p>
<p>A metric that holistically combines many different classification metrics.</p>
<p>A perfect classifier scores 1.0, a random classifier 0.0. Values smaller than 0
indicate worse than random performance.</p>
<p>It's absolute value is proportional to the square root of the Chi-square test statistic.</p>
<p>Quoting Wikipedia:</p>
<blockquote>
<p>Some scientists claim the Matthews correlation coefficient to be the most informative
single score to establish the quality of a binary classifier prediction in a confusion matrix context.</p>
</blockquote>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>mcc</code></li>
<li><code>phi</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#matthews-correlation-coefficient">scikit-learn</a></li>
<li><a href="https://en.wikipedia.org/wiki/Phi_coefficient">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mcc&#39;</span><span class="p">,</span> <span class="s1">&#39;matthews_corrcoef&#39;</span><span class="p">,</span> <span class="s1">&#39;matthews_correlation_coefficient&#39;</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">,</span> <span class="s1">&#39;phi_coefficient&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;diag_mass&#39;</span><span class="p">,</span> <span class="s1">&#39;p_condition&#39;</span><span class="p">,</span> <span class="s1">&#39;p_pred&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">True</span></code>

<a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;matthews_corrcoef&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.MatthewsCorrelationCoefficient.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.CohensKappa" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>CohensKappa</code>


<a href="#prob_conf_mat.metrics._metrics.CohensKappa" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the multiclass Cohen's Kappa coefficient.</p>
<p>Commonly used to quantify inter-annotator agreement, Cohen's kappa can also
be used to quantify the quality of a predictor.</p>
<p>It is defined as</p>
<div class="arithmatex">\[\kappa=\frac{p_o-p_e}{1-p_e}\]</div>
<p>where <span class="arithmatex">\(p_o\)</span> is the observed agreement and <span class="arithmatex">\(p_e\)</span> the expected agreement
due to chance. Perfect agreement yields a score of 1, with a score of
0 corresponding to random performance. Several guidelines exist to interpret
the magnitude of the score.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>kappa</code></li>
<li><code>cohen_kappa</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa">sklearn</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.CohensKappa.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;kappa&#39;</span><span class="p">,</span> <span class="s1">&#39;cohen_kappa&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.CohensKappa.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.CohensKappa.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.CohensKappa.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.CohensKappa.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;diag_mass&#39;</span><span class="p">,</span> <span class="s1">&#39;p_condition&#39;</span><span class="p">,</span> <span class="s1">&#39;p_pred&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.CohensKappa.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.CohensKappa.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">True</span></code>

<a href="#prob_conf_mat.metrics._metrics.CohensKappa.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.CohensKappa.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;cohen_kappa_score&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.CohensKappa.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.F1" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>F1</code>


<a href="#prob_conf_mat.metrics._metrics.F1" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the univariate <span class="arithmatex">\(F_{1}\)</span>-score.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\mathtt{F}_{1}=2\dfrac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}\]</div>
<p>or simply put, the harmonic mean between precision (PPV) and recall (TPR).</p>
<p>It is an exceedingly common metric used to evaluate machine learning performance. It is closely
related to the Precision-Recall curve, an anlysis with varying thresholds.</p>
<p>The 1 in the name from an unseen <span class="arithmatex">\(\beta\)</span> parameter that weights precision and recall.
See the <code>FBeta</code> metric.</p>
<p>The <span class="arithmatex">\(F_{1}\)</span>-score is susceptible to class imbalance. Values fall in the range [0, 1]. A random
classifier which predicts a class with a probability <span class="arithmatex">\(p\)</span>, achieves a performance of,</p>
<div class="arithmatex">\[2\dfrac{\text{prevalence}\cdot p}{\text{prevalence}+p}.\]</div>
<p>Since this value is maximized for <span class="arithmatex">\(p=1\)</span>, <a href="https://proceedings.neurips.cc/paper/2015/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html">Flach &amp; Kull</a>
recommend comparing performance not to a random classifier, but the 'always-on' classifier
(perfect recall but poor precision). See the <code>F1Gain</code> metric.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>f1</code></li>
<li><code>f1@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics">sklearn</a></li>
<li><a href="https://en.wikipedia.org/wiki/F-score">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.F1.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;f1&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.F1.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.F1.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.F1.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.F1.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ppv&#39;</span><span class="p">,</span> <span class="s1">&#39;tpr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.F1.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.F1.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.F1.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.F1.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;f1_score&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.F1.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.FBeta" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>FBeta</code>


<a href="#prob_conf_mat.metrics._metrics.FBeta" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the univariate <span class="arithmatex">\(F_{\beta}\)</span>-score.</p>
<p>Commonly used to quantify inter-annotator agreement, Cohen's kappa can also
be used to quantify the quality of a predictor.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\mathtt{F}_{\beta}=(1+\beta^2)\dfrac{\text{precision} \cdot \text{recall}}{\beta^2\cdot\text{precision} + \text{recall}}\]</div>
<p>or simply put, the weighted harmonic mean between precision (PPV) and recall (TPR).</p>
<p>The value of <span class="arithmatex">\(\beta\)</span> determines to which degree a user deems recall more important than
precision. Larger values (x &gt; 1) weight recall more, whereas lower values weight precision more.
A value of 1 corresponds to equal weighting, see the <code>F1</code> metric.</p>
<p>The <span class="arithmatex">\(F_{\beta}\)</span>-score is susceptible to class imbalance. Values fall in the range [0, 1]. A
random classifier which predicts a class with a probability <span class="arithmatex">\(p\)</span>, achieves a performance of,</p>
<div class="arithmatex">\[(1+\beta^2)\dfrac{\text{prevalence}\cdot p}{\beta^2\cdot\text{prevalence}+p}.\]</div>
<p>Since this value is maximized for <span class="arithmatex">\(p=1\)</span>, <a href="https://proceedings.neurips.cc/paper/2015/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html">Flach &amp; Kull</a>
recommend comparing performance not to a random classifier, but the 'always-on' classifier
(perfect recall but poor precision). See the <code>FBetaGain</code> metric.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>fbeta+beta=2</code></li>
<li><code>fbeta+beta=0.5@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics">sklearn</a></li>
<li><a href="https://en.wikipedia.org/wiki/F-score">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FBeta.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fbeta&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.FBeta.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FBeta.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FBeta.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FBeta.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ppv&#39;</span><span class="p">,</span> <span class="s1">&#39;tpr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.FBeta.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FBeta.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.FBeta.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.FBeta.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;fbeta_score&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.FBeta.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.Informedness" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>Informedness</code>


<a href="#prob_conf_mat.metrics._metrics.Informedness" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the Informedness metric, also known Youden's J.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\mathtt{J}=\text{sensitivity}+\text{specificity}-1\]</div>
<p>where sensitivity is the True Positive Rate (TPR), and specificity is the
True Negative Rate (TNR).</p>
<p>Values fall in the range [-1, 1], with higher values corresponding to better performance and 0
corresponding to random performance.</p>
<p>In the binary case, this metric is equivalent to the adjusted balanced accuracy, <code>ba+adj=True</code>.</p>
<p>It is commonly used in conjunction with a Reciever-Operator Curve analysis.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>informedness</code></li>
<li><code>youdenj@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Youden%27s_J_statistic">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Informedness.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;informedness&#39;</span><span class="p">,</span> <span class="s1">&#39;youdenj&#39;</span><span class="p">,</span> <span class="s1">&#39;youden_j&#39;</span><span class="p">,</span> <span class="s1">&#39;bookmaker_informedness&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.Informedness.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Informedness.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Informedness.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Informedness.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;tnr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Informedness.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Informedness.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.Informedness.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Informedness.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.Informedness.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.Markedness" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>Markedness</code>


<a href="#prob_conf_mat.metrics._metrics.Markedness" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the markedness metric, also known as <span class="arithmatex">\(\Delta p\)</span>.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\Delta p=\text{precision}+NPV-1\]</div>
<p>where precision is the Positive Predictive Value (PPV).</p>
<p>Values fall in the range [-1, 1], with higher values corresponding to better performance and 0
corresponding to random performance.</p>
<p>It is commonly used in conjunction with a Reciever-Operator Curve analysis.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>markedness</code></li>
<li><code>delta_p@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Markedness#">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Markedness.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;markedness&#39;</span><span class="p">,</span> <span class="s1">&#39;delta_p&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.Markedness.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Markedness.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Markedness.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Markedness.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ppv&#39;</span><span class="p">,</span> <span class="s1">&#39;npv&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.Markedness.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Markedness.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.Markedness.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.Markedness.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.Markedness.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.P4" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>P4</code>


<a href="#prob_conf_mat.metrics._metrics.P4" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the P4 metric.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\mathtt{P4}=4\left(\dfrac{1}{\text{precision}}+\dfrac{1}{\text{recall}}+\dfrac{1}{\text{specificity}}+\dfrac{1}{NPV}\right)^{-1}\]</div>
<p>where precision corresponds to the Positive Predictive Value (PPV), recall to the
True Positive Rate (TPR), and specificity to the True Negative Rate (TNR). Put otherwise, it is
the harmonic mean of the 4 listed metrics.</p>
<p>Introduced in 2022 by <a href="https://arxiv.org/abs/2210.11997">Sitarz</a>, it is meant to extend the
properties of the F1, Markedness and Informedness metrics. It is one of few defined metrics
that incorporates the Negative Predictive Value.</p>
<p>Possible values lie in the range [0, 1], with a score of 0 implying one of the intermediate
metrics is 0, and a 1 requiring perfect classification.</p>
<p>Relative to MCC, the author notes different behaviour at extreme values, but otherwise the
metrics are meant to provide a similar amount of information with a single value.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>p4</code></li>
<li><code>p4@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/P4-metric">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.P4.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;p4&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.P4.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.P4.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.P4.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.P4.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ppv&#39;</span><span class="p">,</span> <span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;tnr&#39;</span><span class="p">,</span> <span class="s1">&#39;npv&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.P4.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.P4.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.P4.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.P4.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.P4.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.JaccardIndex" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>JaccardIndex</code>


<a href="#prob_conf_mat.metrics._metrics.JaccardIndex" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the Jaccard Index, also known as the threat score.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\mathtt{Jaccard}=\dfrac{TP}{TP+FP+FN}\]</div>
<p>where <span class="arithmatex">\(TP\)</span> is the count of true positives, <span class="arithmatex">\(FP\)</span> the count of false positives and <span class="arithmatex">\(FN\)</span> the count
of false negatives.</p>
<p>Alternatively, it may be defined as the area of overlap between predicted and conditions,
divided by the area of all predicted and condition positives.</p>
<p>Due to the alternative definition, it is commonly used when labels are not readily present, for
example in evaluating clustering performance.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>jaccard</code></li>
<li><code>critical_success_index@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Jaccard_index#Jaccard_index_in_binary_classification_confusion_matrices">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.JaccardIndex.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;jaccard&#39;</span><span class="p">,</span> <span class="s1">&#39;jaccard_index&#39;</span><span class="p">,</span> <span class="s1">&#39;threat_score&#39;</span><span class="p">,</span> <span class="s1">&#39;critical_success_index&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.JaccardIndex.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.JaccardIndex.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.JaccardIndex.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.JaccardIndex.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;diag_mass&#39;</span><span class="p">,</span> <span class="s1">&#39;p_pred&#39;</span><span class="p">,</span> <span class="s1">&#39;p_condition&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.JaccardIndex.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.JaccardIndex.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.JaccardIndex.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.JaccardIndex.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;jaccard_score&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.JaccardIndex.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>PositiveLikelihoodRatio</code>


<a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the positive likelihood ratio.</p>
<p>It is defined as</p>
<div class="arithmatex">\[\mathtt{LR}^{+}=\dfrac{\text{sensitivity}}{1-\text{specificity}}\]</div>
<p>where sensitivity is the True Positive Rate (TPR), and specificity is the
True Negative Rate (TNR).</p>
<p>Simply put, it is the ratio of the probabilities of the model predicting a positive when the
condition is positive and negative, respectively.</p>
<p>Possible values lie in the range [0.0, <span class="arithmatex">\(\infty\)</span>], with 0.0 corresponding to no true positives,
and infinity corresponding to no false positives. Larger values indicate better performance,
with a score of 1 corresponding to random performance.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>plr</code></li>
<li><code>positive_likelihood_ratio@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing#positive_likelihood_ratio">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;plr&#39;</span><span class="p">,</span> <span class="s1">&#39;positive_likelihood_ratio&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;fpr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;class_likelihood_ratios&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.PositiveLikelihoodRatio.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>LogPositiveLikelihoodRatio</code>


<a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the positive likelihood ratio.</p>
<p>It is defined as</p>
<div class="arithmatex">\[\mathtt{LogLR}{+}=\log\dfrac{\text{sensitivity}}{1-\text{specificity}}\]</div>
<p>where sensitivity is the True Positive Rate (TPR), and specificity is the
True Negative Rate (TNR).</p>
<p>Simply put, it is logarithm of the ratio of the probabilities of the model predicting a
positive when the condition is positive and negative, respectively.</p>
<p>Possible values lie in the range (<span class="arithmatex">\(-\infty\)</span>, <span class="arithmatex">\(\infty\)</span>), with <span class="arithmatex">\(-\infty\)</span> corresponding to no
true positives, and infinity corresponding to no false positives. Larger values indicate better
performance, with a score of 0 corresponding to random performance.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>log_plr</code></li>
<li><code>lplr</code></li>
<li><code>log_positive_likelihood_ratio@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing#positive_likelihood_ratio">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;log_plr&#39;</span><span class="p">,</span> <span class="s1">&#39;lplr&#39;</span><span class="p">,</span> <span class="s1">&#39;log_positive_likelihood_ratio&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;fpr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;class_likelihood_ratios&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogPositiveLikelihoodRatio.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>NegativeLikelihoodRatio</code>


<a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the negative likelihood ratio.</p>
<p>It is defined as</p>
<div class="arithmatex">\[\mathtt{LR}^{-}=\dfrac{1-\text{sensitivity}}{\text{specificity}}\]</div>
<p>where sensitivity is the True Positive Rate (TPR), and specificity is the
True Negative Rate(TNR).</p>
<p>Simply put, it is the ratio of the probabilities of the model predicting a negative when the
condition is positive and negative, respectively.</p>
<p>Possible values lie in the range [0.0, <span class="arithmatex">\(\infty\)</span>], with 0.0 corresponding to no false negatives,
and infinity corresponding to no true negatives. Smaller values indicate better performance,
with a score of 1 corresponding to random performance.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>nlr</code></li>
<li><code>negative_likelihood_ratio@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing#negative_likelihood_ratio">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;negative_likelihood_ratio&#39;</span><span class="p">,</span> <span class="s1">&#39;nlr&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;fnr&#39;</span><span class="p">,</span> <span class="s1">&#39;tnr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;class_likelihood_ratios&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.NegativeLikelihoodRatio.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>LogNegativeLikelihoodRatio</code>


<a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the negative likelihood ratio.</p>
<p>It is defined as</p>
<div class="arithmatex">\[\mathtt{LogLR}{-}=\log \dfrac{1-\text{sensitivity}}{\text{specificity}}\]</div>
<p>where sensitivity is the True Positive Rate (TPR), and specificity is the
True Negative Rate (TNR).</p>
<p>Simply put, it is the logarithm of the ratio of the probabilities of the model predicting a
negative when the condition is positive and negative, respectively.</p>
<p>Possible values lie in the range (<span class="arithmatex">\(-\infty\)</span>, <span class="arithmatex">\(\infty\)</span>), with <span class="arithmatex">\(-\infty\)</span> corresponding to no
true positives, and infinity corresponding to no true negatives. Smaller values indicate better
performance, with a score of 0 corresponding to random performance.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>log_nlr</code></li>
<li><code>lnlr</code></li>
<li><code>log_negative_likelihood_ratio@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing#negative_likelihood_ratio">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;lnlr&#39;</span><span class="p">,</span> <span class="s1">&#39;log_negative_likelihood_ratio&#39;</span><span class="p">,</span> <span class="s1">&#39;log_nlr&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;fnr&#39;</span><span class="p">,</span> <span class="s1">&#39;tnr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="s1">&#39;class_likelihood_ratios&#39;</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogNegativeLikelihoodRatio.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.DiagnosticOddsRatio" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>DiagnosticOddsRatio</code>


<a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the diagnostic odds ratio.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\mathtt{DOR}=\dfrac{\mathtt{LR}^{+}=}{\mathtt{LR}^{-}=}\]</div>
<p>where <span class="arithmatex">\(\mathtt{LR}^{+}=\)</span> and <span class="arithmatex">\(\mathtt{LR}^{-}=\)</span> are the positive and
negative likelihood ratios, respectively.</p>
<p>Possible values lie in the range [0.0, <span class="arithmatex">\(\infty\)</span>]. Larger values indicate better performance,
with a score of 1 corresponding to random performance.</p>
<p>To make experiment aggregation easier, you can log transform this metric by specifying
<code>log_transform=true</code>. This makes the sampling distribution essentially Gaussian.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>dor</code></li>
<li><code>diagnostic_odds_ratio@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Diagnostic_odds_ratio">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dor&#39;</span><span class="p">,</span> <span class="s1">&#39;diagnostic_odds_ratio&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;nlr&#39;</span><span class="p">,</span> <span class="s1">&#39;plr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.DiagnosticOddsRatio.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>LogDiagnosticOddsRatio</code>


<a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the diagnostic odds ratio.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\mathtt{LogDOR}=\mathtt{LogLR}^{+}-\mathtt{LogLR}^{-}\]</div>
<p>where <span class="arithmatex">\(\mathtt{LR}^{+}\)</span> and <span class="arithmatex">\(\mathtt{LR}^{-}=\)</span> are the positive and
negative likelihood ratios, respectively.</p>
<p>Possible values lie in the range (-<span class="arithmatex">\(\infty\)</span>, <span class="arithmatex">\(\infty\)</span>). Larger values indicate better
performance, with a score of 0 corresponding to random performance.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>log_dor</code></li>
<li><code>ldor</code></li>
<li><code>log_diagnostic_odds_ratio@macro</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://en.wikipedia.org/wiki/Diagnostic_odds_ratio">Wikipedia</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;log_dor&#39;</span><span class="p">,</span> <span class="s1">&#39;ldor&#39;</span><span class="p">,</span> <span class="s1">&#39;log_diagnostic_odds_ratio&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;log_plr&#39;</span><span class="p">,</span> <span class="s1">&#39;log_nlr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.LogDiagnosticOddsRatio.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="prob_conf_mat.metrics._metrics.PrevalenceThreshold" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>PrevalenceThreshold</code>


<a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold" class="headerlink" title="Permanent link">#</a></h3>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="&lt;code class=&quot;doc-symbol doc-symbol-heading doc-symbol-class&quot;&gt;&lt;/code&gt;            &lt;code&gt;Metric&lt;/code&gt; (&lt;code&gt;prob_conf_mat.metrics.abc.Metric&lt;/code&gt;)" href="#prob_conf_mat.metrics.abc.Metric">Metric</a></code></p>


        <p>Computes the prevalence threshold.</p>
<p>It is defined as:</p>
<div class="arithmatex">\[\phi \mathtt{e}=\frac{\sqrt{\mathtt{TPR}\cdot(1-\mathtt{TNR})}+\mathtt{TNR}-1}{\mathtt{TPR}+\mathtt{TNR}-1}\]</div>
<p>where <span class="arithmatex">\(\mathtt{TPR}\)</span> and <span class="arithmatex">\(\mathtt{TNR}\)</span> are the true positive and negative rates, respectively.</p>
<p>Possible values lie in the range (0, 1). Larger values indicate <em>worse</em> performance, with a
score of 0 corresponding to perfect classification, and a score of 1 to perfect
misclassifcation.</p>
<p>It representents the inflection point in a sensitivity and specificity curve (ROC), beyond which
a classifiers positive predictive value drops sharply. See <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0240215#sec002">Balayla (2020)</a>
for more information.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <ul>
<li><code>pt</code></li>
<li><code>prevalence_threshold</code></li>
</ul>


<details class="note" open>
  <summary>Read more:</summary>
  <ol>
<li><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7540853/">Balayla, J. (2020). Prevalence threshold (<span class="arithmatex">\(\phi \mathtt{e}\)</span>) and the geometry of screening curves. Plos one, 15(10), e0240215.</a></li>
</ol>
</details>









  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PrevalenceThreshold.aliases" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">aliases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;prev_thresh&#39;</span><span class="p">,</span> <span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="s1">&#39;prevalence_threshold&#39;</span><span class="p">]</span></code>

<a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold.aliases" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PrevalenceThreshold.bounds" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold.bounds" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PrevalenceThreshold.dependencies" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">dependencies</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;tnr&#39;</span><span class="p">)</span></code>

<a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold.dependencies" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PrevalenceThreshold.is_multiclass" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">is_multiclass</span> <span class="o">=</span> <span class="kc">False</span></code>

<a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold.is_multiclass" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>






<div class="doc doc-object doc-attribute">



<h4 id="prob_conf_mat.metrics._metrics.PrevalenceThreshold.sklearn_equivalent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <code class="highlight language-python"><span class="n">sklearn_equivalent</span> <span class="o">=</span> <span class="kc">None</span></code>

<a href="#prob_conf_mat.metrics._metrics.PrevalenceThreshold.sklearn_equivalent" class="headerlink" title="Permanent link">#</a></h4>


    <div class="doc doc-contents ">

    </div>

</div>




  </div>

    </div>

</div>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="June 30, 2025 21:33:29 UTC">June 30, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="index.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Metrics and Averaging">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Metrics and Averaging
              </div>
            </div>
          </a>
        
        
          
          <a href="Averaging.html" class="md-footer__link md-footer__link--next" aria-label="Next: Averaging">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Averaging
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2025 Ivo Verhoeven
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.footnote.tooltips", "content.tooltips", "content.code.copy", "content.code.select", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.expand", "navigation.indexes", "navigation.tracking", "navigation.prune", "toc.follow", "navigation.top", "search.suggest", "search.highlight", "navigation.footer"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../_static/mathjax.js"></script>
      
        <script src="../../assets/external/unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>